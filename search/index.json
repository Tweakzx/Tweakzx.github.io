[{"content":"在Kubernetes集群中使用GPU资源 在前文中， 我们搭建了k8s集群， 但是k8s原生不支持GPU资源， 需要使用各大GPU厂商开发的插件才能使用。\n这一部分大家可以参考NVIDIA/k8s-device-plugin: NVIDIA device plugin for Kubernetes (github.com)\n本文就记录我在k8s集群中配置插件的过程。\n安装前的环境配置  Centos Linux realease 7.9.2009 内核：3.10.0-1062.18.1.el7.x86_64 Docker-ce：18.09.7 Kubernetes：1.16.15 Nvidia Driver： 460.32.03两台，510.47.03一台  安装nvidia-docker2  如果你使用的是新版本的docker (\u0026gt;=19.03)，则推荐使用nvidia-container-toolkit包来代替nvidia-docker2包：\n 由于我的docker版本是18.09.7， 所以我需要安装nvidia-docker2\n##设置仓库 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \\ sudo tee /etc/yum.repos.d/nvidia-docker.repo ##更新仓库中的key DIST=$(sed -n \u0026#39;s/releasever=//p\u0026#39; /etc/yum.conf) DIST=${DIST:-$(. /etc/os-release; echo $VERSION_ID)} sudo yum makecache ##安装nvidia-docker2 sudo yum install nvidia-docker2 这时候， 查看/etc/docker/daemon.json，你会发现内容已经改了， 内容如下\n{ \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 但是， 我还有一些其他的设置， 例如cgroupdriver， 镜像等等也需要设置， 所以我的修改了daemon.json，内容如下\n{ \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://n73pm3wf.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ], \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 修改完之后，记得重启docker\n##重启docker systemctl restart docker 特别注意一个字段\u0026quot;default-runtime\u0026quot;: \u0026quot;nvidia\u0026quot;, 这个default-runtime需要加上才可以在k8s中找到设备。\n加了之后重启docker， 使用\ndocker run --security-opt=no-new-privileges --cap-drop=ALL --network=none -it -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins nvidia/k8s-device-plugin:1.11 如果成功的话会出现\n2023/01/09 12:56:36 Loading NVML\r2023/01/09 12:56:36 Fetching devices.\r2023/01/09 12:56:37 Starting FS watcher.\r2023/01/09 12:56:37 Starting OS watcher.\r2023/01/09 12:56:37 Starting to serve on /var/lib/kubelet/device-plugins/nvidia.sock\r2023/01/09 12:56:37 Registered device plugin with Kubelet\r如果不加的话，使用如下命令也可以验证docker是否可以使用GPU\ndocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi 等待几秒，下载完成后出现会以下结果\n# docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\rMon Jan 9 12:33:59 2023 +-----------------------------------------------------------------------------+\r| NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 |\r|-------------------------------+----------------------+----------------------+\r| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r| | | MIG M. |\r|===============================+======================+======================|\r| 0 Tesla V100-PCIE... Off | 00000000:02:00.0 Off | 0 |\r| N/A 28C P0 24W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r| 1 Tesla V100-PCIE... Off | 00000000:03:00.0 Off | 0 |\r| N/A 29C P0 25W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r| 2 Tesla V100-PCIE... Off | 00000000:82:00.0 Off | 0 |\r| N/A 30C P0 24W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r| 3 Tesla V100-PCIE... Off | 00000000:83:00.0 Off | 0 |\r| N/A 28C P0 23W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r+-----------------------------------------------------------------------------+\r| Processes: |\r| GPU GI CI PID Type Process name GPU Memory |\r| ID ID Usage |\r|=============================================================================|\r| No running processes found |\r+-----------------------------------------------------------------------------+\r在k8s上启动nvidia-device-plugin 在Master节点上运行\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml 然后使用以下命令查看\nkubectl get pods -A -owide 可以发现， 相关的pod已经起来了，对应每个修改了docker运行时的GPU节点都有一个pod\nkube-system nvidia-device-plugin-daemonset-2bhzz 1/1 Running 0 59m\rkube-system nvidia-device-plugin-daemonset-l5djd 1/1 Running 0 59m\rkube-system nvidia-device-plugin-daemonset-lt756 1/1 Running 0 60m\r使用\nkubectl describe nodes\r可以发现可分配资源里出现了nvidia.com/gpu的字段\nCapacity:\rcpu: 56\rephemeral-storage: 2148327Mi\rhugepages-1Gi: 0\rhugepages-2Mi: 0\rmemory: 65678476Ki\rnvidia.com/gpu: 4\rpods: 110\rAllocatable:\rcpu: 56\rephemeral-storage: 2027415715761\rhugepages-1Gi: 0\rhugepages-2Mi: 0\rmemory: 65576076Ki\rnvidia.com/gpu: 4\rpods: 110\rSystem Info:\r测试 我们使用官方文档里的测试方法\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\rapiVersion: v1\rkind: Pod\rmetadata:\rname: gpu-pod\rspec:\rrestartPolicy: Never\rcontainers:\r- name: cuda-container\rimage: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\rresources:\rlimits:\rnvidia.com/gpu: 1 # requesting 1 GPU\rtolerations:\r- key: nvidia.com/gpu\roperator: Exists\reffect: NoSchedule\rEOF\r可以得到\n# kubectl logs gpu-pod [Vector addition of 50000 elements]\rCopy input data from the host memory to the CUDA device\rCUDA kernel launch with 196 blocks of 256 threads\rCopy output data from the CUDA device to the host memory\rTest PASSED\rDone\r遇到的问题  Failed to initialize NVML: could not load NVML library.If this is a GPU node, did you set the docker default runtime to nvidia?\n 你可能没有设置`\u0026ldquo;default-runtime\u0026rdquo;: \u0026ldquo;nvidia\u0026quot;字段， 设置之后重启docker， 将pod全部删除重启，一定要注意操作顺序， 安装docker， 配置/etc/docker/daemon.json， 重启docker， 启动k8s-device-plugin, 进行测试。\nkubectl delete -n kube-system \u0026lt;POD NAME\u0026gt;\r如果需要给不同型号GPU打Label # Label your nodes with the accelerator type they have.\rkubectl label nodes \u0026lt;node-with-k80\u0026gt; accelerator=nvidia-tesla-k80\rkubectl label nodes \u0026lt;node-with-p100\u0026gt; accelerator=nvidia-tesla-p100\r调用的时候， 指定nodeSelector条件\napiVersion: v1\rkind: Pod\rmetadata:\rname: cuda-vector-add\rspec:\rrestartPolicy: OnFailure\rcontainers:\r- name: cuda-vector-add\r# https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile\rimage: \u0026quot;k8s.gcr.io/cuda-vector-add:v0.1\u0026quot;\rresources:\rlimits:\rnvidia.com/gpu: 1\rnodeSelector:\raccelerator: nvidia-tesla-p100 # or nvidia-tesla-k80 etc.\r","date":"2023-01-09T20:06:52+08:00","permalink":"https://tweakzx.github.io/p/kubernetes%E5%A6%82%E4%BD%95%E8%AE%A9kubernetes%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8gpu/","title":"【Kubernetes】如何让kubernetes集群使用GPU"},{"content":"基于深度学习的机器学习集群的任务放置 这是一篇发表在INFOCOM'2019上的论文， 特点是使用了强化学习进行任务放置。\nAbstract   背景\n 虽然作业之间的服务器共享提高了资源利用率，但位于ML 作业之间的干扰可能会导致性能显着下降。 现有的集群调度程序（例如，Mesos）在其作业布置中是忽视干扰的，导致资源效率不佳。 干扰感知工作安置已在文献中进行了研究，但使用详细的工作负载分析和干扰建模进行了处理，这不是通用的解决方案。    Harmony\n 这是一种深度学习驱动的 ML 集群调度程序，它以最小化干扰和最大化性能（即训练完成时间）的方式放置训练作业。 Harmony 是基于一个精心设计的深度强化学习(DRL)框架，并辅以奖励建模。 DRL 采用了最先进的技术来稳定训练和提高收敛性，包括行为者-批评算法、任务感知行为空间探索和经验重放。 鉴于普遍缺乏对应于不同放置决策的奖励样本，我们建立了一个辅助奖励预测模型，该模型使用历史样本进行训练，用于为看不见的放置产生奖励。    实验\n 在 6 台 GPU 服务器的 Kubernetes 集群中使用真实 ML 工作负载进行的实验 Harmony 在平均作业完成时间方面优于代表性调度程序 25%。    Introduction   任务干扰来源\n 许多现有的集群调度器倾向于超额分配，比如 CPU 和内存等资源，以最大化资源利用率(假设并非所有作业在任何时候都完全使用所需的资源)。 作业还共享底层资源，如 CPU 缓存、磁盘 I/O、网络 I/O 和总线(例如 QPI、 PCIe)。  例如，当服务器上的图形处理器卡被分配到不同的机器学习作业时，当作业在它们分配的 CPU 和图形处理器之间洗牌数据时，PCIe 总线被共享; 当两个分配的图形处理器没有连接到同一个非均匀访存模型体系结构中的 CPU 时，QPI 总线被共享。      不同类型任务之间的资源争用\n 一些机器学习任务是 CPU 密集型的，例如 CTC [7] ; 一些是磁盘 I/O 密集型的，例如 AlexNet [8] ，因为需要读取图像进行预处理; 还有一些由于模型大小(参数数量)和小批量(导致工人之间更频繁的参数交换) ，例如 VGG-16[9] ，网络带宽消耗水平很高。    将干扰程度较低的工作放在一起以优化性能是一个自然的想法。\n  现有调度器很大程度上是忽视干扰，这主要是由于难以获得许多作业的潜在干扰级别。\n  考虑干扰的工作\n 许多工作展示了干扰感知调度的潜力和有效性  考虑 MapReduce 作业中的网络争用 HPC 作业的缓存访问强度。   这些研究基于某些观察/假设建立了目标性能的显式干扰模型，并依靠手工设计的启发式方法将干扰纳入调度。 他们通常需要在数十个干扰源下进行详细的应用程序分析，并相应地仔细优化性能模型中的系数或启发式中的阈值。    痛点：通用性是这些白盒方法的一个问题：当工作负载类型或硬件配置发生变化时，启发式方法可能无法正常工作。\n    Harmony：一种黑盒方法，考虑干扰的同时，无需详细的性能建模\n 发现了ML工作负载之间共享资源时严重的性能下降， 提出了使用DRL来调度工作负载，自适应变化 采用了很多训练技术以确保DRL收敛到合适的放置策略， 包括actor-critic algorithm, job-aware action space exploration， experience replay。 建立了一个辅助的奖励预测模型， 用于为看不见的放置产生奖励。 在k8s上实现了原型， 实验评估发现， harmony比普遍策略性能高出25%    系统概述  提交一个ML任务，用户需要提交以下信息：    运行worker和PS的资源需求    使用的worker和PS的数量    训练数据集的epoch数     每个作业的worker和参数服务器的布置在整个训练过程中都不会改变  \rimage-20230104120024756\r\n 工作流程：  提交job，使用DRL模型， 得到放置决策 收集放置任务后的Trace， 进行有监督学习，预测奖励，得到奖励预测模型 使用奖励预测模型生成训练样本， 结合一系列State， 通过决策训练网络得到放置决策， 形成反馈 定期更新DRL模型    离线训练 使用纯粹的在线训练效果很差， 所以需要提前进行离线训练。离线训练共分为两步\n 奖励模型训练  通过历史工作轨迹，Harmony 使用监督学习训练奖励预测神经网络。 输入包括作业集信息和放置状态；标签是每项工作的奖励（训练速度）。 该模型通过相应的安置决策为任何工作集提供快速奖励评估。   DRL模型训练  DRL NN 将各种作业集和集群资源可用性作为输入，并为该集中的新作业生成放置决策。 通过奖励预测模型，我们可以有效地扩展了可用trace set并为 DRL 训练生成足够的样本。    在线推理和模型更新  在每个调度间隔中，Harmony 通过对 DRL NN 的推理来决定新作业批次的放置，并观察与放置决策相对应的实际奖励。 我们使用在线收集的样本定期重新训练 DRL NN 和奖励 NN，以随着时间的推移不断改进决策。  基于深度强化学习的任务放置 DRL框架  状态空间 s = (x, r, w, p, v, d)  x：是任务n模型的独热编码。具有相同架构和批量大小的DNN被认为是同一种模型。 r：是一个2(1+K)维的向量，编码worker和PS的资源需求。其中K是组成一个worker或PS的资源类型的数量。第一个值代表任务n需要的worker的数量，下K个值表示每个worker的K种资源的需求，剩余的1+K个值表示PS的数量及其资源需求。对于All-Reduce架构来说，将PS的资源需求置为0即可。 w, p：一个整数，表示分配给任务n的worker和PS的数量。 v：一个M*K的矩阵，表示每个服务器上每种资源的可用数量，M是物理服务器的数量 d：一个M*2N大小的向量，编码任务n的worker和PS在服务器上的放置。   动作空间：依据s和policy $\\pi(s, a)$ 选择相应的动作a  使用一个神经网络进行policy的训练 空间大小为2MN', N\u0026rsquo;是新到的任务数 (n, 0, m)表示服务器m给任务n分配一个worker (n, 1, m,)表示分配一个PS   奖励  目标是最小化平均任务完成时间 但是任务完成时间需要完成任务才知道 所以使用的是速度的求和， cn表示间隔内的已训练epoch， en是设定的总的epoch \rimage-20230104124524937\r   NN Model  \rimage-20230104120548893\r 每个作业或每个服务器的状态分别连接到一个全连接层，然后在输出层之前连接到几个全连接层。 这样，NN 可以从每个作业或每个服务器中提取特征，然后再合并为一个整体。 为了尊重服务器资源容量，在 NN 的输出层，我们通过在策略分布中将其概率设置为 0 来屏蔽无效操作，然后我们重新调整所有动作的概率，使总和仍然等于 1。    DRL模型训练  我们使用强化学习来训练策略NN， 每一个样本都是一个四元组(s, a, r, s')。（状态， 动作， 奖励， 更新后的状态）。 目标：最大化累计折扣奖励  \rimage-20230104125552347\r   训练技巧    Actor-critic：基本思想是引入依赖于状态的基线函数，以改进 SGD 中用于更新策略 NN 的梯度。    Exploration：确保充分探索行动空间， 以获得良好的Policy    Experience replay: 使用连续样本训练 RL 模型很难收敛，采用经验回放来减轻样本序列中的相关性。      奖励预测模型 我们设计了一个奖励模型，可以预测给定作业和集群状态的奖励，我们可以基于该模型为 DRL 训练生成大量样本。\n\rimage-20230104130738864\r\n NN 架构  NN 的输入状态是 DRL NN 输入的子集：(x, w, p, d)。 输出的是预测的任务训练速度 并发作业中 worker 和 PS 的资源需求不包括在内，因为它们通常可以从作业的模型类型中推断出来。输出是一个向量，包括输入作业的预测训练速度输入状态连接到输出层之前的一系列隐藏的全连接层。 在实践中，我们发现与更复杂的神经层相比，全连接层在我们的场景中工作得很好。   NN 训练  我们通过使用历史痕迹中的可用样本进行监督学习来训练 NN。 我们通过计算预测和标签的相对误差，将神经网络产生的每个作业 n 的预测训练速度 cn 与标签 c′n，即轨迹中每个作业 n 的训练速度进行比较。 然后我们使用 SGD 更新 NN 中的参数以最小化整体相对误差。 我们使用历史轨迹中的样本迭代地训练神经网络，使得神经网络产生的预测收敛于可接受的相对误差（例如 10%）。    ","date":"2023-01-04T10:16:46+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202301041018146.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0harmony%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Harmony论文阅读笔记"},{"content":"Tiresias 论文阅读笔记 Abstract  深度学习 (DL) 训练作业给现有的集群管理器带来了一些独特的挑战，例如  不可预测的训练时间 全有或全无的执行模型 GPU 共享的不灵活性   我们对生产中的大型 GPU 集群的分析表明，现有的大数据调度程序会导致  较长的排队延迟 较低的整体性能   我们介绍了 Tiresias  这是一个为分布式 DL 训练作业量身定制的 GPU 集群管理器，它可以有效地安排和放置 DL 作业以减少它们的作业完成时间 (JCT)。 鉴于 DL 作业的执行时间通常是不可预测的，我们提出了两种调度算法——  离散化二维Gittins索引：基于部分信息 离散化二维 LAS： 与信息无关，旨在最小化平均 JCT   此外，我们描述了何时可以放宽合并放置约束，并提出了一种放置算法来利用这些观察结果而无需任何用户输入。   在具有 60 个 P100 GPU 的密歇根 ConFlux 集群上进行的实验和大规模跟踪驱动模拟表明，  与生产中使用的基于 Apache YARN 的资源管理器相比，Tiresias 将平均 JCT 提高了 5.5 倍。 更重要的是，Tiresias 的性能与假设完美知识的解决方案的性能相当。    Introduction 由于 DDL 训练的独特限制，我们观察到当前集群管理器设计中的两个主要限制。\n 训练时间不可预知的朴素调度  尽管已知最短作业优先 (SJF) 和最短剩余时间优先 (SRTF) 算法可以最小化平均 JCT ，但它们需要作业的剩余执行时间，而这对于 DL 训练作业来说通常是未知的. Optimus可以依靠其重复执行模式并假设其损失曲线会收敛来预测 DL 训练作业的剩余执行时间。然而，这些提议对具有平滑损失曲线和运行完成的工作做出了过于简化的假设；在生产系统中，两者都不总是正确的。 正因为如此，生产中最先进的资源管理器相当天真。例如，微软内部的解决方案是从 Apache YARN 最初为大数据作业构建的 Capacity Scheduler 扩展而来的。它只执行基本的编排，即作业到达时的非抢占式调度。因此，当集群超额订阅时，用户经常会遇到长时间的排队延迟——即使是小作业也可能长达数小时。   放置任务过程中过度激进的合并  现有的集群管理器还试图将 DDL 作业整合到具有足够 GPU 的最少数量的服务器上。例如，一个有 16 个 GPU 的作业在每台服务器 4 个 GPU 的集群中至少需要四台服务器，如果找不到四台完全空闲的服务器，作业可能会被阻塞。 基本假设是应尽可能避免使用网络通信，因为它可能成为瓶颈并浪费 GPU 周期 。然而，我们发现这个假设只是部分有效。    在本文中，我们提出了 Tiresias，一种共享 GPU 集群管理器，旨在解决上述有关 DDL 作业调度和放置的挑战。为确保Tiresias实用且易于部署，我们依靠对生产作业轨迹的分析、对训练各种 DL 模型的详细测量以及两个简单而有效的想法。此外，我们有意让 Tiresias 对用户保持透明，即所有现有作业都可以在没有任何额外的用户指定配置的情况下运行。\n 第一个想法：一个新的调度框架（2DAS），旨在在 DL 作业的执行时间不可预测时最小化 JCT  调度算法  Discretized 2D Gittins index Discretized 2D-LAS   上述两个方案都是给任务一个优先级，前者使用Gittins索引，后者直接应用收到的任务(随着时间会改变，任务会按照优先级来进行调度) 使用上述策略有两个挑战  计算任务优先级的时候。需要同时考虑空间和时间两个维度。空间：GPU个数， 时间：任务运行时间。 相对优先级可能会随着工作接受服务二发生变化， 可能会导致工作被抢占， GPU抢占DDL工作的代价昂贵， 所以为了避免抢占， 作业的优先级需要在固定的时间间隔发生变化。   有先验的情况下使用Gittins索引， 没有先验的情况下使用LAS   第二个想法：尽可能使用模型结构来放松合并放置约束  我们观察到只有某些类型的 DL 模型对其是否合并敏感，并且它们的敏感性是由于其模型中张量大小分布的偏差。我们使用这种洞察将工作分为两类：  对整合敏感（高偏差）的工作 其他工作   我们在 Tiresias 中实现了一个 RDMA 网络分析库，它可以通过网络级活动确定 DDL 作业的模型结构。  通过利用分析库和 DDL 训练的迭代特性，Tiresias 可以透明且智能地放置作业。 Tiresias 首先在试用环境中运行作业几次迭代，然后根据从先前测量中总结的标准确定最佳放置策略。      本文的贡献如下：\n Tiresias是第一个信息不可知的GPU集群资源管理器。同时也是第一个使用了两个维度的扩展和优先级来调度DDL工作。 使用了一个简单的， 外部可观察的， model-specific的评价标准来判断什么时候放松GPU合并的约束 设计简单，易部署， 性能显著提高  背景和动机 DDL分布式深度学习 这里只关注数据并行\n\rimage-20221227122558146\r\n 周期性迭代 参数服务器架构 Trial and error 的探索： 超参数调优  挑战  不可预知的作业时间  当前预测 DL 作业训练时间的解决方案 都假设 DL 作业  (1) 具有平滑的损失曲线 (2) 达到其训练目标并完成   然而  对于许多在试错探索过程中较差的模型，它们的损失曲线并不像探索结束时最终选择的最佳模型的曲线那样平滑。如图2  \rimage-20221227123048934\r   DL的终止条件是不确定的   因此，实际的资源管理器设计不应依赖于准确性/损失曲线来预测最终的作业完成时间。   过度激进的任务合并  在模型聚集阶段尝试减少网络的通信在分布式训练中是一种通用的优化，因为网络可能是性能瓶颈并且浪费GPU周期。 然而，许多现存的GPU管理在放置分布式深度学习任务时盲目地遵从一个合并约束，特别地，他们将作业的所有组件（参数服务器和Worker）分配给相同或最小数量的服务器 一个分布式深度学习作业如果不能合并通常会等待，即使集群中有足够的空闲资源，虽然这个约束是为了高性能，但是会导致更长的队列延迟和低的资源利用。   抢占式的时间开销  由于时间开销大，目前的生产集群并没有抢占作业。 \rimage-20221227124028866\r \rimage-20221227124044585\r    潜在的收益 破除两个谬误， 可以获得巨大的提升\n 谬误一：如果没有确切的工作持续时间，就无法很好地安排工作。  尽管 DDL 作业持续时间通常是不可预测的，但可以从历史日志中了解它们的总体分布。 广泛用于解决经典多臂老虎机问题的 Gittins 指数策略，只要给定工作持续时间分布，就可以降低平均 JCT。 即使没有这些信息，LAS 算法也可以根据获得的服务有效地安排作业。   谬误二：DDL 作业应该始终合并。  虽然合并放置作业确实可以最大限度地减少其通信时间，但我们发现某些 DDL 作业对放置不敏感。 我们确定核心因素是模型结构    Tiresias 的设计 总体架构  调度目标  用户为中心：最小化平均JCT 操作者为中心：提高GPU的利用率 平衡以上二者：任务不能无限饥饿   一些假设  任务在线达到 任务持续时间未知 任务的任务特性未知 All or Nothing的资源分配   任务的生命周期  Tiresias 旨在优化上述目标，而无需在特定 DL 框架下对作业的资源需求、持续时间或其内部特征做出任何假设。 \rimage-20221227125201039\r 图 6 展示了 Tiresias 的架构以及作业生命周期中发生的一系列操作。  1）作业一提交，其 GPU 要求就已知，并且它被附加到 WAITQUEUE 。 2）调度程序定期调度来自 WAITQUEUE 的作业，并在作业到达、作业完成和资源可用性变化等事件时抢占集群中正在运行的作业到 WAITQUEUE。 3）首次启动作业或恢复先前抢占的作业时，调度程序依赖于放置模块来分配其 GPU。 4）如果一个作业是第一次启动，放置模块首先对其进行剖析——剖析器识别作业特定的特征，例如张量分布中的偏斜，以确定是否合并该作业 。      调度 我们观察到抢占式调度对于实现调度目标来说是必要的。\n为什么是二维调度 通过回顾基于时间或大小的启发式方法，我们认为在 GPU 资源有限的集群上调度 DDL 作业时仅考虑一个方面（空间或时间）是不够的。\n 在 SRTF 调度程序中，剩余时间短的大型作业会占用许多 GPU，从而导致许多新提交的小型作业出现不可忽略的排队延迟。 如果调度程序是GPU 的数量最小优先SF的，那么大型作业可能会被一连串的小型作业阻塞，即使它们接近完成  \rimage-20221227130428691\r\n2DAS 二维的获得性基于服务的调度器 2DAS是对传统LAS的扩展，同时考虑了时间和空间，它会赋予任务一个优先级，这个优先级和时间以及空间有关。\n而这个优先级函数有不同的情况，当有没有先验知识的时候，即没有任务持续时间的分布的时候，使用LAS算法，如果有先验分布，则使用Gittins索引\n\rimage-20221227132002748\r\n优先级的离散化 使用连续的优先级会导致一系列的抢占和一系列的重启，对于分布式深度学习任务来说，抢占和重启的代价很高，过多的抢占会导致2DAS不可用\n为了解决这个问题，基于传统的**多级别反馈队列(MLFQ)**算法实现优先级离散化的框架。即，将原有的一个队列变成K个队列。\n 整体结构确保具有相似 (WJtJ) 值的作业保留在同一队列中。具有高度不同 (WJtJ) 值的作业保持在不同的优先级。 使用 LAS 时，同一队列中的作业按其开始时间的 FIFO 顺序进行调度（即，首次调度它们的时间），没有任何 HOL 阻塞的风险。 Gittins 指数中的服务量 Δ 也是离散化的。对于 Qi 中的作业，Δi 等于 Qhi i，这是 Qi 的上限。当一个作业用完它的所有服务量时，它将被降级到较低优先级的队列。对于 Gittins 索引，同一队列中的作业根据其 Gittins 索引值进行调度。在最后一个队列 QK 中，ΔK 被设置为 ∞。在这种极端情况下，Gittins 索引的表现与 LAS 类似，最后一个队列中的作业按照 FIFO 顺序进行调度。  \rimage-20221227131451335\r\n放置 给定一个任务，需要参数服务器以及Worker，如果有足够的资源，Tiresias需要知道是否在尽可能少的机器中合并一些任务的GPU或者去分发它们，前者在微软的的生产集群中实现，故一个任务即使资源足够也可能被放置在等待队列。本文使用ILP，即整数线性规划来优化这个分配问题。\n合并对于任务来说重要吗? 深度学习模型中对于合并敏感的一般都有较大的张量， 原因是模型聚合中的消息大小与模型的结构密切相关。 例如，TensorFlow中的模型由许多张量组成。 每个张量都被包装为单个通信消息。因此，DDL中的消息大小分布取决于模型的张量大小分布。 张量大小通常分布不均匀; 有时存在巨大的张量，其中包含这些模型中的大部分参数。 因此，聚合较大的张量会更严重地受到网络争用的影响，而较小张量的传输往往会相互交错。\n利用这个直觉，设计了Tiresias的分析器，用于分析每个模型的偏差程度，再使用放置的算法 \rimage-20221227132216616\r\n总结  与 Apache YARN 的容量调度程序 (YARNCS) 和 Gandiva 相比，Tiresias 旨在最小化平均 JCT。 与 Optimus 不同，Tiresias 可以在没有或有部分先验知识的情况下有效地安排工作（表 2）。 此外，Tiresias 可以根据 Tiresias 分析器自动捕获的模型结构巧妙地放置 DDL 作业。  实现 中心Master 除了启动新作业和完成现有作业外，master 的一个主要功能是当它们的（GPU）资源被调度程序分配给其他作业时抢占正在运行的作业。\n由于 DL 作业的迭代性质，我们不需要将所有数据保存在 GPU 和主内存中以进行作业抢占。\n目前，我们使用几乎所有 DL 框架提供的检查点功能来为抢占作业保存最新模型。\n当触发抢占时，作业首先被暂停；然后它的首席工作者将它的模型检查点到一个集群范围的共享文件系统。\n当调度程序再次恢复暂停的作业时，将在重新启动之前加载其最近的检查点。中央主机还使用放置算法和分析器确定作业的放置。\n分布式RDMA监控 由于 RDMA 在 GPU 集群中广泛用于 DDL 作业，我们将分析器实现为可加载库，用于拦截 RDMA ibverbs API。\n因此，它可以记录每个服务器上的所有 RDMA 活动，例如建立连接、发送和接收数据。所有相关工作人员和参数服务器的 RDMA 级信息随后在中央分析器中聚合。\n基于聚合信息（例如，消息大小和总流量），Tiresias 可以解析给定 DDL 作业的详细模型信息，包括其偏差。\n虽然是为 RDMA 网络实现的，但分析器可以通过拦截套接字 API 轻松扩展以支持 TCP/IP 网络。\n","date":"2022-12-27T09:46:16+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212270951184.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0tiresias%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Tiresias论文阅读笔记"},{"content":"Tensorflow kernal launch 的过程 分析session执行的过程， 并分析Antman对执行过程的修改\n函数调用链 Run()\u0026ndash;\u0026gt;RunInternel()\u0026ndash;\u0026gt;RunAsync()\u0026ndash;\u0026gt;ScheduleReady()\u0026ndash;\u0026gt;Process()\n修改了direct_session.cc , 在session执行前后运行中间件框架\n修改了executor.cc ， 新增一个异步调用队列， 并将需要插入时间槽的异步Op加入队列，在OpManager线程中等待执行。\n\rimg\r\nSession的执行 Session的代码逻辑在TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/direct_session.cc的Run()函数中，\nDirectSession::Run Status DirectSession::Run(const RunOptions\u0026amp; run_options, const NamedTensorList\u0026amp; inputs, const std::vector\u0026lt;string\u0026gt;\u0026amp; output_names, const std::vector\u0026lt;string\u0026gt;\u0026amp; target_nodes, std::vector\u0026lt;Tensor\u0026gt;* outputs, RunMetadata* run_metadata) { //判断计算图是否构建  TF_RETURN_IF_ERROR(CheckNotClosed()); TF_RETURN_IF_ERROR(CheckGraphCreated(\u0026#34;Run()\u0026#34;)); //计数器  direct_session_runs-\u0026gt;GetCell()-\u0026gt;IncrementBy(1); // Extract the inputs names for this run of the session.  //提取输入的张量名字和张量大小  std::vector\u0026lt;string\u0026gt; input_tensor_names; input_tensor_names.reserve(inputs.size()); size_t input_size = 0; for (const auto\u0026amp; it : inputs) { input_tensor_names.push_back(it.first); input_size += it.second.AllocatedBytes(); } metrics::RecordGraphInputTensors(input_size); // Check if we already have an executor for these arguments.  // 检查是否已经创建执行器， 没有的话， 创建  // 一般情况下 每个设备都有一个执行器， 负责这个设备上计算子图的执行  ExecutorsAndKeys* executors_and_keys; RunStateArgs run_state_args(run_options.debug_options()); run_state_args.collective_graph_key = run_options.experimental().collective_graph_key(); TF_RETURN_IF_ERROR(GetOrCreateExecutors(input_tensor_names, output_names, target_nodes, \u0026amp;executors_and_keys, \u0026amp;run_state_args)); { mutex_lock l(collective_graph_key_lock_); collective_graph_key_ = executors_and_keys-\u0026gt;collective_graph_key; } // Configure a call frame for the step, which we use to feed and  // fetch values to and from the executors.  //设置函数调用帧的参数， Tensorflow使用feed和fetch字典来和执行器进行数据交互  //feed是输入， fetch是输出  //构建FunctionCallFrame call_frame, Session与执行器之间相互交互  //处理执行器的输入与输出  FunctionCallFrame call_frame(executors_and_keys-\u0026gt;input_types, executors_and_keys-\u0026gt;output_types); gtl::InlinedVector\u0026lt;Tensor, 4\u0026gt; feed_args(inputs.size()); for (const auto\u0026amp; it : inputs) { if (it.second.dtype() == DT_RESOURCE) { Tensor tensor_from_handle; TF_RETURN_IF_ERROR( ResourceHandleToInputTensor(it.second, \u0026amp;tensor_from_handle)); feed_args[executors_and_keys-\u0026gt;input_name_to_index[it.first]] = tensor_from_handle; } else { feed_args[executors_and_keys-\u0026gt;input_name_to_index[it.first]] = it.second; } } // 设置输入参数  const Status s = call_frame.SetArgs(feed_args); if (errors::IsInternal(s)) { return errors::InvalidArgument(s.error_message()); } else if (!s.ok()) { return s; } const int64 step_id = step_id_counter_.fetch_add(1); if (LogMemory::IsEnabled()) { LogMemory::RecordStep(step_id, run_state_args.handle); } //准备好执行环境之后， 开始调用RunInternal执行计算  TF_RETURN_IF_ERROR(RunInternal(step_id, run_options, \u0026amp;call_frame, executors_and_keys, run_metadata, thread::ThreadPoolOptions())); ... ... // 获取并处理计算图的执行结果 } DirectSession::RunInternal() // RunInternal会启动多个并行的执行器， // 创建执行器的barrier， 确保执行器都执行完， 执行完后返回Run()函数 Status DirectSession::RunInternal( int64 step_id, const RunOptions\u0026amp; run_options, CallFrameInterface* call_frame, ExecutorsAndKeys* executors_and_keys, RunMetadata* run_metadata, const thread::ThreadPoolOptions\u0026amp; threadpool_options) { const uint64 start_time_usecs = options_.env-\u0026gt;NowMicros(); const int64 executor_step_count = executors_and_keys-\u0026gt;step_count.fetch_add(1); ////////////////////////////////////////////////////////  // Running all pre session run action in grouping //  // 在session计算执行之前添加SessionRunActionRegistry //  // 以运行在session开始之前的中间件 //  ////////////////////////////////////////////////////////  SessionRunActionOptions action_options; action_options.device_mgr = \u0026amp;device_mgr_; action_options.sess_ptr = this; TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::PRE_SESSION_RUN, action_options)); //  //  //标记运行状态  RunState run_state(step_id, \u0026amp;devices_); ... ... // profiler TraceMe  //构建 IntraProcessRendezvous 用于本地Tensor管理  run_state.rendez = new IntraProcessRendezvous(device_mgr_.get()); ... ... // ifndef _ANDROID  // Start parallel Executors.  //开始并行执行器  //构建 ExecutorBarrier 用于协调多个 Executor 并行计算，保持 graph 一致性  const size_t num_executors = executors_and_keys-\u0026gt;items.size(); ExecutorBarrier* barrier = new ExecutorBarrier( num_executors, run_state.rendez, [\u0026amp;run_state](const Status\u0026amp; ret) { { mutex_lock l(run_state.mu_); run_state.status.Update(ret); } run_state.executors_done.Notify(); }); ... ... //构建args  // Register this step with session\u0026#39;s cancellation manager, so that  // `Session::Close()` will cancel the step.  ... ...//处理`Session::Close()`  // Use std::unique_ptr to ensure garbage collection  //创建线程池实际运行执行器  std::unique_ptr\u0026lt;thread::ThreadPool\u0026gt; threadpool_wrapper; thread::ThreadPool* pool = nullptr; ...//设置线程池  //异步启动执行器  for (const auto\u0026amp; item : executors_and_keys-\u0026gt;items) { thread::ThreadPool* device_thread_pool = item.device-\u0026gt;tensorflow_device_thread_pool(); if (!device_thread_pool) { args.runner = default_runner; } else { args.runner = [this, device_thread_pool](Executor::Args::Closure c) { device_thread_pool-\u0026gt;Schedule(std::move(c)); }; } if (handler != nullptr) { args.user_intra_op_threadpool = handler-\u0026gt;AsIntraThreadPoolInterface(); } /////////////// 执行器的启动///////////////////  item.executor-\u0026gt;RunAsync(args, barrier-\u0026gt;Get()); /////////////////////////////////////////////  } //等待执行结果  WaitForNotification(\u0026amp;run_state, \u0026amp;step_cancellation_manager, run_options.timeout_in_ms() \u0026gt; 0 ? run_options.timeout_in_ms() : operation_timeout_in_ms_); ... ... //保存运行结果  if (!run_state.tensor_store.empty()) { TF_RETURN_IF_ERROR(run_state.tensor_store.SaveTensors( {executors_and_keys-\u0026gt;callable_options.fetch().begin(), executors_and_keys-\u0026gt;callable_options.fetch().end()}, \u0026amp;session_state_)); } ... ... ///////////////////////////////////////////////////////////  // Running all post session run action in grouping //  // 在session计算执行结束之后添加SessionRunActionRegistry， //  // 以运行在session结束之后的中间件 //  ///////////////////////////////////////////////////////////  uint64 session_end_time = tensorflow::Env::Default()-\u0026gt;NowMicros(); action_options.sess_duration_us = time_duration_usecs; action_options.graph_id = reinterpret_cast\u0026lt;uint64\u0026gt;(executors_and_keys); TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::POST_SESSION_RUN, action_options)); return Status::OK(); } 执行器逻辑 ExecutorState::RunAsyn() # ExecutorState::RunAsync 的实现 # 概述：初始化ready队列， 开启线程池 void ExecutorState::RunAsync(Executor::DoneCallback done) { const Graph* graph = impl_-\u0026gt;graph_.get(); TaggedNodeSeq ready; // 获取 context map，即运行时上下文  Device* device = impl_-\u0026gt;params_.device; const Status fill_status = device-\u0026gt;FillContextMap(graph, \u0026amp;device_context_map_); if (!fill_status.ok()) { done(fill_status); return; } // 初始化 ready 队列，即存放入度为0的node  for (const Node* n : impl_-\u0026gt;root_nodes_) { DCHECK_EQ(n-\u0026gt;in_edges().size(), 0); ready.push_back(TaggedNode{n, root_frame_, 0, false}); } if (ready.empty()) { done(Status::OK()); } else { num_outstanding_ops_ = ready.size(); root_frame_-\u0026gt;iterations[0]-\u0026gt;outstanding_ops = ready.size(); done_cb_ = std::move(done); // 线程池入口  ScheduleReady(ready, nullptr); } } ExecutorState::ScheduleReady() # ExecutorState::ScheduleReady 的实现 # 概述：将节点分为 expensive \u0026amp; inexpensive 节点，将inexpensive节点放入 inline_ready 中 void ExecutorState::ScheduleReady(const TaggedNodeSeq\u0026amp; ready, TaggedNodeReadyQueue* inline_ready) { if (ready.empty()) return; int64 scheduled_usec = 0; if (stats_collector_) { scheduled_usec = nodestats::NowInUsec(); } if (inline_ready == nullptr) { // 运行所有 ready ops \t// 运行ready队列里的节点 ready是当前线程要处理的队列  for (auto\u0026amp; tagged_node : ready) { runner_([=]() { Process(tagged_node, scheduled_usec); }); } return; } // 将节点分类，运行 expensive node  const GraphView\u0026amp; gview = impl_-\u0026gt;gview_; const TaggedNode* curr_expensive_node = nullptr; for (auto\u0026amp; tagged_node : ready) { const NodeItem\u0026amp; item = *gview.node(tagged_node.node-\u0026gt;id()); if (tagged_node.is_dead || !item.kernel_is_expensive) { //  inline_ready-\u0026gt;push_back(tagged_node); } else { //对于高开销节点启动新的线程去执行  if (curr_expensive_node) { runner_(std::bind(\u0026amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec)); } curr_expensive_node = \u0026amp;tagged_node; } } if (curr_expensive_node) { //高开销节点  if (inline_ready-\u0026gt;empty()) { // inline_ready为空， 将首个高开销节点放入inline_ready  inline_ready-\u0026gt;push_back(*curr_expensive_node); } else { // inline_ready不为空， 将高开销节点放入其他线程中执行  runner_(std::bind(\u0026amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec)); } } ... ... } ExecutorState::Process() # ExecutorState::Process 详解 # 概述：线程池中跑的内容，代码太长不贴了。 # 主要流程： # + 将当前节点添加到 inline_ready 队列中。 # + 循环从 inline_ready 队列获取节点并运行，运行完毕后执行 NodeDone（有可能会添加新节点到inline_ready队列） # + 当inline ready队列为空时，跳出循环。 # 其他重要内容： # + 运行节点通过 device 的 ComputeAsync 或 Compute 方法 # + 处理输出结果使用 ProcessOutputs 函数和 PropagateOutputs 函数 # + 计算结束后通过 NodeDone 来收尾 void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) { WithContext wc(context_); ... ... // Parameters passed to OpKernel::Compute.  TensorValueVec inputs; DeviceContextVec input_device_contexts; AllocatorAttributeVec input_alloc_attrs; OpKernelContext::Params params; params.step_id = step_id_; // Override device\u0026#39;s threadpool if user provides an intra_op_threadpool  Device* device = impl_-\u0026gt;params_.device; ... ... bool completed = false; inline_ready.push_back(tagged_node); uint64 sess_op_num = 0; //循环处理inline_ready中的每个节点 直到为空  while (!IsAsyncGPUOpQueueEmpty() || !inline_ready.empty()) { tagged_node = inline_ready.front(); inline_ready.pop_front(); ... ... //准备输入数据， 确保输入是有效的  s = PrepareInputs(item, first_input, \u0026amp;inputs, \u0026amp;input_device_contexts, \u0026amp;input_alloc_attrs, \u0026amp;is_input_dead); ... ... // 绝大多数的Op是同步计算模式， send/recv是异步计算模式  if (item.kernel_is_async) { //异步计算, send/recv是高开销的  launched_asynchronously = true; Device* kernel_device = impl_-\u0026gt;params_.device; // Only enqueue this op if it is an async GPU op.  if (need_to_insert_idle_time_ \u0026amp;\u0026amp; (kernel_device-\u0026gt;name()).find(\u0026#34;GPU\u0026#34;) != string::npos) { //////////////////////////////////////////////////////////////////////  // 把这个GPU Op放入async_gpu_op_queue队列中 如果需要在它启动之前插入时间槽的话  // Enqueue this GPU op therefore we can insert a time slot before launching this op.  // 将原本执行异步计算代码的Op放入自定义的async_gpu_op_queue队列中，  // 交由OpManager执行  //////////////////////////////////////////////////////////////////////  sess_op_num++; ... ... // Enqueue this asyn GPU op.  async_gpu_op_queue_lock_.lock(); async_gpu_op_queue.emplace_back(async_gpu_kernel); num_queued_op.fetch_add(1); async_gpu_op_queue_lock_.unlock(); } else { /////////////////////////////////////////////////////////////////////  // 不需要插入时间槽， 所以不放入async_gpu_op_queue队列  // Do not enqueue this op.  // 调用原本的计算异步的函数  /////////////////////////////////////////////////////////////////////  device-\u0026gt;ComputeAsync(async, \u0026amp;state-\u0026gt;ctx, done); } else { // 同步计算  // Synchronous computes.  OpKernelContext ctx(\u0026amp;params, item.num_outputs); nodestats::SetOpStart(stats); ... ... //进行计算 deivce-\u0026gt;Compute(op_kernel, \u0026amp;ctx)  nodestats::SetOpEnd(stats); //处理输出  s = ProcessOutputs(item, \u0026amp;ctx, \u0026amp;outputs, stats); ... ... //传播输出  if (s.ok()) { PropagateOutputs(tagged_node, \u0026amp;item, \u0026amp;outputs, \u0026amp;ready); } ... ... //传播后处理  //结束  completed = NodeDone(s, item.node, ready, stats, \u0026amp;inline_ready); } } // while !inline_ready.empty()  if (sess_op_num \u0026gt; 0) { // Record the total number of the queued op running in this session.  GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm != nullptr) { rm-\u0026gt;SetExecutorQueuedOpNum(impl_, sess_op_num); } } // This thread of computation is done if completed = true.  if (completed) ScheduleFinish(); } ExecutorState::AsyncGPUOpManager() void ExecutorState::AsyncGPUOpManager() { uint64 sleep_time_us = 0; need_to_insert_idle_time_ = false; GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm == nullptr) { return; } while (!terminate_op_magager_thread_) { //设置队列中的Op是否需要插入时间槽  need_to_insert_idle_time_ = rm-\u0026gt;GetEstimatedIdleTime() \u0026gt; 0 ? true : false; std::function\u0026lt;void(void)\u0026gt; queued_call_func = nullptr; async_gpu_op_queue_lock_.lock(); if (!async_gpu_op_queue.empty()) { queued_call_func = async_gpu_op_queue.front(); } async_gpu_op_queue_lock_.unlock(); if (queued_call_func != nullptr) { queued_call_func(); async_gpu_op_queue_lock_.lock(); if (!async_gpu_op_queue.empty()) { async_gpu_op_queue.erase(async_gpu_op_queue.begin()); num_queued_op.fetch_sub(1); } async_gpu_op_queue_lock_.unlock(); // Estimate idle time  uint64 idle_time = rm-\u0026gt;GetEstimatedIdleTime(); uint64 queued_op_num = rm-\u0026gt;GetExecutorQueuedOpNum(impl_); idle_time = queued_op_num \u0026gt; 0 ? (idle_time / queued_op_num) : 0; usleep(idle_time); uint64 remain_time = rm-\u0026gt;GetEstimatedIdleTime(); remain_time = remain_time \u0026gt; idle_time ? (remain_time - idle_time) : 0; rm-\u0026gt;SetEstimatedIdleTime(remain_time); } usleep(default_check_interval); } return; } Antman对内存分配器的修改 主要新增了自己的vmen内存分配器， 调用host的内存\n在TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/gpu/gpu_process_state.cc#做了修改\nAllocator* GPUProcessState::GetGPUAllocator(const GPUOptions\u0026amp; options, TfGpuId tf_gpu_id, size_t total_bytes) { CHECK(process_state_); #if (defined(GOOGLE_CUDA) \u0026amp;\u0026amp; GOOGLE_CUDA) || \\ (defined(TENSORFLOW_USE_ROCM) \u0026amp;\u0026amp; TENSORFLOW_USE_ROCM)  const string\u0026amp; allocator_type = options.allocator_type(); mutex_lock lock(mu_); GpuIdUtil::CheckValidTfGpuId(tf_gpu_id); if (tf_gpu_id.value() \u0026gt;= static_cast\u0026lt;int64\u0026gt;(gpu_allocators_.size())) { gpu_allocators_.resize(tf_gpu_id.value() + 1); } AllocatorParts\u0026amp; allocator_parts = gpu_allocators_[tf_gpu_id.value()]; if (allocator_parts.allocator == nullptr) { // Validate allocator types.  if (!allocator_type.empty() \u0026amp;\u0026amp; allocator_type != \u0026#34;BFC\u0026#34;) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Invalid allocator type: \u0026#34; \u0026lt;\u0026lt; allocator_type; return nullptr; } PlatformGpuId platform_gpu_id; TF_CHECK_OK(GpuIdManager::TfToPlatformGpuId(tf_gpu_id, \u0026amp;platform_gpu_id)); int bus_id = BusIdForGPU(tf_gpu_id); DCHECK_GE(bus_id, 0); while (bus_id \u0026gt;= gpu_visitors_.size()) { gpu_visitors_.push_back({}); } se::StreamExecutor* stream_exec = GpuIdUtil::ExecutorForPlatformGpuId(platform_gpu_id).ValueOrDie(); GPUMemAllocator* sub_allocator = new GPUMemAllocator( stream_exec, platform_gpu_id, (options.per_process_gpu_memory_fraction() \u0026gt; 1.0 || options.experimental().use_unified_memory()), gpu_visitors_[bus_id], {}); GPUBFCAllocator* gpu_bfc_allocator = new GPUBFCAllocator(sub_allocator, total_bytes, options, strings::StrCat(\u0026#34;GPU_\u0026#34;, tf_gpu_id.value(), \u0026#34;_bfc\u0026#34;)); Allocator* gpu_allocator = gpu_bfc_allocator; // GPUVMemAllocator will allocate host memory as backup after running out of  // gpu device memory to avoid OOM failures  //////////////////////////////////////////////////////////////////////////////////  gpu_allocator = maybe_create_gpu_vmem_allocator(gpu_allocator, bus_id, platform_gpu_id, tf_gpu_id.value(), stream_exec); //////////////////////////////////////////////////////////////////////////////////  SharedCounter* timing_counter = nullptr; if (options.experimental().timestamped_allocator()) { timing_counter = new SharedCounter; gpu_bfc_allocator-\u0026gt;SetTimingCounter(timing_counter); } // If true, checks for memory overwrites by writing  // distinctive patterns on both ends of allocated memory.  if (useCudaMemoryGuardAllocator()) { gpu_allocator = new GPUDebugAllocator(gpu_allocator, platform_gpu_id); gpu_allocator = new GPUNanResetAllocator(gpu_allocator, platform_gpu_id); } else if (useCudaMallocAllocator()) { // If true, passes all allocation requests through to cudaMalloc  // useful for doing memory debugging with tools like cuda-memcheck  // **WARNING** probably will not work in a multi-gpu scenario  gpu_allocator = new GPUcudaMallocAllocator(gpu_allocator, platform_gpu_id); } Allocator* recording_allocator = nullptr; if (process_state_-\u0026gt;ProcessState::FLAGS_brain_gpu_record_mem_types) { ProcessState::MemDesc md; md.loc = ProcessState::MemDesc::GPU; md.dev_index = platform_gpu_id.value(); md.gpu_registered = false; md.nic_registered = true; recording_allocator = new internal::RecordingAllocator( \u0026amp;process_state_-\u0026gt;mem_desc_map_, gpu_allocator, md, \u0026amp;mu_); } allocator_parts = {std::unique_ptr\u0026lt;Allocator\u0026gt;(gpu_allocator), std::unique_ptr\u0026lt;SharedCounter\u0026gt;(timing_counter), gpu_bfc_allocator, sub_allocator, std::unique_ptr\u0026lt;Allocator\u0026gt;(recording_allocator)}; } if (process_state_-\u0026gt;ProcessState::FLAGS_brain_gpu_record_mem_types) { return allocator_parts.recording_allocator.get(); } else { return allocator_parts.allocator.get(); } #else  LOG(FATAL) \u0026lt;\u0026lt; \u0026#34;GPUAllocator unavailable. Not compiled with --config=cuda or \u0026#34; \u0026#34;--config=rocm.\u0026#34;; return nullptr; #endif // GOOGLE_CUDA || TENSORFLOW_USE_ROCM } gpu_process_state 是个单例模式, 只有一个实例存在\n/*static*/ GPUProcessState* GPUProcessState::singleton(GPUProcessState* ps) { static GPUProcessState* instance = ps ? ps : new GPUProcessState; DCHECK((!ps) || (ps == instance)) \u0026lt;\u0026lt; \u0026#34;Multiple calls to GPUProcessState with non-null ps\u0026#34;; return instance; } GPUProcessState::GPUProcessState() : gpu_device_enabled_(false) { process_state_ = ProcessState::singleton(); } ","date":"2022-12-23T18:57:14+08:00","permalink":"https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90tensorflow%E7%9A%84session%E6%89%A7%E8%A1%8C%E5%88%86%E6%9E%90/","title":"【代码分析】Tensorflow的session执行分析"},{"content":"Antman对Tensorflow的代码修改 总体的关系图，主要包括两个实现， 内存方面的GPUResourceManagement以及算力方面的GpuOpManager。\ngraph TD Agpu_resource_manage_file] B[SessionRunRegistry] C[SessionRunAction] D[Executor] E[GPUResouceManagement] F[GPU Statistic] G[GpuOpManager] H[GpuUsageAdjustment] I(dump gpu statistic) J[GPU Process State] K[GPUVMemAllocator] L[GPUAdjustableAllocator] A --|FileListener| E B --|Register| E E --|need_to_adjust_memory_| H H --|new| L H --|get| K C --|Derive| E C --|Derive| F B --|Register| F F --|need_to_dump_statistics_| I B --|Run| C J --|maybe_create_gpu_vmem_allocator|K D --|run thread| G E --|GetEstimatedIdleTime| G  GPUVMemAllocator GPUVMemAllocator 可以分配host的mem作为显存的备用，以免出现OOM错误。\n创建allocator maybe_create_gpu_vmem_allocator(\u0026hellip;)可以根据情况返回合适的allocator\ngraph TD A([start]) --|gpu_allocator|B[maybe_create_gpu_vmem_allocator]--C{if !gpu_vmem} C --|true| D([返回gpu_allocator]) C --|false|Z(准备生成VMemAllocator) Z -- E[new GpuHostAllocator] E --|sub_allocator| F[new BFCAllocator] Z --|gpu_allocator| G[new GPUVMemAllocator] F --|host_allocator|G G --|gpu_vmem_allocator|H([返回gpu_vmem_allocator])  Allocator* maybe_create_gpu_vmem_allocator(Allocator* gpu_allocator, int bus_id, PlatformGpuId platform_gpu_id, int tf_gpu_id, se::StreamExecutor* stream_exec) { bool gpu_vmem = false; Status status = ReadBoolFromEnvVar(\u0026#34;TF_GPU_VMEM\u0026#34;, true/*enabled by default*/, \u0026amp;gpu_vmem); if (!status.ok()) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;GetGPUAllocator: \u0026#34; \u0026lt;\u0026lt; status.error_message(); } if (!gpu_vmem) { return gpu_allocator; } SubAllocator* sub_allocator = new GpuHostAllocator( GpuIdUtil::ExecutorForPlatformGpuId(platform_gpu_id).ValueOrDie(), bus_id, {}, {}); int64 cuda_host_mem_limit_in_mb = -1; status = ReadInt64FromEnvVar(\u0026#34;TF_CUDA_HOST_MEM_LIMIT_IN_MB\u0026#34;, 1LL \u0026lt;\u0026lt; 16 /*64GB max by default*/, \u0026amp;cuda_host_mem_limit_in_mb); if (!status.ok()) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;GetGpuHostAllocator: \u0026#34; \u0026lt;\u0026lt; status.error_message(); } int64 cuda_host_mem_limit = cuda_host_mem_limit_in_mb * (1LL \u0026lt;\u0026lt; 20); Allocator* host_allocator = new BFCAllocator(sub_allocator, cuda_host_mem_limit, true /*allow_growth*/, strings::StrCat(\u0026#34;GPUHost_\u0026#34;, tf_gpu_id, \u0026#34;_bfc\u0026#34;)); Allocator* gpu_vmem_allocator = new GPUVMemAllocator(gpu_allocator, host_allocator, tf_gpu_id, stream_exec); return gpu_vmem_allocator; } 分配虚拟内存 先尝试分配GPU内存， 分配成功则返回， 分配失败则分配CPU内存。\nvoid* GPUVMemAllocator::AllocateRaw(size_t alignment, size_t num_bytes) { mutex_lock l(lock_); AllocationAttributes new_attr; // Tell the device_allocator_ not to retry  // since we can alloc host memory as backup  new_attr.no_retry_on_failure = true; void* ret = device_allocator_-\u0026gt;AllocateRaw(alignment, num_bytes, new_attr); if (ret != nullptr) { device_ptrs_.insert(ret); return ret; } ret = host_allocator_-\u0026gt;AllocateRaw(alignment, num_bytes); VLOG(3) \u0026lt;\u0026lt; \u0026#34;host_allocator_ allocates \u0026#34; \u0026lt;\u0026lt; (num_bytes/1024.0/1024) \u0026lt;\u0026lt; \u0026#34; MiB\u0026#34;; return ret; } SessionRunActionRegistry（中间件框架） 添加了一个SessionRunActionRegistry框架， 方便在session开始之前或者结束之后添加执行动作\n修改direct_session.cc 和 master_session.cc /修改了原先的session执行流程 在session执行前后分别执行actions\n// Running all pre session run action in grouping  SessionRunActionOptions action_options; action_options.device_mgr = \u0026amp;device_mgr_; action_options.sess_ptr = this; TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::PRE_SESSION_RUN, action_options)); ... const uint64 time_duration_usecs = options_.env-\u0026gt;NowMicros() - start_time_usecs; metrics::UpdateGraphExecTime(time_duration_usecs); // Running all post session run action in grouping  uint64 session_end_time = tensorflow::Env::Default()-\u0026gt;NowMicros(); action_options.sess_duration_us = time_duration_usecs; action_options.graph_id = reinterpret_cast\u0026lt;uint64\u0026gt;(executors_and_keys); TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::POST_SESSION_RUN, action_options)); 注册action void SessionRunActionRegistry::Register( Grouping grouping, int phase, SessionRunAction* action) { VLOG(2) \u0026lt;\u0026lt; \u0026#34;Register session run action \u0026#34; \u0026lt;\u0026lt; action-\u0026gt;name(); groups_[grouping][phase].emplace_back(action); } // 一些宏函数: 提供注册中间件的接口 #define REGISTER_SESSION_RUN_ACTION(grouping, phase, action) \\ REGISTER_ACTION_UNIQ_HELPER(__COUNTER__, grouping, phase, action)  #define REGISTER_ACTION_UNIQ_HELPER(ctr, grouping, phase, action) \\ REGISTER_ACTION_UNIQ(ctr, grouping, phase, action)  #define REGISTER_ACTION_UNIQ(ctr, grouping, phase, action) \\ static ::tensorflow::session_run_action_registration:: \\ SessionRunActionRegistration register_session_run_action_##ctr( \\ grouping, phase, new action(), \\ #action)  } // namespace tensorflow  #endif // TENSORFLOW_CORE_COMMON_RUNTIME_SESSION_RUN_ACTION_REGISTRY_H_  //定义了一个RunAction()的接口， Action 必须实现这个接口 class SessionRunAction { public: virtual ~SessionRunAction() {} virtual Status RunAction(const SessionRunActionOptions\u0026amp; options) = 0; void set_name(const string\u0026amp; name) { name_ = name; } std::string name() const { return name_; } private: // The name of the action, which is the same as the inherited  // class name.  string name_; }; GPU memory limit adjustment 实现动态资源分配， 自动调整现存限制， 当发现 Host 内存被使用的时候，会提高显存的限制阈值，这样所有的 Tensor 都可以申请在显卡上。这样只会影响一个 mini batch 的性能，后面的 mini batch 跑前向后向计算的时候，所有的 Tensor 都会被申请在显存上。\n修改了tensorflow 原本的 BFC_Allocator.h 增加了friend class GPUAdjustableAllocator;\n// Declare the GPUAdjustableAllocator to be friend of the BFCAllocator,  // therefore it can adjust the memory limit by modifying the private  // member variables of BFCAllocator.  friend class GPUAdjustableAllocator; 增加了扩缩显存分配的函数 class GPUAdjustableAllocator final { public: // Adjust the memory_limit_ to allow memory grow/shrink at runtime  // Returns adjusted memory_limit_. If the return value is less than  // the new_memory_limit, the adjustment failed.  size_t AdjustMemoryLimit(size_t new_memory_limit, BFCAllocator* bfc_allocator); // Get the memory pool size and in used memory size of the bfc_allocator.  void GetMemPoolStats(BFCAllocator* bfc_allocator, int64_t* deviceMemPoolSize, int64_t* deviceMemStable); private: // Free the memory regions that are not in use  size_t FreeEmptyMemory(size_t target_memory_bytes, BFCAllocator* bfc_allocator) EXCLUSIVE_LOCKS_REQUIRED(lock_); }; size_t GPUAdjustableAllocator::AdjustMemoryLimit(size_t new_memory_limit, BFCAllocator* bfc_allocator) { mutex_lock l(bfc_allocator-\u0026gt;lock_); if (new_memory_limit \u0026gt;= bfc_allocator-\u0026gt;total_region_allocated_bytes_) { // 1) new_memory_limit \u0026gt;= memory_limit_ : grow memory size  // 2) memory_limit_ \u0026gt; new_memory_limit \u0026gt;= total_region_allocated_bytes_:  // shrink, but don\u0026#39;t need to free memory  // In both cases, no action needed by changing the memory limit  bfc_allocator-\u0026gt;memory_limit_ = new_memory_limit; bfc_allocator-\u0026gt;stats_.bytes_limit = new_memory_limit; } else { // total_region_allocated_bytes_ \u0026gt; new_memory_limit:  // shrink, need to free memory  size_t free_res = FreeEmptyMemory( new_memory_limit, bfc_allocator); if (free_res \u0026lt;= new_memory_limit) { bfc_allocator-\u0026gt;memory_limit_ = new_memory_limit; bfc_allocator-\u0026gt;stats_.bytes_limit = new_memory_limit; } else { bfc_allocator-\u0026gt;memory_limit_ = free_res; bfc_allocator-\u0026gt;stats_.bytes_limit = free_res; } } return bfc_allocator-\u0026gt;memory_limit_; } File Listener 监控配置文件是否发生改变， 如果发生改变则触发响应的handler,以及一个回调函数\nvoid FileListener::RegisterFileListener(const std::string\u0026amp; file_path, const std::string\u0026amp; handler_name, callback callback_func) { LOG(INFO) \u0026lt;\u0026lt; \u0026#34;Register a file listener named \u0026#34; \u0026lt;\u0026lt; handler_name \u0026lt;\u0026lt; \u0026#34; on file \u0026#34; \u0026lt;\u0026lt; file_path; FileInfo new_file(file_path); std::vector\u0026lt;CallbackFunc\u0026gt; new_handlers; InfoAndHandlers value = {new_file, new_handlers}; CallbackFunc new_callback(handler_name, callback_func); mutex_lock l(lock_); auto res = listeners_.emplace(file_path, value); res.first-\u0026gt;second.file_handlers_.emplace_back(new_callback); if (file_monitor_thread_ == nullptr) { // Note we should start only one monitor thread  StartMonitorThread(); } } GPU resource management（中间件） 继承了SessionRunAction, 是一个资源管理中间件，定义如下\nclass GPUResourceManagement : public SessionRunAction { public: // Note that we will enable TF_FORCE_GPU_ALLOW_GROWTH and TF_GPU_VMEM  // automatically if the GPUResourceManagement feature is enabled.  GPUResourceManagement(); ~GPUResourceManagement() override; ...\t... //🚨配置文件更新后， 解析新的配置并存放于此  // For recording the parsed new gpu resource limit.  //🚨 GPU 资源限制  std::unordered_map\u0026lt;std::string, GPUResourceLimitInfo\u0026gt; gpu_resource_management_info_; // For recording the parsed new gpu performance limitation  // (if the value is 0, then it means to suspend this job).  // 🚨 GPU 性能限制， 如果值为0， 意味着挂起这个job  std::atomic\u0026lt;int\u0026gt; gpu_perf_control_; // For recording the total time of all inserted time slot.  uint64 total_time_slot_; // For recording the estimated total idle time.  uint64 estimated_total_idle_time_; // For recording the total number of queued GPU op running in  // the specified executor.  // 🚨记录每一个Executor要执行的OP数目  std::unordered_map\u0026lt;const void*, uint64\u0026gt; executor_queued_op_num_; // Determine if we need to adjust the GPU usage limit.  // 🚨表示是否需要更改配置  std::atomic\u0026lt;bool\u0026gt; need_to_adjust_memory_; // For performing the adjustment.  // 修改配置的类的实例  GPUUsageAdjustment* gpu_usage_adjustment_; const std::string FILE_LISTENER_NAME = \u0026#34;GPUResourceManage\u0026#34;; }; GPUResourceManagement() GPUResourceManagement::GPUResourceManagement() : need_to_adjust_memory_(false), gpu_perf_control_(100), gpu_usage_adjustment_(new GPUUsageAdjustment()) { //从环境变量中读取gpu配置文件的路径  ReadStringFromEnvVar(\u0026#34;GPU_CONFIG_FILE\u0026#34;, \u0026#34;\u0026#34;, \u0026amp;gpu_resource_manage_file_path_); if (gpu_resource_manage_file_path_.empty()) { enable_gpu_resource_manage_ = false; } else { enable_gpu_resource_manage_ = true; // Note that we will enable TF_FORCE_GPU_ALLOW_GROWTH and TF_GPU_VMEM  // automatically if the GPUResourceManagement feature is enabled.  setenv(\u0026#34;TF_FORCE_GPU_ALLOW_GROWTH\u0026#34;, \u0026#34;true\u0026#34;, 1); setenv(\u0026#34;TF_GPU_VMEM\u0026#34;, \u0026#34;true\u0026#34;, 1); // Register a handler that will be triggered when the file named  FileListener::GlobalFileListener()-\u0026gt;RegisterFileListener( gpu_resource_manage_file_path_, FILE_LISTENER_NAME, //FILE_LISTENER_NAME: \u0026#34;GPUResourceManage\u0026#34;  [](const std::string\u0026amp; str) { // The callback func which is invoked when file changed.  // 传入一个json文件，包含ManageInfo  // 当文件更改时， 获取相应的在session结束后调用顺序为2的action中名为GPUResourceManagement的action  // action解析新的配置信息  // 等到session 触发RunAction()， 更新限制  SessionRunAction* act = SessionRunActionRegistry::Global()-\u0026gt;GetAction( SessionRunActionRegistry::POST_SESSION_RUN, 2, \u0026#34;GPUResourceManagement\u0026#34;); if (act == nullptr) { std::cout \u0026lt;\u0026lt; \u0026#34;Cannot get the instance of GPUResourceManagement \\n\u0026#34;; } if (act != nullptr) { GPUResourceManagement* rm = dynamic_cast\u0026lt;GPUResourceManagement *\u0026gt;(act); if (rm != nullptr) { rm-\u0026gt;ParseManageInfoFromJson(str); } } }); } } 注册中间件 意味着session 结束前后要执行RunAction（\u0026hellip;）\n#if GOOGLE_CUDA // We register the GPUResourceManagement as a POST_SESSION_RUN action // during the initialization phase of the program. REGISTER_SESSION_RUN_ACTION(SessionRunActionRegistry::POST_SESSION_RUN, 2, GPUResourceManagement); #endif // GOOGLE_CUDA 实现函数RunAction（\u0026hellip;） 如果需要进行显存调整， 则调用GPUUsageAdjustment调整资源\nStatus GPUResourceManagement::RunAction( const SessionRunActionOptions\u0026amp; options) { if (!need_to_adjust_memory_ \u0026amp;\u0026amp; gpu_perf_control_ \u0026gt;= 100) { // TODO(shiru): do we need to unregister the  // GPUResourceManagement if the environment variable  // GPU_CONFIG_FILE is set to null?  return Status::OK(); } if (need_to_adjust_memory_) { mutex_lock l(manage_mu_); // Start to adjust the resource limit as required.  for (const auto\u0026amp; it : gpu_resource_management_info_) { gpu_usage_adjustment_-\u0026gt;AdjustMemLimit(it.first, //GPU总线id  it.second.mem_limit_, options.device_mgr,\t//新的显存限制， 设备管理器  options.device_set);\t//设备集合  } need_to_adjust_memory_ = false; } // 暂停一段时间 或者 挂起这个job  DoSleepOrSuspend(options.sess_duration_us); return Status::OK(); } GPUUsageAdjustment.cc bool GPUUsageAdjustment::AdjustMemLimit(const std::string\u0026amp; gpu_pci_bus_id, size_t new_mem_limit, const std::unique_ptr\u0026lt;const tensorflow::DeviceMgr\u0026gt;* device_mgr, const std::unique_ptr\u0026lt;DeviceSet\u0026gt;* device_set) { mutex_lock l(adj_mu_); //一个记录对应gpu使用情况的map  auto cur_info = cur_usage_info_.find(gpu_pci_bus_id); if (cur_info == cur_usage_info_.end()) { //如果没有相应的使用信息， 则立刻获取使用信息  GPUBFCAllocator* allo = GetGPUAllocator(device_mgr, device_set, gpu_pci_bus_id); if (allo == nullptr) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Failed to get the allocator of gpu_pci_bus_id: \u0026#34; \u0026lt;\u0026lt; gpu_pci_bus_id; return false; } GPUUsageInfo usage_info; usage_info.gpu_allocator_ = allo; usage_info.cur_limit_.mem_limit_ = ULONG_MAX; // Get the VGPU_MEMORY_LIMIT  absl::optional\u0026lt;AllocatorStats\u0026gt; device_stats = allo-\u0026gt;GetStats(); usage_info.cur_limit_.initial_mem_limit_ = device_stats ? *device_stats-\u0026gt;bytes_limit : ULONG_MAX; auto ret = cur_usage_info_.emplace(gpu_pci_bus_id, usage_info); if (ret.second == false) { return false; } cur_info = ret.first; } //如果超出虚拟GPU的使用限制， 则使用上限  if (new_mem_limit \u0026gt; cur_info-\u0026gt;second.cur_limit_.initial_mem_limit_) { // The new mem size limit exceeds VGPU_MEMORY_LIMIT  new_mem_limit = cur_info-\u0026gt;second.cur_limit_.initial_mem_limit_; LOG(WARNING) \u0026lt;\u0026lt; \u0026#34;The new mem size limit exceeds VGPU_MEMORY_LIMIT, \u0026#34; \u0026lt;\u0026lt; \u0026#34;therefore, adjust the new mem size limit to : \u0026#34; \u0026lt;\u0026lt; new_mem_limit; } //如果在限制范围内，并且需要调整， 且调整后的值不为0，  //调用GPUAdjustableAllocator， 更改内存限制， 并且更新使用信息  if (cur_info-\u0026gt;second.cur_limit_.mem_limit_ != new_mem_limit \u0026amp;\u0026amp; new_mem_limit \u0026gt;= 0) { // Adjust the memory limit of this GPU  LOG(INFO) \u0026lt;\u0026lt; \u0026#34;Start to manage the mem size limit to \u0026#34; \u0026lt;\u0026lt; new_mem_limit \u0026lt;\u0026lt; \u0026#34; of device gpu_pci_bus_id: \u0026#34; \u0026lt;\u0026lt; gpu_pci_bus_id; GPUAdjustableAllocator* adj = new GPUAdjustableAllocator(); size_t cur_mem_limit = adj-\u0026gt;AdjustMemoryLimit(new_mem_limit, cur_info-\u0026gt;second.gpu_allocator_); cur_info-\u0026gt;second.cur_limit_.mem_limit_ = cur_mem_limit; if (cur_mem_limit \u0026gt; new_mem_limit) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Failed to manage the mem size limit to \u0026#34; \u0026lt;\u0026lt; new_mem_limit \u0026lt;\u0026lt; \u0026#34; of device gpu_pci_bus_id: \u0026#34; \u0026lt;\u0026lt; gpu_pci_bus_id; // TODO(shiru): need to check is gpu_allocator_ has been changed!  return false; } return true; } return false; } Gpu Statistics （中间件） 在session运行结束后执行，执行顺序为1， 判断是否需要导出GPU统计数据\nStatus GPUStatistics::RunAction(const SessionRunActionOptions\u0026amp; options) { if (!need_to_dump_statistics_) { return Status::OK(); } bool huge_change = RecordSessionRunDuration( options.graph_id, options.sess_duration_us); if (!ShouldCheckGPUStatistics() \u0026amp;\u0026amp; !huge_change) { return Status::OK(); } { // Global lock.  mutex_lock l(check_mu_); bool dur_flag = CheckSessionRunDuration(options.graph_id, options.sess_duration_us); bool stat_flag = CheckGPUVMemAllocatorStatistics(options.device_mgr, options.device_set); if (dur_flag || stat_flag) { dumpGPUStatistics(); } gpu_statistics_last_write_ = time(0); } return Status::OK(); } void GPUStatistics::dumpGPUStatistics() { Json::Value dump_json; Json::Value gpu_info_json; for (const auto\u0026amp; a : allocator_status_lists_) { Json::Value device_json; device_json[\u0026#34;deviceMemUsedMax\u0026#34;] = Json::Int64(a.deviceMemUsedMax); device_json[\u0026#34;deviceMemUsedMin\u0026#34;] = Json::Int64(a.deviceMemUsedMin); device_json[\u0026#34;deviceMemPoolSize\u0026#34;] = Json::Int64(a.deviceMemPoolSize); device_json[\u0026#34;deviceMemStable\u0026#34;] = Json::Int64(a.deviceMemStable); device_json[\u0026#34;hostMemUsedMax\u0026#34;] = Json::Int64(a.hostMemUsedMax); device_json[\u0026#34;hostMemUsedMin\u0026#34;] = Json::Int64(a.hostMemUsedMin); device_json[\u0026#34;hostMemPoolSize\u0026#34;] = Json::Int64(a.hostMemPoolSize); device_json[\u0026#34;swapReason\u0026#34;] = a.swapReason; device_json[\u0026#34;deviceMemUsedNvidia\u0026#34;] = Json::Int64(-1); gpu_info_json[a.gpu_pci_bus_id] = device_json; } dump_json[\u0026#34;gpuUsageInfo\u0026#34;] = gpu_info_json; Json::Value sess_json; uint64 max_duration = 0; for (const auto\u0026amp; s : sess_run_durations_) { uint64 du = s.second.duration_; time_t rec = s.second.recording_time_; sess_json[\u0026#34;graph_\u0026#34; + std::to_string(s.first)] = Json::UInt64(du); if (du \u0026gt; max_duration \u0026amp;\u0026amp; time(0) - rec \u0026lt; max_record_interval) { max_duration = du; } } dump_json[\u0026#34;miniBatchDuration\u0026#34;] = Json::UInt64(max_duration); dump_json[\u0026#34;Durations\u0026#34;] = sess_json; Json::StreamWriterBuilder stream_writer; std::unique_ptr\u0026lt;Json::StreamWriter\u0026gt; writer(stream_writer.newStreamWriter()); std::ofstream statistics_file; statistics_file.open(gpu_statistics_file_); writer-\u0026gt;write(dump_json, \u0026amp;statistics_file); statistics_file.close(); // LOG(INFO) \u0026lt;\u0026lt; \u0026#34;gpu_statistics_file updated.\u0026#34;; } 注册中间件\n#if GOOGLE_CUDA // We register the GPUStatistics as a POST_SESSION_RUN action // during the initialization phase of the program. REGISTER_SESSION_RUN_ACTION(SessionRunActionRegistry::POST_SESSION_RUN, 1, GPUStatistics); #endif // GOOGLE_CUDA GpuOpManager  GpuOpManager continuously profiles the GPU operators execution time and simply distributes idle time slots before launching the GPU operators.\n 在GPUResourceManagement.cc 中实现了set()和get() 对应executor要执行OP数的接口，\n在Executor中以一个线程的形式运行， 在ExecutorState::ExecutorState中新增了一个thread成员变量\n修改Executor.cc\n构造函数里，初始化gpu_op_manger_thread // 获取GPUResourceManagement  GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm != nullptr) { enable_op_management = (rm-\u0026gt;GetEstimatedIdleTime() \u0026gt; 0); } if (enable_op_management \u0026amp;\u0026amp; gpu_op_manager_thread_ == nullptr) { gpu_op_manager_thread_ = new std::thread(\u0026amp;ExecutorState::AsyncGPUOpManager, this); } manager负责插入时间槽， 也就是计算好sleep的时间， 释放资源\n// The manager thread which is in charge of inserting the time slot // before launching each queued async GPU op. void ExecutorState::AsyncGPUOpManager() { uint64 sleep_time_us = 0; need_to_insert_idle_time_ = false; GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm == nullptr) { return; } while (!terminate_op_magager_thread_) { need_to_insert_idle_time_ = rm-\u0026gt;GetEstimatedIdleTime() \u0026gt; 0 ? true : false; std::function\u0026lt;void(void)\u0026gt; queued_call_func = nullptr; // 1）从队列中获取第一个要执行的Op  async_gpu_op_queue_lock_.lock(); if (!async_gpu_op_queue.empty()) { queued_call_func = async_gpu_op_queue.front(); } async_gpu_op_queue_lock_.unlock(); if (queued_call_func != nullptr) { //2）调用这个Op  queued_call_func(); async_gpu_op_queue_lock_.lock(); //3）从队列中删除这个op  if (!async_gpu_op_queue.empty()) { async_gpu_op_queue.erase(async_gpu_op_queue.begin()); // 数目减一  num_queued_op.fetch_sub(1); } async_gpu_op_queue_lock_.unlock(); // Estimate idle time 预测空闲时间  uint64 idle_time = rm-\u0026gt;GetEstimatedIdleTime(); uint64 queued_op_num = rm-\u0026gt;GetExecutorQueuedOpNum(impl_); idle_time = queued_op_num \u0026gt; 0 ? (idle_time / queued_op_num) : 0; // 等待一个op的 idle_time  usleep(idle_time); // 设置剩余时间  uint64 remain_time = rm-\u0026gt;GetEstimatedIdleTime(); remain_time = remain_time \u0026gt; idle_time ? (remain_time - idle_time) : 0; rm-\u0026gt;SetEstimatedIdleTime(remain_time); } usleep(default_check_interval); } return; } Process 处理过程 // Process():  ... Device* kernel_device = impl_-\u0026gt;params_.device; // Only enqueue this op if it is an async GPU op. \t// 1 如果是一个异步Op， 则加入到异步OP队列  if (need_to_insert_idle_time_ \u0026amp;\u0026amp; (kernel_device-\u0026gt;name()).find(\u0026#34;GPU\u0026#34;) != string::npos) { // Enqueue this GPU op therefore we can insert a time slot before launching this op.  // 在启动这个op之前我们可以插入一个时间槽  sess_op_num++; const GraphView\u0026amp; gview_t = impl_-\u0026gt;gview_; const NodeItem\u0026amp; item_t = *gview_t.node(id); AsyncState* state = new AsyncState(params, tagged_node, \u0026amp;item_t, first_input, stats); auto async_gpu_kernel = [this, state, id, stats, op_kernel, device] { AsyncOpKernel* async = state-\u0026gt;item-\u0026gt;kernel-\u0026gt;AsAsync(); DCHECK(async != nullptr); auto done = [this, state]() { Device* device = impl_-\u0026gt;params_.device; NodeExecStatsInterface* stats = state-\u0026gt;stats; // Shorthand  Entry* first_input = state-\u0026gt;first_input; // Shorthand  nodestats::SetOpEnd(stats); EntryVector outputs; Status s = ProcessOutputs(*state-\u0026gt;item, \u0026amp;state-\u0026gt;ctx, \u0026amp;outputs, stats); nodestats::SetMemory(stats, \u0026amp;state-\u0026gt;ctx); if (vlog_) { VLOG(2) \u0026lt;\u0026lt; \u0026#34;Async kernel done: \u0026#34; \u0026lt;\u0026lt; state-\u0026gt;item-\u0026gt;node-\u0026gt;id() \u0026lt;\u0026lt; \u0026#34; step \u0026#34; \u0026lt;\u0026lt; step_id_ \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; SummarizeNode(*state-\u0026gt;item-\u0026gt;node) \u0026lt;\u0026lt; (state-\u0026gt;tagged_node.is_dead ? \u0026#34; is dead\u0026#34; : \u0026#34;\u0026#34;) \u0026lt;\u0026lt; \u0026#34; device: \u0026#34; \u0026lt;\u0026lt; device-\u0026gt;name(); } // Clears inputs.  const int num_inputs = state-\u0026gt;item-\u0026gt;num_inputs; for (int i = 0; i \u0026lt; num_inputs; ++i) { (first_input + i)-\u0026gt;ClearVal(); } FrameState* input_frame = state-\u0026gt;tagged_node.input_frame; const int64 input_iter = state-\u0026gt;tagged_node.input_iter; const int id = state-\u0026gt;tagged_node.node-\u0026gt;id(); MaybeMarkCompleted(input_frame, input_iter, id); TaggedNodeSeq ready; if (s.ok()) { PropagateOutputs(state-\u0026gt;tagged_node, state-\u0026gt;item, \u0026amp;outputs, \u0026amp;ready); } outputs.clear(); if (s.ok() \u0026amp;\u0026amp; impl_-\u0026gt;device_record_tensor_accesses_) { // Get the list of all tensors accessed during the execution  TensorReferenceVector accessed; state-\u0026gt;ctx.retrieve_accessed_tensors(\u0026amp;accessed); nodestats::SetReferencedTensors(stats, accessed); // callee takes ownership of the vector  device-\u0026gt;ConsumeListOfAccessedTensors(state-\u0026gt;ctx.op_device_context(), accessed); } const bool completed = NodeDone(s, state-\u0026gt;item-\u0026gt;node, ready, stats, nullptr); delete state; if (completed) ScheduleFinish(); }; nodestats::SetOpStart(stats); { profiler::TraceMe activity( [\u0026amp;] { return strings::StrCat( op_kernel-\u0026gt;name(), \u0026#34;:\u0026#34;, op_kernel-\u0026gt;type_string(), \u0026#34;#id=\u0026#34;, step_container_ ? step_container_-\u0026gt;step_id() : 0, \u0026#34;,device=\u0026#34;, device-\u0026gt;name(), \u0026#34;,async=true#\u0026#34;); }, profiler::GetTFTraceMeLevel(op_kernel-\u0026gt;IsExpensive())); device-\u0026gt;ComputeAsync(async, \u0026amp;state-\u0026gt;ctx, done); } }; // Enqueue this asyn GPU op.  async_gpu_op_queue_lock_.lock(); async_gpu_op_queue.emplace_back(async_gpu_kernel); //添加kernel到op队列  num_queued_op.fetch_add(1); //加一  async_gpu_op_queue_lock_.unlock(); } else { //2 如果不是异步的OP 则不加入  // Do not enqueue this op.  AsyncOpKernel* async = item.kernel-\u0026gt;AsAsync(); DCHECK(async != nullptr); AsyncState* state = new AsyncState(params, tagged_node, \u0026amp;item, first_input, stats); auto done = [this, state]() { Device* device = impl_-\u0026gt;params_.device; NodeExecStatsInterface* stats = state-\u0026gt;stats; // Shorthand  Entry* first_input = state-\u0026gt;first_input; // Shorthand  nodestats::SetOpEnd(stats); EntryVector outputs; Status s = ProcessOutputs(*state-\u0026gt;item, \u0026amp;state-\u0026gt;ctx, \u0026amp;outputs, stats); nodestats::SetMemory(stats, \u0026amp;state-\u0026gt;ctx); if (vlog_) { VLOG(2) \u0026lt;\u0026lt; \u0026#34;Async kernel done: \u0026#34; \u0026lt;\u0026lt; state-\u0026gt;item-\u0026gt;node-\u0026gt;id() \u0026lt;\u0026lt; \u0026#34; step \u0026#34; \u0026lt;\u0026lt; step_id_ \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; SummarizeNode(*state-\u0026gt;item-\u0026gt;node) \u0026lt;\u0026lt; (state-\u0026gt;tagged_node.is_dead ? \u0026#34; is dead\u0026#34; : \u0026#34;\u0026#34;) \u0026lt;\u0026lt; \u0026#34; device: \u0026#34; \u0026lt;\u0026lt; device-\u0026gt;name(); } // Clears inputs.  const int num_inputs = state-\u0026gt;item-\u0026gt;num_inputs; for (int i = 0; i \u0026lt; num_inputs; ++i) { (first_input + i)-\u0026gt;ClearVal(); } FrameState* input_frame = state-\u0026gt;tagged_node.input_frame; const int64 input_iter = state-\u0026gt;tagged_node.input_iter; const int id = state-\u0026gt;tagged_node.node-\u0026gt;id(); MaybeMarkCompleted(input_frame, input_iter, id); TaggedNodeSeq ready; if (s.ok()) { PropagateOutputs(state-\u0026gt;tagged_node, state-\u0026gt;item, \u0026amp;outputs, \u0026amp;ready); } outputs.clear(); if (s.ok() \u0026amp;\u0026amp; impl_-\u0026gt;device_record_tensor_accesses_) { // Get the list of all tensors accessed during the execution  TensorReferenceVector accessed; state-\u0026gt;ctx.retrieve_accessed_tensors(\u0026amp;accessed); nodestats::SetReferencedTensors(stats, accessed); // callee takes ownership of the vector  device-\u0026gt;ConsumeListOfAccessedTensors(state-\u0026gt;ctx.op_device_context(), accessed); } const bool completed = NodeDone(s, state-\u0026gt;item-\u0026gt;node, ready, stats, nullptr); delete state; if (completed) ScheduleFinish(); }; nodestats::SetOpStart(stats); { profiler::TraceMe activity( [\u0026amp;] { return strings::StrCat( op_kernel-\u0026gt;name(), \u0026#34;:\u0026#34;, op_kernel-\u0026gt;type_string(), \u0026#34;#id=\u0026#34;, step_container_ ? step_container_-\u0026gt;step_id() : 0, \u0026#34;,device=\u0026#34;, device-\u0026gt;name(), \u0026#34;,async=true#\u0026#34;); }, profiler::GetTFTraceMeLevel(op_kernel-\u0026gt;IsExpensive())); device-\u0026gt;ComputeAsync(async, \u0026amp;state-\u0026gt;ctx, done); } } 结束函数 // Finish():  if (gpu_op_manager_thread_ != nullptr) { terminate_op_magager_thread_ = true; if (gpu_op_manager_thread_-\u0026gt;joinable()) { gpu_op_manager_thread_-\u0026gt;join(); } delete gpu_op_manager_thread_; terminate_op_magager_thread_ = false; } 总结 graph TD Agpu_resource_manage_file] B[SessionRunRegistry] C[SessionRunAction] D[Executor] E[GPUResouceManagement] F[GPU Statistic] G[GpuOpManager] H[GpuUsageAdjustment] I(dump gpu statistic) J[GPU Process State] K[GPUVMemAllocator] L[GPUAdjustableAllocator] A --|FileListener| E B --|Register| E E --|need_to_adjust_memory_| H H --|new| L H --|get| K C --|Derive| E C --|Derive| F B --|Register| F F --|need_to_dump_statistics_| I B --|Run| C J --|maybe_create_gpu_vmem_allocator|K D --|run thread| G E --|GetEstimatedIdleTime| G  ","date":"2022-12-04T23:08:20+08:00","permalink":"https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90antman%E5%AF%B9tensorflow%E7%9A%84%E4%BF%AE%E6%94%B9/","title":"【代码分析】Antman对Tensorflow的修改"},{"content":"EasyScale 论文阅读笔记 Abstract  分布式同步GPU训练通常被用于深度学习。  使用固定GPU的资源约束  使得大规模的深度学习训练工作受到影响 降低了集群的利用率   纳入资源弹性  往往会引入模型精度的非确定性\u0026lt;\u0026mdash;\u0026ndash;缺乏隔离能力     本文介绍EasyScale，  这是一个弹性框架  可以在异构GPU上扩展分布式训练 同时产生确定性的深度学习模型   实现了弹性的精度一致的模型训练。  EasyScale严格遵循数据并行训练流程 仔细追踪与精度相关的因素 有效利用深度学习特性进行上下文切换   为了使异构GPU的计算能力达到饱和  EasyScale根据我们的作业内和作业间调度策略动态地分配工人 最大限度地减少GPU的空闲时间 并相应地提高综合作业的吞吐量。   实验  部署在CompanyA的一个在线服务集群中 EasyScale为弹性深度学习训练作业提供动力，使其适时地利用空闲的GPU 在不违反SLA的情况下将集群的整体利用率提高了62.1%      Introduction   弹性深度学习框架很少在行业中使用\n 根本障碍：在使用不同资源进行训练时模型准确性不一致 资源弹性对训练程序和模型收敛都引入了非确定性。  通过调整特定基准中的超参数（如学习率或批量大小）收敛到类似的精度不能说服用户，因为预期的计算流程已经隐性改变。 在改变数据集或模型结构时，对模型准确性的非确定性的担忧仍然没有得到解决，这使得深度学习从业者在拥抱资源弹性时犹豫不决      EasyScale：\n 第一个在同构和异构 GPU 的资源弹性上实现一致模型精度的训练框架  提高了整体集群效率\u0026lt;\u0026mdash;-通过尽最大努力利用空闲 GPU 来进行弹性模型训练   将深度学习模型训练视为科学实验，将确定性和可重复性作为第一流的目标 EasyScale探讨了将分布式模型训练过程与硬件资源解耦的可能性  无论分配的GPU数量和类型如何，都能产生位数一致的模型 这是通过一个名为EasyScaleThread的抽象来实现的  它封装了从数据加载、采样、计算到通信的所有阶段 并使它们与在固定的GPU中执行的完全一样   EasyScale利用深度学习的特点  实现了快速的上下文切换 解决了训练工作者状态的潜在非确定性 并在资源重新配置时有效地执行追踪和检查点   EasyScale引入了一个工作内策略\u0026mdash;-\u0026gt;以负载平衡的方式在异构GPU上安排训练工人 进一步优化了工作间的资源分配\u0026mdash;-\u0026gt;以最大限度地提高总的吞吐量。      贡献如下\n 我们调查了现有的弹性深度学习框架和异构环境中的非确定性行为，并对散布在整个DLT软件堆栈中的位数差异进行了溯源。 我们介绍用于弹性分布式模型训练的EasyScale，它以确定性的方式实现了一致的准确性。EasyScale利用EasyScaleThread来保持与PyTorch DDP相同的弹性训练行为，并以可忽略不计的开销有效地进行上下文切换。 我们引入了新的作业内和作业间调度策略，以提高单个EasyScale作业的吞吐量和聚合集群的吞吐量，从而灵活有效地利用异构的GPU。 我们在生产集群中全面部署了EasyScale，将弹性训练作业与在线模型服务放在一起，在满足模型服务SLA的约束下，显著提高了集群利用率。    Motivation 弹性训练带来的不确定性   不一致的模型精度\n  \rimage-20221110093121968\r\n  用弹性框架进行模型训练的多次运行，未能在使用不同数量的资源时产生一致的模型精度。\n  图2说明了ResNet18在CIFAR10上的验证精度  实验条件  这是一个用不同数量的V100 GPU训练的弹性模型 超参数和随机种子与默认值相同，只使用不同分配的GPU TorchElastic（TE）被配置为调整学习率的线性缩放规则 Pollux可以自动决定相应的学习率和批次大小   实验结论  与固定GPU上的分布式模型训练相比，资源弹性带来了不同的训练行为。 Pollux在产生模型质量方面引入了相对较小的差异，然而，这种差异仍然是不可忽视的     图3中报告了总体和每类的继续训练到100个epoch准确率  结果  总体准确率的差异仍然很明显：TorchElastic和Pollux分别为0.6%和2.8%。 每类准确率的差异甚至更大：达到7.4%和17.3%   结论  这表明弹性训练的模型与使用固定GPU的模型相比，偏差不同。 其他指标仍未披露，模型质量差距的上限仍然未知           很难理解超参带来的影响\n  \rimage-20221110093137841\r\n  研究人员很难推理出gamma如何影响训练损失曲线\n  图4显示了ResNet50在CIFAR10上的实验  实验设置  比较对象  固定4个GPU上的DDP训练 1/2/4个GPU上的使用Pollux的弹性模型训练   除了学习率调度器的一个超参数gamma外，其他配置都是一致的  gamma决定了本实验中20个epochs后的学习率降低比例 DDP实验以相同的设置运行了三次4-GPU训练，但gamma值分别为0.1、0.3和0.5 Pollux也运行同样的实验，1个GPU的gamma为0.1，2个GPU的gamma为0.3，而4个GPU的gamma为0.5     实验结论  使用DDP，可以清楚地推断出超参数gamma是如何影响模型训练过程的。  训练损失在前20个 epochs的三次运行中保持一致 之后，较小的gamma导致较小的训练损失   使用Pollux，研究人员很难推理出gamma如何影响训练损失曲线。           总结\n DL 模型现在是算法，框架和计算资源组合的结果，这是限制 DLT 作业使用弹性资源的根本原因 现有的弹性 DL 框架缺乏将资源与模型超参数解耦的能力\u0026mdash;-\u0026gt;无法提供与弹性训练一致的模型精度 我们需要支持弹性训练，同时保持一致的模型精度    Design Overview   弹性训练应该生成与使用固定数量的GPU进行的DDP训练相同的模型参数\n  实现弹性准确度一致的模型训练的关键挑战是找到一种实用的方法，将一个GPU有效地分享给多个worker\n  EasyScaleThread   EasyScaleThread： 捕捉深度学习的训练过程并将其与硬件资源解耦\n 每个GPU都由一个EasyScale PyTorch worker启动。 原始训练worker的执行被视为EST的执行，它可以动态地分配给PyTorch worker进程。 在一个worker中，多个EasyScaleThreads轮流占用GPU进行计算。 使用  通过使用白盒方法，EasyScale通过用户注释将模型训练的关键步骤挂钩，数据加载、反向传播和模型更新，因此在mini-batch边界进行精度一致的上下文切换。 用户定义的模型训练语义，包括模型结构、数据增量、批次大小、优化等，都照常保留。 至于编程，用户考虑总的逻辑训练工作者的数量来决定超参数（如全局批处理量和学习率），这与他们使用固定GPU的经验相同，但自动受益于EasyScale提供的弹性。      执行\n mini-batch的执行：  输入数据被分割到所有的EasyScaleThreads中 一个GPU上每次执行一个EST时， 其他EST被冻结 在所有EasyScaleThreads完成后，mini-batch就完成了   当两个EasyScaleThreads切换时  EasyScaleThread的训练状态需要被保存到GPU之外，因此要确保有足够的GPU内存给下一个EasyScaleThread，这可能是昂贵的。 在GPU上为每个minibatch有效切换EasyScaleThreads的关键是减少上下文切换所需的状态。 而EasyScale选择在完成前向后向计算后切换EasyScaleThreads，最大限度地减少GPUCPU内存拷贝。   我们通过以下方式最小化状态的大小：  i）定位影响最终精度的非确定性来源，最小化需要记录的必要状态 ii）利用DL的数据并行行为，最小化数据交换的工作集。   EasyScaleThread的GPU内存中的工作集  可以分为时间张量和激活、模型参数和优化状态以及梯度， 处理  首先，对于时间张量和激活，它们在前向步骤中创建，在完成梯度生成后在后向步骤中销毁。  因此，它们会在小批处理结束时自动释放出来。   其次，关于模型参数和优化器状态，每个数据并行工作者在训练过程中都会保留一个副本，并且在一个小批处理结束时进行更新。  因此，在EasyScale中，当切换EasyScaleThreads时，它们可以被重复使用。   最后，梯度是根据EasyScaleThreads的不同数据输入计算的，因此不能重复使用。然而，梯度通常很小，并且只在minibatch结束时的分布式梯度同步中使用。  因此，在EasyScale中，我们在上下文切换时将梯度迁移到主机DRAM中，并与下一个EasyScaleThread的计算重叠。       通过这种方式，我们交替执行EasyScaleThreads，直到所有计算完成。之后，分布式同步被触发，模型更新被进行一次以完成小批量的计算。    重配置\n 当资源重新配置被触发时，EasyScale采用按需检查点的方式来持续保持最小和必要的状态。 检查点包含了  所有EasyScaleThreads的上下文 额外的状态（包括训练进度和其他实现精度一致性的状态） 深度学习参数（例如，模型、优化器和学习率调度器）   与EasyScaleThread上下文不同的是  额外的状态和参数只需要一个副本，因为它们在mini-batch结束时对所有EasyScaleThread都是一样的。   请注意，在重新启动模型训练后，每个GPU的EasyScale运行时  会加载额外的状态和模型参数的副本 以及重新分布的EasyScaleThreads的相应上下文      优化\n  为了overlap数据加载和GPU训练\n data loader在独立的处理器中执行（即PyTorch中的加载器工作者进程），异步加载训练样本并执行数据增强（例如，裁剪或旋转图像）以建立训练批次。 在EasyScale中，我们优化了所有EasyScaleThreads之间共享data loader，因为每次只有一个EasyScaleThread在GPU上进行训练。 尽管共享了多个EasyScaleThreads，但数据消耗率与专用GPU中的数据消耗率相似。    为了实现data loader的共享\n  EasyScale采用了一个分布式数据采样器，该采样器共同考虑了EasyScaleThreads的全局index和时分模式，在一个队列中生成数据index\n  然后，这些数据索引被数据工作者有序地处理\n \rimage-20221110211808582\r\n 图7显示了将三个数据工作者共享给两个EasyScaleThreads的情况，其中EasyScaleThread的总数量为四个（即图6中的2-GPU训练）。 EST0和EST1的训练批次为：小批0的b0和b1，小批1的b4和b5。 在专用GPU中为EasyScaleThread i处理数据指数的数据工作者j的状态被表示为Ri-j 为了平衡负载，EasyScale中的数据工作者轮流从队列缓冲区中获取给定数据指数的相应状态（即Ri-j）进行预处理，完成后将状态提交回队列缓冲区中。     注意，由于data loader的异步执行\u0026ndash;\u0026gt;\n data loader的进程通常在训练进度之前\u0026mdash;\u0026gt; 为了跟踪和保持弹性的一致状态，引入了一个排队缓冲区来记录未被消耗的小批的必要状态\u0026mdash;\u0026gt; worker的状态根据训练进度从队列缓冲区中去排队，然后在检查点中被视为额外状态的一部分        不确定性的溯源与解决  自上而下的方法来比较EasyScale和DDP， 我们发现非确定性的根本原因分散在训练管道的几乎整个软件栈中，从训练框架到通信，再到GPU内核。  首先，在训练框架层面  框架有一些状态需要在整个训练过程中保持一致，以保证确定性 尽管深度学习训练在DAG图中组织运算符（例如卷积、批量归一化），但一些运算符隐含地依赖一些状态，而不是其前辈的输出。例如  Dropout依赖于GPU中的随机数发生器（RNG）状态； BatchNorm通过考虑工作者的等级来跟踪其运行状态； 数据加载器和数据增强的转化器依赖于Python、NumPy和PyTorch的随机状态，等等。     第二，在通信层面  通过全还原的梯度同步在资源弹性下是不确定的 在同步过程中，梯度被聚集到通信桶中，以优化通信性能。 梯度到桶的映射最初由DAG图的静态反转拓扑顺序决定。 它在第一个小批处理结束时根据收到的梯度张量的顺序进行重构。 然而，在弹性训练期间，工作者重新启动将重建通信渠道，这可能会影响第一个恢复的小批的梯度聚合顺序。 由于环形Allreduce的实现，这最终会引入非确定性。   最后，在GPU内核层面  为同一运算器选择不同的内核也会导致结果的细微差别 导致不同内核选择的原因有两个。  首先是框架、编译器或供应商库中的一些基于剖析的优化，在小批量中应用不同的内核实现，以收集性能统计数据，找到最佳匹配。 另一个是内核的实现可以与硬件相关。例如，一些内核实现是基于流处理器单元的数量、硬件特定的低位组件等，因此不能应用于所有类型的GPU。       确定性的等级与应对方案  EasyScale定义了不同级别的弹性训练的确定性，为用户提供明确的一致性保证，并设计了相应的处理方法来实现它们。 D0：固定DoP的确定性  \u0026ndash;用固定的GPU资源进行多次训练应该产生相同的模型 实现D0需要训练框架及其所选内核的一致行为。  对于框架，我们在训练开始时固定RNG的随机种子，并在数据加载工作者状态中记录RNG的状态，在上下文中记录EasyScaleThreads的状态，以便EasyScaleThreads自动保持状态一致。 我们还禁用了最适合的算法选择，并选择了确定性的算法（例如，没有原子指令）。     D1：弹性确定性  \u0026ndash;在检查点重启的情况下，用数量不断变化的同质GPU进行多次训练，应该产生相同的模型 在D0之外，D1需要解决通信层面的非确定性。  为此，我们为每个EasyScaleThread分配了一个固定的虚拟通信等级。 我们还将形成梯度桶的索引记录到检查点中。 重新启动后，在训练之前，首先用记录的指数重建桶。 后面的通信通道重建被禁止。     D2：异质性确定性  \u0026ndash;用不同类型的GPU进行的多次训练应该产生相同的模型。 为了实现D2，我们开发了与硬件无关的GPU内核。具体来说，  1）我们修改内核实现（例如PyTorch中的reduce、dropout），限制SM和线程的数量； 2）我们通过向高层调用传递algo_id，强制选择相同的低层实现（例如cuDNN中的convolution，以及cuBLAS中的gemm、gemv）。       如何确定确定性等级  在EasyScale中，D0和D1是默认启用的，因为我们的实现对实现它们的开销可以忽略不计 实现D2对于某些类型的模型（如CV模型）可能会有较高的开销，因为它们不能对某些GPU类型使用一些供应商优化的内核（如卷积）。  EasyScale可以透明地分析一个模型（通过扫描PyTorch nn.Modules），并识别它是否依赖于需要硬件特定内核优化的运算符。 如果不是，我们就启用D2，允许它使用异构的GPU，否则就限制它使用同构的GPU。      调度原则  用户在提交EasyScale作业时  可以指定一个maxP：即要启动的最大工作者数量，这也是作业执行过程中EasyScaleThreads的数量 用户还可以指定一个minP（\u0026gt;=0），表示所需的保证GPU 设置minP == maxP意味着作业将回到使用与DDP相同的固定DoP   小结：ESTi抽象 -\u0026gt; 确定性处理 -\u0026gt; 调度EST  EasyScaleThreads（EST）的抽象将DL训练和底层GPU资源解耦，因此与DDP兼容的训练作业可以持续地在同质GPU的弹性数量上运行。 确定性处理实现了弹性训练下的精度一致性，即使是在异构GPU上   在异构GPU上调度EST的关键挑战在于计算能力的异质性和GPU内存的异质性。  Pollux和VirtualFlow  采用为每一种GPU单独扩展批次大小的方法 对于EasyScale来说是完全不可接受的，因为它改变了固有的训练超参数，从而破坏了精度的一致性   EST消耗固定计算能力与固定内存消耗  在EST执行过程中，每个EST消耗固定的计算能力，其中GPU的微架构特征（即SM数量和缓存大小）决定了理论能力。具有较高计算能力的GPU可以在一定时间内执行更多的EST 而每个EST都有固定大小的GPU内存使用峰值。此外，由于同一GPU执行器中的EST的内存是完全重复使用/共享的，执行器的内存用量可以代表EST的内存用量 为简洁起见，我们将固定的计算能力和内存用量分别定义为计算单元（CU）和内存单元（MU）   复杂的分析规划：避免因为严重的负载不平衡而造成重大的性能浪费    异构感知的EST规划   Planning CUs\n 分配策略  当分配的GPU是同质的，并且数量是maxP的一个因素时，在这些GPU上均匀地分配CU，因为所有CU都有相同的计算时间 在异构GPU的情况下，需要根据GPU的计算能力来分配CU以达到平衡，并最多消除空闲周期   我们提出了一个新的指标，叫做浪费，  即由于CU的整数倍和GPU的实际连续能力不匹配而浪费的计算能力，或者说是负载不平衡。 浪费主要包括两个方面：  ∂异构GPU之间的负载不平衡，是由于用CU的整数倍不准确地逼近连续的实际计算能力造成的； ∑同构GPU之间的负载不平衡，是由于没有足够的CU分配来充分利用所有的GPU，因为CU的总量被maxP所限制。   因此，我们建立一个分析模型来量化浪费。  符号表示  可用的GPU数量表示为Ni，其中下标i代表GPU类型。 与工作负载相关的计算能力Ci被估计为每秒的小批处理数量。 分配给GPU类型i的CU的最大数目被表示为Ai。   \rimage-20221112203217230\r 为了确保所有EasyScaleThreads被执行，异构GPU上的最大CU总数（CU_capacity）应该大于或等于maxP（公式1a）。 过载系数foverload代表所请求的异构GPU的最大过载，其中过载被定义为每个计算能力的CU（等式1b）。 如果一个GPU类型承担了太多的CU，它就会成为性能瓶颈，并由于Sync-SGD而拖慢其他GPU的速度。因此，浪费被表述为：∂Ci和Ai之间的差距按foverload缩放，∑超额配置的CU_capacity按foverload缩放（公式1c）。 为了进一步区分当前Ni、Ci和Ai下的CU分配效率，得出了归一化的浪费百分比（公式1d）。 还得出了估计性能（等式1e）。        Planning MUs\n  异构GPU之间的内存容量也存在异质性\n 将MU分配给每个GPU的单个执行器可以最大限度地减少整体内存占用，因为EasyScaleThreads引入的内存开销可以忽略不计。 而且它们的MU可以完全重复使用。 因此，所有的GPU都显示出与MU相同的峰值内存使用量，导致具有较大内存容量的GPU出现闲置内存。    我们提出了一个多执行器的设计\n  它允许在GPU上分配一个以上的执行器，这样就可以同时执行多个EST。\n  具有较大内存的GPU可以权衡执行器数量和每个执行器的EST数量\n 同时保持 （#执行器 × #EST） 不变 例如，在分配了两个EST的情况下，有两种选择：  a）\u0026lt;1执行器×2EST\u0026gt; b）\u0026lt;2执行器×1EST\u0026gt;      它拓宽了高效场景，即运行更多的执行器不会超过GPU资源（SM核、内存），即使考虑到干扰，仍然有助于提高性能。\n 推荐模型（如Wide\u0026amp;Deep）的训练通常显示出对GPU计算能力的利用不足，通常低于50%。在这种情况下，分配多个执行器可以利用剩余的计算能力，而且执行器之间不会产生不利影响，从而提高综合吞吐量。\n     我们还对（等式1）进行了调整，以对多个执行器的浪费进行建模。\n 与工作负载相关的计算能力Ci被MCi = m × Ci × Ii所取代，表示m个执行器的整体能力，其中包括干扰Ii。 分配给GPU Ai的CU数量由MAi = m × Ai取代，代表m个执行器的总CU数量。      EeayScale 调度器  如图9所示，EasyScale采用了一个分层调度架构。  每个作业都包含一个作业内的调度器，名为AIMaster，它负责：  a）在同质和异质GPU之间分配EasyScaleThreads，尽量减少浪费，使资源利用率最大化； b）通过估计潜在的速度提升，提出所需的资源进行扩展。   此外，一个集群调度器以全局模式行事，协调作业之间的资源。    \rimage-20221110214614664\r\n Intra-Job scheduler  基本职责：在给定的GPU下生成EST分配配置  首先，在当前可用的GPU下，它选择估计吞吐量最高的配置，并相应分配EST。 其次，它试图用一个增量的GPU来扩展，从而产生新的配置，并选择top-K的配置作为提交给集群调度器的建议。   配置组成与约束  由\u0026lt;nums, executors, threads, waste, perf\u0026gt;组成  nums, executors, threads是具有相同长度的GPU类型的数组，分别代表GPU数量、执行器数量和每个执行器的EST数量 waste和perf代表该配置通过分析模型估计的浪费和性能   这些配置应该满足集群总资源、minP、maxP和归一化浪费的阈值（实际为30%）的约束。   寻找可用配置  不同的工作负载使用不同GPU的吞吐量有差异，但在没有实际执行的情况下很难预测 因此，AIMaster模块使用作业的运行时执行统计数据来了解工作负载差异，并确定每个GPU类型i的工作负载相关计算能力Ci。 鉴于每个GPU类型i的剖析计算能力Ci，我们计算它们的整数近似值（例如，ceil(t×Ci), floor(t×Ci)），假设每个能力承担k个EST，并形成它们的组合 然后我们遍历这些组合，找到可用的配置。对于具有相同\u0026lt;数、执行器、线程\u0026gt;的配置，选择具有最小浪费的配置，其他配置则被过滤掉   配置回退  对浪费的估计有时可能是不正确的，这可能会导致更差的训练性能 一旦在重新配置后观察到性能下降，我们就会退回到使用以前的资源并释放新分配的资源     Inter-job cluster scheduler   它通过考虑资源可用性和提案的优先级来响应AIMaster提案。\n 为了提高集群的整体利用率和聚合作业的吞吐量 它采用了一种启发式算法，倾向于接受每个GPU具有较高速度的建议，如算法1所示。  作业间调度器按照报告的平均加速比对建议进行降序排序。 然后，它对这些建议进行循环，开始接受最高的建议。 如果多个建议引入了相同的平均GPU加速，我们的调度器会优先考虑拥有更多GPU的建议。    \rimage-20221112210504940\r\n  集群调度器允许弹性作业最好地利用空闲资源\n 这些资源通常属于其他人，但暂时是闲置的。 然而，如果这些GPU需要返回，可能会触发抢占。  在这种情况下，集群调度器将尝试把与抢占的GPU相同的GPU分配给弹性作业。 当分配超时时，EasyScale作业会回落到利用它目前拥有的可用GPU。        Implementation   DLT 作业在具有 EasyScale 实现的 Docker 容器中运行。\n 在 Kubernetes 上实现了一个原型自定义集群调度器以进行评估。 EasyScale 在我们内部的 GPU 集群调度器中得到了充分的实现，它是 Kubernetes 调度器的一个优化版本，可以为日常的 GPU 生产任务提供服务。    DL 框架中 EasyScale 的实现与 PyTorch 1.8 LTS 兼容。\n 它需要大约1,200行 Python 代码和2,000行 PyTorch 中的 C + + 修改代码，以及一个基于 PyTorch 实现的插件库。 PyTorch 框架的 C + + 实现包括一个支持弹性的分布式数据并行通信库 ElasticDDP，它可以支持多个 EasyScaleThread 之间的通信，以全面减少梯度，并在触发资源弹性时在重新启动任务时始终如一地构建通信桶。 执行流控制和上下文切换作为 PyTorch 的附加组件在 Python 模块中实现。    实现了 AIMaster\n 为了决定负载平衡分配并控制作业以使用更多的 GPU 进行扩展 三个组成部分。  首先，我们通过一个 rpc 库收集 EasyScale 运行时报告的性能分析。 其次，我们提出资源建议并监视状态，从而通过 Kubernetes Python 告密者了解资源分配超时。 第三，实现策略控制器来计算增量资源请求并将其提交给集群调度器。为了支持资源弹性时的持续工作培训，我们采用按需检查点记录用户定义的模型，时代和小批量状态以及基本上下文切换状态      一个半自动剖析工具来执行张量之间的按位比较\n 从而找到运算符不一致的结果，识别资源弹性中不确定性的来源。    ","date":"2022-11-09T15:09:23+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202211091510408.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0easyscale%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】EasyScale论文阅读笔记"},{"content":"Gandiva 论文阅读笔记 Abstract   Gandiva: 一个集群调度框架，使用特定领域知识，优化了GPU集群训练深度学习模型的延迟与效率\n  深度学习job的特征\n 1）反馈驱动的探索：  一个用户经常运行一组作业(或 a multi-job)来获得特定任务的最佳结果 并使用关于准确性的早期反馈来动态优先考虑或杀死一个作业子集 同步发生的多个作业的早期反馈是至关重要的   2）深度学习工作在资源使用方面的异构，这使得它很难实现最适合的先验。 3）作业内可预测性：因为作业会重复执行叫做mini-batch的迭代  Gandiva利用这个特征解决了1）2）两个问题 利用可预测性对GPU进行多个job间进行时分复用， 这提供了低延迟 这种预测性还可以用于内省job性能并动态迁移到最合适的GPU上，提高了集群效率      我们通过一个原型实现和微基准测试表明\n Gandiva 可以在深度学习过程加快超参数搜索一个数量级 并通过透明迁移和job时分实现更好的利用，使job与资源的更好地匹配。 在一个运行在180-GPU 集群中的实际工作负载中，Gandiva 将集群的总利用率提高了26% 这为深度学习提供了一种管理大型 GPU 集群的新方法。    Introduction   DLT job的特征\n  反馈驱动\n  超参数搜索：\n 用户通常会尝试一个作业的几个配置（a multi-job），并利用这些作业的早期反馈来决定是否优先处理或杀死其中的一些子集。     传统的调度器运行一个工作子集， 完成前其他工作排队\n 这种模式不适合multi-jobs，因为a multi-job中的所有工作需要同时得到早期反馈。 另外，伴随着multi-jobs，其他已经确定了正确的超参数的DLT作业，运行了几个小时到几天，导致了行头阻塞，因为长期运行的作业对GPU拥有独家访问权，直到完成，而取决于早期反馈的多作业则在队列中等待。 长的排队时间迫使用户要么使用预留的GPU，要么要求集群超额配置，从而降低集群效率。     异构\n Jobs之间的固有差异  显存使用 核使用 带宽敏感性 job之间的干扰     传统调度器将job视作黑箱，无法取得最优的集群效率\n   作业内可预测\n    Gandiva\n Gandiva利用可预测性来执行剖析驱动的自省。  它使用小批量的进度率来不断反省其决策，以提高集群效率。 例如，只有在内存和GPU利用率较低时，它才会将多个作业装箱在同一个GPU上 它动态地将通信密集型作业迁移到更多的亲和力强的GPU上 它还会适时地 \u0026ldquo;增长 \u0026ldquo;作业的并行程度，以利用空闲资源，并在空闲资源消失后缩小作业。 我们目前实施的自省策略是一个有状态的试错策略，它是可行的，因为它的预测能力很强。   Gandiva除了特定的内省核调度策略，还提供了一些API API：（a）高效的挂起恢复或时间切片，（b）低延迟迁移，（c）细粒度监控，（d）弹性，以及（e）动态优先级。  这些原语高效的关键是Gandiva的协同设计：跨越调度器层与DLT工具包层（如pytorch， tensorflow)   通过利用GPU集群的专用性，Gandiva为深度学习的特定工作负载定制了调度器，从而为调度器提供了对工作的更多可见性和控制，同时仍然实现了对任意DLT工作的通用性。 Gandiva的实现  修改两个流行的框架 PyTorch 和 Tensorflow ，为调度程序提供必要的新原语 并在 Kubernetes 和 Docker 容器之上实现了一个初始调度策略管理器      本文贡献\n 我们说明了深度学习工作流程的各种独特特征，并将其映射到集群调度所需的具体要求。 我们确定了DLT作业调度策略可以使用的通用原语，并提供了应用感知技术，通过利用DL特有的作业内周期性知识，使时间切割和迁移等原语的效率提高了一个数量级，从而变得实用。 我们提出并评估了一个新的自省式调度框架，该框架利用DLT工作的特定领域知识来不断完善其调度决策，从而显著改善早期反馈时间并提供高集群效率。    Backgroud   反馈驱动的探索。\n 实现高精确度的一个前提条件是模型的选择。新模型的发现，如ResNet或Inception，如今大多是一个试错的过程，尽管使之自动化的方法是一个活跃的研究领域。 除了模型结构外，还有一些参数，称为超参数，也需要作为DLT工作的一部分被指定。超参数包括模型中的层数/权重、最小批量大小、学习率等。这些参数通常由用户根据领域知识和试错来选择，有时甚至会导致早期训练失败。 因此，DLT工作的早期反馈是至关重要的，特别是在训练的初始阶段。    multi-job\n  一旦用户确定了要进一步探索的特定模型，用户通常会进行超参数搜索以提高任务的准确性。\n  这可以在超参数空间上使用各种搜索技术来完成；也就是说，用户生成多个DLT任务或多任务，每个任务使用一组超参数或配置进行全面训练。由于用户通常会探索数百个这样的配置，这个过程在计算上是很昂贵的。\n  因此，文献中出现了复杂版本的超参数搜索，如HyperOpt和Hyperband。\n 例如，Hyperband最初可能会产生128个DLT作业，并在每一轮（例如100个小批量迭代）中，杀死一半精度最低的作业。\n   同样，对于这些算法来说，对整个作业集的早期反馈是至关重要的，因为否则他们将无法做出有效的训练决定。\n    DLT job的特征 对位置（locality)敏感  多GPU DLT工作的性能取决于分配的GPU的亲和力。  不同的DLT工作对GPU间的亲和力表现出不同程度的敏感性。 即使是同一台机器上的GPU，由于不对称的架构，我们观察到不同程度的GPU之间的亲和力  两个GPU可能位于不同的CPU插槽（表示为DiffSocket） 在同一个CPU插槽，但在不同的PCIe Switch（表示为SameSocket） 在同一个PCIe Switch（表示为SamePCIeSw）      \rimage-20221108193230029\r\n 图1显示了两个模型VGG16[44]和ResNet-50[24]对服务器内定位的不同敏感性。  当使用Tensorflow的两个P100 GPU进行训练时，VGG16在不良定位下受到很大影响。 在最差的定位下，当两个GPU位于不同的CPU插座上时，VGG16只实现了最佳定位配置的60%，即两个GPU被放置在同一个PCIe开关下。 另一方面，在这种设置下，ResNet-50不受GPU定位的影响。这是因为VGG16是一个比ResNet-50更大的神经模型，因此在每个小批次中的模型同步会在底层PCIe总线上产生更高的通信负荷。   我们在分布式环境中观察到类似的趋势。图2显示了一个4GPU Tensorflow作业的性能，它以不同的服务器间定位运行，训练ResNet-50和InceptionV3[46]模型。  即使是用40G InfiniBand网络互连，当作业被分配到4个GPU时，性能差异明显，  其中它们均匀地分散在4台服务器（表示为4*1-GPU） 2台服务器（表示为2*2-GPU） 以及全部在一台服务器（表示为本地4GPU） 尽管两个模型对位置性的敏感性不同。     因此，DLT调度器在分配GPU时必须考虑到作业对位置的敏感性。  对干扰敏感  当在一个共享的执行环境中运行时，DLT工作可能会因为资源争夺而相互干扰。我们再次观察到，不同的DLT工作表现出不同程度的干扰。  \rimage-20221108193107517\r\n  即使对于单GPU作业，也存在干扰。\n 图3显示了: 当把一个语言模型作业（标记为LM）与另一个作业放在同一个PCI-e交换机下时，由于服务器内的干扰而导致的性能下降情况。  当两个LM一起运行时，两个作业都会遭受19%的减速。 然而，ResNet-50并没有受到GPU与LM共处的影响。 神经机器翻译（GNMT）[51]对LM的干扰程度不大。 同样地，我们也观察到不同类型的训练模型对多GPU训练的不同程度的干扰。      图4显示了用40G InfiniBand网络连接的两个4GPU服务器上的服务器间干扰。\n 当运行多个2-GPU作业时，每个GPU被放在不同的服务器上，  ResNet-50显示出高达47%的减速， InceptionV3显示出30%的减速， 而DeepSpeech[23]仅显示出5%的减速。      总之，不同应用领域的流行深度学习模型，如视觉、语言和语音，表现出对位置性和干扰的不同程度的敏感性。为了迎合这些挑战，Gandiva利用了DLT工作的一个关键特征，我们接下来会详细说明。\n  job内可预测性 \rimage-20221108193308480\r\n 一个DLT作业包括许多小批量的迭代， 呈现对应的周期性。  图5(a)显示了在四个K80 GPU上使用ResNet-50模型对ImageNet数据进行训练的20s快照期间使用的GPU总内存。  所使用的GPU内存明显遵循一个周期性模式。 每个周期都对应着一个小批次的处理（大约1.5s），内存在前向传递中增加，在后向传递中减少。 使用的最大和最小的GPU内存分别为23GB和0.3GB，或77倍的系数。 这个比例随着迷你批处理量的增加而扩大（通常在16到256之间；本例中为128）。   图5(b)显示了在一个K80 GPU上使用GNMT模型时，对WMT'14英语德语数据集进行训练的20s快照所使用的GPU总内存。  虽然小批量迭代与ImageNet的例子不完全相同（由于不同的句子长度和PyTorch中使用的动态图），但该图具有类似的循环性质。 最大值和最小值之间的差异较小（3倍），主要是由于较大的模型（0.4GB）和较小的迷你批次大小（本例中为16）。   除了这里显示的图像和语言模型外，其他训练领域，如语音、生成式逆向网络（GAN）和变异自动编码器都遵循类似的循环模式（由于空间限制没有显示），因为训练的核心是梯度下降算法，执行许多小批量迭代。   充分利用可预测性。  首先，一个DLT作业可以被自动分割成小批量的迭代，这些迭代在60秒内的集合，例如一个微任务，形成一个调度间隔。 第二，通过在内存周期的最小值上执行暂停操作，可以大大减少从GPU复制到CPU中的内存量，从而使暂停/恢复和迁移的效率比naive的实现要高一个数量级。 第三，可以对小批量的进度进行分析，并将其作为评估装箱或迁移等机制的有效性的代理。    设计 \rimage-20221108193438681\r\n 如今的集群中出现高延迟和低利用率的问题  是因为DLT作业被专门分配了一组固定的GPU。 独占访问 GPU 的原因行头阻塞，阻塞了早期反馈，导致作业的排队时间过长。 当作业无法完全利用其分配的GPU时，对一组固定的GPU的独家访问也会导致GPU的低利用率。    机制  三种方式消除GPU对DLT作业的排他性和固定分配来解决低效率问题  首先，在过载期间，Gandiva允许后来的工作与现有的工作共享GPU  而不是等待当前工作的离开。 这是为DLT作业定制的挂起-重启机制和选择性装箱而实现的。   第二，Gandiva支持DLT作业从一组GPU到另一组的高效迁移  迁移允许时间碎片化的作业迁移到其他（最近空出的）GPU上 或者对集群进行去碎片化处理，从而使后来的作业被分配到具有良好位置性的GPU上。   第三，Gandiva支持GPU增长-缩减机制  这样空闲的GPU就可以适时地被使用。 为了有效地支持这些机制并实现有效的资源管理，Gandiva通过不断地剖析DLT作业的资源使用情况并估计其性能，对DLT作业进行内省。      挂起-重启与装箱   Suspend-resume\n 挂起-重启是Gandiva用来消除一组GPU对DLT作业的独占性的一种机制。  Gandiva利用这种机制，增加了对GPU时间分割的自定义支持。   Gandiva的关键思想是利用这种周期性行为，在DLT作业的GPU显存使用量最低时暂停-恢复。  1）发出暂停调用 2）DLT工具包会等到内存使用周期的最小值，将存储在GPU中的对象复制到CPU，释放其所有GPU内存分配（包括缓存） 3）然后调用经典的CPU暂停机制 4）之后，当CPU恢复工作时，DLT框架首先分配适当的GPU内存，将存储的对象复制回GPU，然后恢复工作。   Suspend-resume也可以在同一台服务器中启动GPU的更换  虽然更换GPU的成本很高，但我们可以把这个延迟从关键路径中隐藏  典型的图像分类工作，暂停-恢复一起可以在100ms内完成 而对于大型语言翻译工作，暂停-恢复可能需要1s   考虑到1分钟的时间切分间隔，这相当于2%或更少的开销   延迟  Gandiva中的suspend可能最多延迟DLT作业的一个小批次间隔 值得  由于减少了GPU-CPU的复制成本和更少的CPU使用的内存，它的开销明显减少 在这个延迟期间还完成了有用的工作。   调度器跟踪这一延迟，并相应地调整时间分割的间隔，以实现公平性      装箱\n 暂停-恢复的另一种方法是装箱  在一个GPU上同时运行多个DLT作业，让GPU分担作业的时间。   有效性  只有当装箱的作业不超过GPU的资源，并且不会对彼此产生不利影响时，GPU中的装箱才是有效的。 如果作业相互干扰，装箱就会比暂停-恢复差很多。   装箱的使用  当DLT作业有排他性的访问时，我们使用Profiling来监测它们的资源和进度 如果两个作业被确定为装箱的候选者，我们就把它们装箱在一起，并继续监控它们 如果给定的装箱结果对作业的性能产生了不利影响，我们就解除这些作业的装箱并恢复到暂停-恢复状态      迁移  迁移是Gandiva用来改变分配给DLT作业的GPU集的机制。  迁移在几种情况下是有用的  i）将时间分割的作业移到集群中任何地方的空闲GPU上 ii）将相互干扰的作业移开 iii）对集群进行去碎片化处理，使进入的作业获得具有良好位置性的GPU   我们评估了两种解决DLT进程状态迁移的方法。  在第一种方法中，我们利用一个通用的进程迁移机制，如CRIU。  CRIU本身不支持使用GPU设备的进程迁移  我们首先对GPU对象进行检查 调用CRIU之前从进程中删除所有GPU状态   因为CRIU检查点和恢复整个进程的内存  对于这些使用PyTorch的DLT作业来说，检查点的大小是GB级别的。 对于单GPU作业来说，所产生的迁移开销约为810s，对于多GPU作业来说则更高。     我们考虑的第二种方法是使用具有检查点意识的DLT作业。  Tensorflow等DLT框架已经支持允许自动检查点和恢复模型的API 通过在迁移前对目的地进行预热，并且只迁移必要的训练状态。我们可以降低迁移开销小到一两秒     无论采用哪种方法，我们发现，与它在提高GPU总体利用率方面提供的好处相比，服务器间迁移的开销是值得的    增长与收缩  Gandiva用来消除GPU对DLT作业的排他性的第三个机制是增长-收缩。  这种机制主要针对集群可能没有被完全利用的情况 基本思想  在空闲时间内，适时地增加可用于作业的GPU数量 在负载增加时相应地减少可用的GPU数量   许多DLT工作，特别是在图像领域，随着GPU数量的增加，会看到线性的性能扩展。  Gandiva只对那些特别声明他们有足够的适应性来利用这些增长机会的DLT工作应用这一机制。 当多个DLT工作符合这一标准时，Gandiva使用Profiling来估计每个工作的进度，然后相应地分配GPU。      Profiling  Gandiva监控资源使用情况  如CPU和GPU利用率，CPU/GPU内存等。 Gandiva的独特之处在于，它还以一种应用感知的方式内省  利用了DLT作业表现出的规律内省 利用周期性以估计DLT进度   Gandiva估计DLT作业的mini_batch时间  即对一批输入数据做一次前向/后向传递的时间，作为GPU内存使用周期的两个最小值之间的时间 由于DLT作业在其生命周期中通常会执行数百万次这样的小批量操作，调度器会在调度决策之前和之后比较DLT的mini_batch时间以确定其有效性。   Gandiva可以决定装箱是否有效  通过比较装箱前后两个DLT作业的小批量时间 如果没有这样的剖析，为了做出装箱的决定  人们不仅要对两个DLT作业在不同GPU上的性能进行建模， 还要对它们可能相互干扰的各种方式进行建模（例如，缓存、内存带宽等） 这不是一项简单的任务        调度原则   定义：在我们描述调度器的细节之前，我们定义一些术语。\n DLT作业  被封装在容器中, 包括  所需的GPU数量 优先级（可以是动态的 ） 一个指示作业是否能够增长-收缩的标志   我们假设一个作业所要求的GPU数量是2的幂。   集群  一个集群由一个或多个服务器组成 每个服务器有一个或多个GPU 我们假设一个专门的GPU集群用于DLT工作   服务器的高度  定义为⌈M/N]， 其中M是分配的GPU数量，N是总GPU的数量。 只有当服务器的高度超过1时，才会使用暂停/恢复机制。   集群的高度  被定义为其所有服务器的最大高度。 当集群的高度大于1时，就会出现过载；即所有作业的请求/分配的GPU之和大于GPU的总数。   服务器的亲和力  被定义为分配给该服务器的作业类型。 例如，最初服务器的亲和力为零，如果一个需要两个GPU的作业被分配到一个服务器上，那么该服务器的亲和力就会变成两个。 这个参数被调度器用来将具有类似GPU需求的作业分配到同一台服务器上。      目标\n Gandiva调度器的主要设计目标是为作业提供早期反馈。  在流行的调度器中，作业在过载期间会在队列中等待。 Gandiva通过立即为新作业分配GPU并使用suspend-resume机制提供早期结果来支持超额订阅。   第二个设计目标是集群效率。  通过一个持续的优化过程来实现，该过程使用了剖析和贪婪的启发式，利用了诸如装箱、迁移和增长-收缩等机制。   集群级的公平性不是Gandiva的设计目标。  只关注使用暂停-恢复机制在每个服务器上提供作业之间的公平性 集群级的公平性留给未来工作   为了实现这些目标，Gandiva调度器以两种模式运行：  响应模式：调度器对诸如工作到达、离开、机器故障等事件做出反应 内省模式： 一个持续的过程，过程中，调度器的目标是提高集群的利用率和作业完成时间 请注意，调度器可以同时在两种模式下运行。      响应模式 响应模式被设计用来处理诸如工作到达、离开和机器故障的事件\n\rimage-20221108220646915\r\n当一个新的作业到达时，调度器为该作业分配服务器/GPU。\n  Gandiva使用的节点分配策略如算法1所示。\n findNodes是一个函数，用于返回满足作业请求的节点候选者，并有一个亲和力约束的可选参数。 最初，Gandiva试图找到与新工作具有相同亲和力的节点，并在这些节点中找到具有最小负载的节点。如果存在这样的节点，并且它们的高度小于1（第5-6行），该节点将被分配。 否则，Gandiva试图找到并分配未亲和的节点（第7-8行）。 如果没有这样的空闲服务器，第三个选择是寻找有空闲GPU的节点，同时忽略亲和力（第9-10行）。 这可能会导致多个节点之间的碎片化分配，但正如我们将在后面看到的，迁移可以用于碎片化。如果上述方法都不奏效，这意味着集群中没有可用的GPU。在这种情况下，如果存在具有相同亲和力的节点，它们将被用于暂停-恢复（第11-12行）； 如果没有，作业将被排队（第13-14行）。    放置job\n   传统的调度器将使用作业离开来触发从等待队列中挑选下一个作业。\n   Gandiva检查集群的高度是否可以减少\n 将被暂停的作业迁移到新的空闲GPU上 这个作业可能来自同一台服务器或集群中的任何其他服务器。 作业的离开也可以触发迁移，以改善位置性      Gandiva的工作安排Policy时考虑到了两个因素。\n Gandiva允许超额认购。  当一个服务器被超额认购时，我们会进行加权轮流调度，给每个作业公平的时间份额。   GPU分配不是作业到达时的一次性事件，Gandiva使用自省模式来持续改善集群的利用率。  因此，Gandiva依靠一个简单的作业安置策略来快速分配GPU资源给新作业，从而实现早期反馈。      内省模式 在自省模式下，Gandiva持续监控并优化作业在集群中的GPU上的位置，以提高DLT作业的整体利用率和完成时间。\n  装箱\n 只有在过载时才会考虑装箱。  基本思想：在GPU上同时运行两个或多个作业以提高效率。 可能无效  如果装箱工作的内存需求加起来高于GPU的内存，那么从CPU内存中 \u0026ldquo;分页 \u0026ldquo;的开销就会很高，装箱就没有效果。 当两个或更多作业的内存需求小于GPU内存时，装箱仍然可能不比暂停-恢复更有效。     鉴于DLT作业的异质性，对装箱的性能进行分析建模是一个具有挑战性的问题。  相反，Gandiva依靠一种贪婪的启发式方法来装箱job。  当job到达时，我们总是使用暂停-恢复的独占模式运行它们，并收集剖析信息（GPU利用率、内存和作业进展率）。 基于剖析数据，调度器维护一个按GPU利用率排序的作业列表。 调度器贪婪地挑选出GPU利用率最低的作业，并试图将其装箱到具有最低GPU利用率的GPU上。我们只在装箱作业的总内存利用率不超过GPU的总内存时才这样做。 当装箱作业的总吞吐量大于时间切割时，装箱被认为是成功的。 如果装箱不成功，我们就撤消装箱，并尝试下一个利用率最低的GPU。 如果装箱成功，我们找到下一个利用率较低的作业并重复这个过程。   根据我们的评估，我们发现这个简单的贪婪的启发式方法实现了26%的效率提升。      迁移\n GPU的位置性在一些作业的性能中起着重要作用。 在Gandiva中  每当作业离开时，我们都会使用迁移来改善位置性， 同时也作为一个后台进程来 \u0026ldquo;整理 \u0026ldquo;集群的内容。 为了改善位置性，我们挑选那些不在同一地点的作业，并试图找到一个新的同一地点的位置。     \rimage-20221109111201868\r\n 图8展示了一个集群实验的例子（第6.4节）。  当一个有4个作业、每个作业需要2个GPU的多作业被安排时，它的GPU亲和性很差；  只有J0的两个GPU被安排在一起，而多作业中的其他3个作业（J1、J2和J3）被分配到不同的GPU。 三分钟后，一个背景训练作业DeepSpeech完成了，并释放了它的8个GPU。 8个GPU中的3个，在图8中标记为D，位于三个不同的服务器（服务器1、3和4），可以提高多任务的训练效率。 因此，Gandiva启动了迁移过程，将J1、J2和J3重新安置到同地的GPU。        去碎片化  我们在所有非空闲的服务器中挑选出拥有最多空闲GPU的服务器。 然后我们尝试将运行在该服务器上的作业迁移到其他服务器上。 只要性能损失可以忽略不计，作业就会被迁移到另一个空闲GPU较少的服务器上。 我们重复这个过程，直到每台非空闲服务器上的空闲GPU数量少于阈值（在我们的实验中是4个中的3个），或者没有作业会从迁移中受益。      扩缩\n 增长-收缩的条件  集群未被充分利用 DLT作业明确指出自己可以进行增长-收缩   限制  在我们目前的系统中，我们只让作业增长到单台服务器中可用的最大GPU数量。 此外，我们只在空闲一段时间后触发增长，以避免惊扰， 在新作业可能需要GPU时立即收缩。      时分\n 我们在每个服务器中支持轮回调度，以公平地分享GPU。 当作业有多个优先级时，较高优先级的作业将不会被暂停以适应较低优先级的作业。 如果一台服务器被较高优先级的作业完全利用，如果可行的话，较低优先级的作业将被迁移到另一台服务器。    实现 DLT作业被封装为Docker容器，其中包含我们定制的DL工具箱和Gandiva客户端的版本。这些工作被提交给Kubernetes系统。Gandiva还实现了一个定制的调度器，然后对这些作业进行调度。\n调度器   Gandiva由一个定制的中央调度器和一个客户端组件组成，客户端是每个DLT工作容器的一部分。\n  调度器只是另一个由Kubernetes管理的容器。\n Kubernetes负责整体集群管理， Gandiva调度器管理DLT作业的调度。 Gandiva调度器使用Kubernetes API来获取集群节点和容器信息，每当提交一个新的容器时，调度器会根据调度策略将其分配给集群中的一个或多个GPU。    当一个容器被安排在一个节点上时\n 最初只有Gandiva客户端开始执行。 然后，它轮询Gandiva调度器，以确定哪些GPU可用于DLT工作， 并使用暂停/恢复和迁移命令控制DLT工作的执行。    虽然我们集群中所有GPU的调度完全由中央调度器控制，但如果弹性成为我们关心的问题，可能需要一个分层的方法。\n  DL工具的修改   PyTorch的时分。\n  发出SIGTSTP信信号\n Gandiva client会发出一个SIGTSTP信号，表示工具包必须暂停进程。 它还指示恢复是否应该通过内存文件在新的GPU中发生。 收到信号后，工具包会设置一个暂停标志，并且只在一个小批处理边界的末端执行暂停。    识别mini-batch边界\n 在Tensorflow这个定义并运行的工具包中，mini-batch的边界很容易识别（session.run()的结束）。 在PyTorch这个逐一定义的工具包中，  我们通过跟踪GPU内存使用周期来确定mini-batch的边界， 这是PyTorch的GPU内存管理器（THCCachingAllocator）的一部分， 每当GPU内存被释放时，我们就会寻找一个周期最小值。      执行暂停\n 一旦检测到周期最小值，工具包 i）将所有存储的对象从GPU复制到CPU，ii）释放GPU分配，以及iii）暂停进程。    恢复进程\n 当Gandiva客户端发出SIGCONT信号时，工具包会分配GPU内存，将存储的对象从CPU复制到GPU，并恢复进程。 为了处理重新启动时的地址改变，我们跟踪工具包中的GPU对象，并用新的地址对其进行修补。 改变GPU需要调用cudaDeviceReset和CudaInit，这可能需要5-10秒。我们通过在 \u0026ldquo;暂停 \u0026ldquo;时在后台执行这些操作来隐藏这个延时      Tensorflow迁移\n 我们在每个服务器上部署了一个迁移助手，以支持按需checkpoint和迁移。  当收到来自调度器的迁移命令时，目的地助手首先预热TF会话并等待检查点的到来。 然后，源帮助者要求TF保存检查点，在跨服务器迁移的情况下将检查点移到目的地，最后恢复训练会话。 为了加快迁移过程，我们采用Ramdisk将检查点保存在内存中。在跨服务器的情况下，修改后的TF通过网络文件系统（NFS）协议将检查点直接保存到远程Ramdisk。   当Migration Helper要求作业执行checkpoint时  修改后的TF会在一个小批处理结束时调用tf.Saver。 对于数据的并行性，检查点只包括一个GPU中的模型，而不考虑训练中使用的GPU数量。 为了进一步加快TF的迁移，我们在检查点中不包括元图结构，因为它可以根据用户代码进行重建。   在预热阶段，修改后的TF检查GPU配置并重建元图。  它进一步创建Executor来运行预热操作，以确保初始化不会被懒惰地推迟。 当恢复训练过程时，修改后的TF加载检查点，由多个GPU并行加载，并继续进行训练。      ","date":"2022-11-08T09:59:52+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202211081933118.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0gandiva%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Gandiva论文阅读笔记"},{"content":"docker 的使用方法 docker的安装 docker的版本比较多， 大家可以自行搜索安装方式， 以下只是参考。\nUbuntu 安装docker.io比较方便\n查询可安装版本 apt-cache madison docker 安装docker sudo apt install docker.io 指定安装版本 sudo apt install docker.io=18.09.1-0ubuntu1 启动docker sudo systemctl start docker 重启docker sudo systemctl daemon-reload sudo systemctl restart docker 设置开机自启动 $ sudo systemctl enable docker 或 $ sudo systemctl enable docker.service --now 验证docker systemctl status docker docker --version centOS 需要安装docker-ce， docker社区版\n卸载旧版本 yum remove docker docker-common docker-selinux docker-engine 安装依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 设置源 阿里源\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 清华源\nyum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装 查看可安装版本\nyum list docker-ce --showduplicates | sort -r 选择版本安装\nyum -y install docker-ce-18.03.1.ce 启动docker systemctl start docker systemctl enable docker docker 的基本命令 本地镜像管理  images rmi tag build history save load import  容器生命周期管理  run start/stop/restart kill rm pause/unpause create exec  容器操作  ps inspect top attach events logs wait export port stats  容器rootfs命令  commit cp diff  镜像仓库   login\n  pull\n  push\n  search\n  https://www.runoob.com/docker/docker-import-command.html)\n  info|version  info version  docker 部署实例 Hello World runoob@runoob:~$ docker run ubuntu:15.10 /bin/echo \u0026quot;Hello world\u0026quot;\rHello world\rRedis docker pull redis:latest\rdocker run -itd --name redis-test -p 6379:6379 redis\rdocker exec -it redis-test /bin/bash\r相关问题 1. docker.io 和docker-ce 的区别  维护者：\n docker-ce 是 docker 官方维护的 docker.io 是 Debian 团队维护的  依赖管理：\n docker.io 采用 apt 的方式管理依赖 docker-ce 用 go 的方式管理依赖，会自己管理所有的依赖。   2. ","date":"2022-11-01T16:44:33+08:00","permalink":"https://tweakzx.github.io/p/dockerdocker%E7%94%A8%E6%B3%95/","title":"【Docker】Docker用法"},{"content":"突发恶疾 关于手头的工作 现在在工位上摸鱼，准备在别人的代码（gaiaGPU）上增加一些功能。\n但是自己看相关的代码，论文，知识已经过去好久了，始终没有下手写出代码。\n似乎折腾了很久，确实也没有弄懂什么。\n这个项目是课题组唯一的项目了，按道理来说我应该有着大量的精力来做这件事，但不知为什么总是精神郁郁。\n关于为什么会突发恶疾 所谓突发， 无非是突然间什么都不想做了，进入到一种抑郁颓废的状态。\n听到后面课题组的学姐在吐槽她带的一个学生能力很差，无法推进工作。\n我听到之后觉得好像是有很多蚂蚁在身上爬，毕竟我也能力很差，项目做了一段时间但是没有推进。\n另一方面就是导师把我手头的工作好像偷偷交给一个学弟来做了，这是让我更加有压力的一件事。\n毕竟导师是一个擅长阴阳怪气揶揄讽刺的人。\n他偷偷的这样做虽然不一定是出于坏心，但是学弟如果做出来了，进度比我快的话，那我一定是免不了收到一些不大友善的评论的。\n什么是我想要的呢 这个项目涉及到k8s， gpu， docker，机器学习，似乎项目涉及的东西很多，我一时间摸不到该怎么扩充自己的知识。\n实际要做的功能又似乎什么都不需要懂，只要在别人的代码上加一点逻辑就行，可是我看源码又是摸不到该在什么地方加。\n似乎我总是在制造障碍，阻碍自己做到这件事。\n我总是希望有个懂做项目的人来指导一下我，点破当前的迷津。\n可是导师肯定是指望不上了，他几乎什么都不懂，组里的同学做的东西又都不一样。\n现实往往是\u0026hellip; 不光是导师水平奇差，没有指导。\n自己在学习这件事上也是不太擅长，每次打算要学习什么东西，总是感觉自己学不会。\n或者说根本体会不到在学习的感觉，更别谈学会什么的感觉了。\n可能是多年的高考应试思维吧，几乎是不学什么真正的知识，而是利用记忆力去完成一些事情。\n大学几年下来没有将什么知识真正转化成我的技能。\n差不多都是在考前一天利用记忆力通过考试，分数不会很高但也不会很低。\n​\n我觉得长期的抑郁情绪已经严重影响到我的记忆能力了。\n我开始频繁的发现自己的记忆卡壳。\n本来就没有实际技能的我，失去了原本的一些优势之后就真的什么都没有了。\n现在要自己上手做一些开发，对我来说真的就是很困难。\n这样一门动手的学科我却恰恰缺乏动手的能力。\n​\n将来的工作似乎也无法获得解脱。\n在工作上，如果也是自己去漫无边际的摸索，那对我来说将是一场苦旅。\n在这样容易迷失的荒漠之中，我急切需要一个愿意提供指导的引路人。\n从来没有成长，只有伤痕的增加 人们常说吃一堑长一智，这句话对我来说似乎不太起作用。\n我似乎在成长这件事上也做的很是吃力，痛苦随着年龄的增加似乎出现了指数级的增长。\n我总是在毕业之后才能明白，有些孤独与矛盾是没有必要存在的。\n关于本科阶段为什么停滞 似乎高考留下的后遗症，大一的时候一直卷高数，卷完高数，卷大物，还要卷大学的其他课程\n从来没有胆量考虑过自己要做一个不能加学分的项目，目光一直盯着比赛，大创，以及很水的本科课程\n结果就是成绩还不错，专业前十保研到了计算所\n让我回望本科，后悔的事情有很多，\n一是没有一个感兴趣的方向，然后努力学习下去\n二是没有很好的和同学做好朋友，大概是竞争导致的，和同学有所疏远\n三是没有开始刷题，如果当时多刷一点题的话，保研和找实习的笔试都会比较顺利\n关于第一件，我实在是见到的，知道的太少了\n似乎从来都缺少一个契机，漫无目的，没有方向\n在不知道什么的竞争心理作用下，被看不见的锁链困在原地，不敢真正的做点有用的东西\n学习当然也可能没有那么重要，毕竟人的变化不一定能跑赢环境的变化\n第二件的话，实话说和同学相处的都还可以，起码没有关系很差的同学\n但也很少有关系很好的同学\n关于室友，起初可能就是一些细小的矛盾和危机感导致的\n关系还不错，但是总是有一些生疏，就是很有礼貌的生疏\n大家都各自有了朋友，可能是太多缺乏安全感导致的，我对朋友去交别的朋友这件事十分敏感\n现在想来真的是，明明把很多事情说开，活得会轻松很多，朋友之间相处也会自在很多\n积郁着，总是淡淡愁，如同雨季里听不见远方的笑声\n一个人在细雨打湿的长椅上，看着微微透着光亮的阴天\n最喜欢在校园里行走，浓荫遮蔽的交大校园，适合浮躁的人散散心\n关于我的敏感，焦虑，危机感导致和朋友疏远这件事\n我想我总是负着一些愧疚的，或许我轻松一点，我周围的朋友也可以轻松一点\n其实隐藏在这背后的，软院的学习氛围其实推动了一些同学包括我的心理的扭曲\n学院不算很卷，但是资源并不多，实话说课程质量其实不太能提高学生的实际水平\n学院提供不了学生需要的成长与发展，尤其在计算机这个看重能力的领域\n导致学生出现焦虑也很正常\n大家都想快速获得一些发展，超越同龄人，导致了一些黑暗森林法则的事情的发生\n不要出声，自己在学什么，自己在做什么，不要出声\n闷声发大财的理念是大家的默契\n然而闷声的很多，发财的很少\n关于第三件事，没有好好刷题，其实是根本没有刷题的方法与紧迫感\n当时的紧迫感主要来自周围的同学都在做什么\n刷leetcode题对绩点能有什么帮助呢？\n即使我们都知道找工作要刷算法题，保研也要过算法考试\n但是大三前大家没有这样做的打算，大三后也已经来不及\n另外就是我当时不知道刷算法的路线与教程\n自己打开lc，随便看几道都是不会做的题\n甚至想到解法，代码的实现细节还是无法落地\n现在还好刷了400多道题，虽说不是刷题高手，但是起码不会害怕题目了\n这件事为什么会导致我的后悔呢？\n没有刷题导致我错过了很多机会\n关于保研的时候为什么做出错误的选择 保研的时候，一方面自己的简历没有出众的地方\n甚至关于保研的准备就是拿到了保研资格，保研之后的找导师几乎是没有准备的\n面试也都是几乎没有准备的，自己简历空有绩点而无实质\n现在的导师，是让我很后悔的一个选择\n导师的学术能力很差，课题组没有实际的科研方向，捞钱是唯一的方向\n除了提供一些十分牵强离谱的idea，提供不了什么指导\n在他身上我看到了自卑和自负是一枚硬币的正反两面\n然而我自己也没有什么想法，或许选择做科研并不是一条适合我的路\n科研嘛，要么导师有好的想法，要么自己有\n要是都没有，那就不好搞了，尤其是导师沉浸于宏大叙事而无实事求是能力的情况下\n选择他，主要还是因为当时并不知道，中科院计算所的导师水平可以这么差\n原本以为导师脾气差一点无所谓，脾气差可能是要求高，对自己不一定是一件坏事\n但后来我发现，脾气差往往是无能的表现，脾气越差的人越是无能狂怒\n当然可能是网络上被不太好的评价刺激到了\n导师现在十分收敛，对学生天天是点赞，夸奖，尬吹，（阴阳）\n大家对他的夸奖感到十分的厌烦，可以说导师的夸人的情商非常低\n一个错误的选择真的耽误了我很长的发展时间\n甚至未来两年也不会太乐观\n关于刚刚被骗钱的事 对了，倒霉的事对我来说可不止一件\n9月底，我经历电信诈骗，被骗走了很多钱\n刚刚问了警察同志，反正钱大概率是追不回来了\n算了，就当丢了吧\n损失这么惨重，导致我10月其实有一点消沉\n不过还好，工作不顺利才是我头疼的主要原因\n紧接着被拉去酒店自费隔离，是真的，本不富裕的生活雪上加霜\n交完酒店的钱，山穷水尽了，啊不，我还有外债\n去他妈的人生\n关于对别人的羡慕 一个负面情绪很多的人对正常人的自然羡慕 我真的负面情绪很多吗？我想说不\n我表现出来的很多，但更多的是无奈与疲惫\n这些都是可以解决的，如果我能力突飞猛进\n或者有一个适合我成长的环境，很多问题都会得到缓解\n我常常看到周围的同学目标明确飞速进步而感到焦虑\n但每个人都有不同的发展空间嘛，发展的时间节点也不太一致\n大不了给自己一点时间，晚几年也没什么的\n我的焦虑是现实原因导致的，几乎无法缓解\n周围的同学可以自由的成长和生活，免不了让我心生很多羡慕\n一切都来得及 但是我还是会许愿\n一切都来得及，每个人都有不同的人生课题\n人这一辈子自己是唯一的长期观众\n","date":"2022-10-21T15:52:45+08:00","permalink":"https://tweakzx.github.io/p/%E5%BF%83%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%BE%80%E4%BA%8B/","title":"【心及】一些往事"},{"content":"《AntMan: Dynamic Scaling on GPU Clusters for Deep Learning》论文阅读笔记 Abstract 如何在大规模GPU集群上有效调度深度学习工作， 对于工作性能，系统吞吐量和硬件利用率至关重要。\n随着深度学习的工作量变得更加复杂，它变得越来越具有挑战性。\n本文将介绍Antman， 这是一种深入学习的基础设施，该基础架构共同设计了集群调度程序，并已在阿里巴巴部署在生产中，以管理数以万计的每日深度学习工作。\n Antman适应深度学习训练的波动资源需求。因此，它利用备用GPU资源在共享GPU上共同执行多个作业。 Antman利用深度学习训练的独特特征，在深度学习框架内为显存和计算资源引入动态缩放机制。这允许job之间的细粒度协调并防止工作干扰。 评估表明，Antman在我们的多租户集群中不损害公平性的情况下，整体将显存利用率提高了42％，计算资源利用率提高了34％，为有效利用大规模的GPU提出了新方法。  Introduction  在共享多租户的DL集群， 许多工作排队等待资源的时候会导致GPU利用率低下，有两个原因  大多数的训练任务在执行过程中不能完全利用GPU资源  训练一个DL模型通常需要多种计算的混合 当使用分布式的训练的时候，90%的时间会被浪费到网络通信上   基于资源预留的集群调度方案导致显著的GPU空闲，DL工作中总有部分资源没有投入使用  例如，随机梯度下降是同步的，需要获取所有的资源以进行gang-scheduling， 在得到所有资源之前，已得到的部分资源就会陷入空闲     在共享GPU上进行packing job  可以提高GPU的利用率，可以使得同样的集群整体上胜任更多的job。 但是这个策略在生产集群上很少使用， 原因  尽管提升GPU的利用率是有利的，但也要保证resource-guarantee jobs（RGJ，资源保证性job）的性能。同一个GPU上同时执行多个job会导致干扰\u0026ndash;\u0026gt;RGJ性能出现显著下降 job packing策略可以给并发job引入内存竞争。job需要的资源陡然增加的话，有可能导致训练任务failure。   因此，job资源的独占分配在显存的GPU集群生产环境中比较典型   我们提出AntMan  简述  一个DL系统提高GPU集群的利用率 同时保证公平性与RGJ的性能 通过合作性的资源扩缩来减少job干扰   DL系统中引入了新的分配机制在job训练过程中来动态地精确分配所需资源（显存和计算单元） 使用超卖机制使得任何空闲的GPU资源（显存和计算单元）都能被利用 重新设计了集群调度器和DL框架来适应生产job的波动的资源特点  通过  框架信息感知调度 透明显存扩展 快速可持续的任务间协调   使用这个结构，Antman为DL任务的同时执行的policy design 开辟了空间   AntMan采用了简单且有效的策略来最大化集群吞吐  为RGJ提供资源保证的同时 分配一些偶然性的任务来尽力而为的利用GPU资源（低优先级且不保证资源）     本文主要贡献如下  对生产环境的DL集群进行调研，发现低利用率来自于三个方面：硬件，集群调度，job行为 在DL框架中为显存和计算单元管理引入了两种动态放缩机制，来解决GPU共享问题。新机制利用DL的工作特征来动态调整DL job的资源使用情况，在作业执行期间。 通过共同设计集群调度器和DL框架以利用动态缩放机制，我们为GPU共享引入了一种新的工业方法。这在多租户集群中维护工作服务级协议（SLA），同时通过机会调度来改善集群利用率。 在超过5000个GPU上进行了实验    Motivation deep learning Training  深度学习训练通常包括数百万次迭代，每个迭代过程都称为mini-batch。  通常，训练mini-batch可以分为三个阶段。  首先，计算样品和模型权重以产生一组得分，称为forward pass 其次，使用目标函数在产生的分数和所需的分数之间计算loss error。然后，损失通过模型向后扩散，以计算梯度，称为backward pass。 最后，通过由优化器定义的学习率来缩放梯度，以更新模型参数。   正向通行的计算输出通常包括许多数据输出，每个数据输出称为张量。这些张量应暂时保存在内存中，并被向后通过以计算梯度。通常，为了监视培训中的模型质量，定期触发评估.   为了使用大量数据培训模型，DL通常在多个GPU中采用数据并行性，其中每个GPU负责并行处理数据子集，同时在模型更新之前执行每个迷你批次梯度同步。 在大型公司中，多租户集群通常用于改善硬件利用率，用户有时可以超额订阅GPU资源配额，尤其是当GPU要求爆发时。  Characterizing Production DL Cluster 我们从三个角度研究生产集群中的资源使用率：硬件，集群调度和job行为。\n  在使用中的GPU的低利用率\n \rimage-20220831200958715\r 内存容量。如图所示，只有20％的GPU正在运行消耗一半以上GPU存储器的应用程序。 关于计算单元的使用，只有10％的GPU获得了高于80％的GPU利用率。 该统计数据表明，GPU内存和计算单元均未得到充分利用，因此浪费了昂贵的硬件资源。    对gang-schedule的空闲等待\n Gang scheduling是在并发系统中将多个相关联的进程调度到不同处理器上同时运行的策略，其最主要的原则是保证所有相关联的进程能够同时启动，防止部分进程的异常，导致整个关联进程组的阻塞。例如，您提交一个批量Job，这个批量Job包含多个任务，要么这多个任务全部调度成功，要么一个都调度不成功。这种All-or-Nothing调度场景，就被称作Gang scheduling。\n  \rimage-20220831201015900\r 多GPU训练工作需要Gang scheduling，这意味着除非同时提供所有必需的GPU，否则job将不会开始训练 由于GPU资源往往不能同时全部获得， 所以会出现idle waiting。 一个job需要的GPU越多， 它的平均闲置时间就越长 **资源到达的不可预测导致了预留资源的idle waiting。**一个幼稚的解决方案是在GPU idle waiting的时候执行别的job。但是这会导致资源需求大job的饥饿，影响分配公平性。另外一旦资源需要被满足导致的GPU需求激增或许导致GPU之间的资源冲突，导致工作fail    动态资源需求\n \rimage-20220831201047910\r 在一个DL job生命周期中， GPU资源往往未被充分利用。 图3：在Criteo数据集上运行DEEPFM [20]时的前10分钟。一开始，数据集上的预处理仅需要CPU。但是，GPU的SM利用率和内存使用量在275秒时启动。 图4：训练可以包含几个阶段，不同阶段的SM和Memory的使用率都是不一样的 资源需求的动态变化与固定的资源分配和漫长的训练时间相矛盾。资源需要满足job的峰值需要，导致这个昂贵的硬件被低效利用。显存的DL框架的memory caching设计隐藏了显存使用随时间的变化，一定程度上阻止了GPU的潜在共享。    Opportunities in DL Uniqueness   超卖有机会提高集群的吞吐\n 不可预测的job内和job间的需求激增为安全的资源共享引入了挑战  因为资源竞争， jobs可能会把显存用光 在多租户集群中，当jobs在共享环境执行时，为持有定额资源的job提供性能隔离是十分重要的   AntMan利用DL training的特性来利用这个机会    我们在生产环境集群的10k个task中取样发现\n \rimage-20220905143619325\r 只有一小部分用来存储模型， 且90%的模型大小不超过500M 大部分显存在同一个mini-batch中被分配和释放 除此之外，一个mini-batch的消耗时间很短， 80%的任务在600ms之内消耗一个mini-batch    我们通过多种方式利用这种独特的特征来安排共享GPU上的作业。\n 首先，根据通常的模型大小，大部分显存在共同执行的作业中可以拿来调度 其次，mini-batch的周期通常很短，可以在每个mini-batch边界处进行细粒度的GPU内存和计算资源的调度。这可以进一步允许job间的快速资源协调。 第三，mini-batch通常进行相似的计算，这可以被利用去描述job性能，因此进度率可以被创建作为性能矩阵来量化干扰。    Design AntMan深入共同设计集群调度程序和DL框架， 本部分将介绍三部分：DL 框架的修改，调度器与调度原则\nDynamic Scaling in DL Frameworks  低使用效率的GPU集群有着执行更多任务的潜力，但需要解决一下挑战  使用最下需求执行job的同时防止GPU内存使用的爆发导致的失效 适应计算单元的使用波动的同时限制潜在的干扰   DL框架是专注于GPU执行， 缺乏job合作的能力。这激发了动态缩放机制的设计，机制包含内存和计算两部分  Memory Management AntMan以tensor为单位，在GPU和CPU的内存间进行动态内存管理。类似于操作系统的虚拟内存，AntMan以tensor为单位进行了显存虚拟化，通过这种方式，DL框架可以提供超出上限的显存。\n  显存分配\n \rimage-20220905151151692\r 在张量被销毁后，GPU内存被缓存在DL框架内的一个全局内存分配器中，普遍情况下，一些张量只在DL训练的某些阶段使用（如数据预处理、评估），不再需要了。然而，这部分缓存的GPU内存不会被释放（图6c）。DL框架中的这种缓存内存设计优化了单个作业的性能，但代价是失去了共享潜力。 AntMan转向了扩展GPU内存上限的方法。它主动检测使用中的内存，以缩小缓存的内存，从而自省地将GPU内存的使用调整合适。这是通过监测应用性能和处理小批量时的内存需求来实现的（图6d）。AntMan使用其最大的努力在GPU设备上分配张量，然而，如果GPU内存仍然缺乏，张量可以在GPU之外用主机内存分配（图6e）。当GPU内存的上限增加时，Tensors可以自动分配回GPU（图6f）。    显存上限动态调整\n  \rimage-20220905151235705\r\n  图7a说明了内存扩展如何解决突发需求。在T0，正在运行的DL训练作业的内存需求增加，由于GPU内存的有限上限，一些张量不能放在GPU内存中，而是使用主机内存创建。AntMan检测到主机内存的使用情况，在T1，它根据主机内存的使用情况提高该作业的GPU内存的上限，虽然在这一个小批次中，这个运行作业的性能可能会减慢，因为张量被放置在主机内存中，但是后续的tensor都会申请到显存上。考虑到一个典型的DL训练往往需要数百万个小批次，这种性能开销是可以忽略不计的。\n    运行时显存细粒度调整\n 此外，AntMan在运行时提供细粒度的GPU内存调度。如图7b所示，一个训练作业可能会收缩以确保其他作业的内存资源，并在其他作业完成后再增长。它说明了一个DL作业在T0时缩减，在T1时增加，代价是在主机内存上分配了一些张量。因此，在同一共享GPU中运行的作业在T0和T1之间对剩余GPU内存的使用是有保障的。    Computation Management  动态计算单元管理，用于控制DL训练作业的GPU利用率。   类似cgroup，可以在运行时动态地隔离DL特定进程的GPU计算资源访问。\n  当多个DL作业在同一个GPU上启动时，干扰主要是由潜在的GPU内核排队延迟和PCIe总线争用引起的，这会导致所有作业的性能一致下降，如果packing job是在相同的模型和配置上运行的话。\n  **如果不同的作业被打包在一起，作业会以不同的方式变慢。**这是因为作业在获取GPU计算单元方面有不同的能力。因此，作业性能在GPU共享中几乎无法保证或预测，导致多租户集群的GPU共享部署困难。\n  AntMan中，GPU运算器的执行是由一个新引入的模块负责的，称为GpuOpManager。\n 当GPU运算器准备执行时，它被添加到GpuOpManager，而不是直接启动。 GpuOpManager的主要思想是通过延迟执行GPU运算符来控制启动频率。GpuOpManager通过这种方式来限制DL训练作业的GPU利用率。GpuOpManager不断对GPU运算器的执行时间进行分析，并在启动GPU运算器之前简单地分配空闲的时间段。 请注意，GpuOpManager只是延迟了GPU内核的执行。因此，运算符（包括GPU运算符和CPU运算符）之间的潜在依赖关系被保留下来，这意味着如果可能的话，CPU运算符可以继续。    \rimage-20220905151222951\r\n 图8说明了在同一GPU上执行的两个作业的GPU计算单元干扰的例子。图8a说明了Job-A是如何以细粒度的方式在GPU上执行的。简而言之，GPU内核将被按顺序放置，并由GPU计算单元逐一处理。请注意，在图8中，Job-A可能无法使GPU完全饱和，导致GPU周期闲置，GPU利用率低，有可能被其他作业使用。\n因此，作业-B被安排在这个GPU上（图8b）。Job-B的GPU操作者启动在GPU中执行的内核（绿色块），这可以填满它，从而延迟其他GPU内核（蓝色块）的执行，导致Job-A的性能不佳。\n这种干扰主要来自于缺乏控制GPU内核执行频率的能力。为了解决这个问题，我们在DL框架中引入了一个GPU运算器管理器（图8c）。\n现有的DL框架一旦满足了GPU运算器的控制依赖性，就会在GPU运算器中发布GPU内核。在如图8c所示，第三个CPU操作符没有被阻止，然而，第四个操作符被延迟了，因为它依赖于第二个GPU操作符，它的执行被GpuOpManager延迟了。\n     Collaborative Scheduler   AntMan的整体架构\n \rimage-20220905152947633\r AntMan采用了一个分层结构  一个全局调度器负责job 调度 每个工作服务器都包含一个本地协调器，负责通过考虑DL框架报告的统计数据，使用动态资源扩展的基元管理作业的执行   AntMan是为多租户GPU集群设计的。  在多租户集群中，每个租户通常拥有一定的资源，被注释为资源配额（即GPU的数量），这是可以分配给该租户的作业的并发性能保证资源。 每个租户的GPU资源配额之和小于等于一个GPU集群的总容量。   在AntMan中，工作被分为两种，由全局调度器应用不同的调度策略  资源保证型工作：资源保障型作业会消耗其相应租户的一定数量的GPU资源配额。AntMan确保资源保证作业的性能应该与独占执行的性能一致。 机会型工作：机会型作业则不会。      AntMan各模块的运作方式\n 调度决策可以被视为一个自上而下的控制流  在AntMan中，与传统的集群调度器类似，调度决策由全局调度器派发给本地协调器 本地协调器使用动态缩放机制对GPU资源进行内省式调度，以达到DL训练作业的目的   数据统计流信息由本地协调器的统计模块收集，并以自下而上的方式汇总到集群统计模块上  信息  硬件信息  GPU利用率 GPU内存使用率   DL框架报告的详细作业信息  小型批次持续时间 峰值内存使用率 最小内存使用率 主机内存消耗等     这些信息可以帮助全局调度器做出作业调度决策  峰值内存和最小内存使用量是用来指示可以快速提供的GPU内存大小 批处理时间显示GPU内存多久可以用于另一个DL训练作业     当作业在GPU服务器上启动，本地调度器管理其端到端执行  由于DL训练作业的负载波动，本地协调器以自省(introspective)的方式行事，对DL框架进行持续的作业控制 它从硬件和DL框架中收集所有作业的统计数据 -\u0026gt; 使用我们在第3.1节中介绍的新原语 \u0026ndash;\u0026gt; 通过资源使用调整（例如，收缩GPU内存）\u0026ndash;\u0026gt; 来控制作业性能      Scheduling Policy   目标\n  由于集群上的负载和作业的资源需求不断波动，在提供公平性（如确保DL作业的SLA，保证资源）和实现高资源利用率（如GPU利用率）之间存在着固有的矛盾。 普遍的生产型DL集群调度器经常以某些方式用公平性换取效率。  例如，空闲资源被分配给超额配置的租户。 然而，这样的GPU资源在没有抢占的情况下很难拿回来。一般来说，抢占很少被使用，因为它使正在运行的作业失败，同时浪费了昂贵的GPU周期。 此外，还报告了歧视大型作业的失序行为（即分配更多的GPU），导致倾向于小型作业的不公平。      首要目标：多租户公平性  AntMan通过在全局调度器和本地协调器中实施的政策实现了公平性 这些政策由动态扩展机制提供支持。   第二优先：提高集群效率，从而实现更高的吞吐量  AntMan中还引入了GPU机会主义工作，以窃取GPU中的空闲周期，从而最大限度地提高集群的利用率。      全局调度器：维护着工作到达的多个租户队列，决定为工作分配的GPU的位置\n  调度策略\n 对于资源保证型工作和机会型工作，AntMan应用不同的调度策略，如算法1所示。 \rimage-20220905153001724\r findNodes是一个函数，它返回满足工作请求的节点和GPU候选者，并有一个可选的参数来指定约束。 全局调度器在有足够的GPU资源的情况下公平地分配资源保证作业。 除此之外，资源保证作业还可以使用空闲的GPU资源来最大化作业性能 。 如果一个作业的资源请求只能部分满足，全局调度器就会为这个作业保留资源。 保留的资源将永远不会被其他资源保证的作业占用，但是它们可以被机会主义作业所利用。    机会主义作业\n 默认情况下，全局调度器将估计没有设置GPU配额的作业的排队时间。排队时间长的作业将被自动作为机会主义作业执行。 目的：为了最大限度地利用自由资源。 它通过考虑实际的GPU利用率在GPU上分配机会主义作业。只有在过去10秒内利用率**低于M（目前设定为80%）**的GPU可以被选为候选。 在最空闲的候选人上分配机会主义作业（即minLoadNodes，第9-10行）。 分配在同一个GPU上的作业由本地协调者管理    AntMan默认会自动选择机会主义作业，但它也允许用户在提交时手动确定作业类型\n 明确指定为资源保证作业，以确保SLA 一个作业也可以被指定为机会主义作业，永远不会占用租户的资源配额   在实践中，用户通常以机会主义模式提交作业，以避免潜在的排队延迟，目的是进行调试和超参数调整，这都是由早期反馈驱动的。\n     本地调度器：协作执行共享GPU上的作业\n  如何在共享执行中确保资源保证作业的性能\n 一个GPU只会分配给一个资源保证作业，因为它消耗GPU配额 本地协调者首先限制机会主义工作使用GPU，防止资源保证工作受到干扰 在启动DL训练作业时，需要由DL框架初始化GPU设备  如果GPU处于高负荷状态，则需要更多时间。（初始化时占用更多资源） 一旦资源保证作业稳定执行，本地协调器将把剩余的GPU内存分配给机会主义作业。   通过监测作业性能（即小批量时间），在不干扰资源保证作业的情况下，逐步增加机会主义作业的GPU计算单元使用量 同样，当一个机会主义作业到达共享GPU时，本地协调器在不影响资源保证作业的情况下，以阶梯式的方式提高其GPU资源使用率。    如何处理资源保证作业的资源需求激增\n 为了意识到动态的资源需求，本地协调器监测DL框架报告的指标 当一个资源保证作业增加了GPU内存需求时，由于有了通用内存，张量被暂时使用主机内存存储。 本地协调者缩减其他机会主义作业的GPU内存使用量，并提高资源保证作业的GPU内存限制，以恢复其性能。 这对GPU计算单元的使用协调是类似的。 🚨AntMan依靠应用层面的指标（即迷你批处理时间）来表明资源保证作业的性能。如果它观察到资源保证作业的性能不稳定，它就会采取悲观的策略来限制其他机会主义作业对GPU资源的使用。    当一个GPU只被机会主义工作所共享时，最大限度地提高聚合工作的性能。\n  如果只有一个机会主义作业，那么GPU资源就可以被这个作业充分利用，而没有任何约束。\n  有时，一个GPU有可能被多个机会主义工作占用。\n  AntMan通过最大限度地提高GPU内存效率来优化聚合作业的性能。\n  在启用动态缩放机制后，我们发现不同的工作负载在内存限制带来的性能下降方面表现出不同的敏感性\n   \rimage-20220905153042457\r\n  如图10所示，\n SR模型即使在其设备内存减少90%的情况下，也只遭受了大约25%的性能下降 Cifar10数据集上的VGG16[43]模型（VGG）即使在设备内存减少一半后，也能保持其大部分的原始性能。 ImageNet数据集（ResNet）上的ResNet50[22]对内存缩减很敏感；10%的内存缩减会带来60%以上的速度下降。       当机会主义作业的总的GPU内存需求超过了GPU的内存容量时\n 将GPU内存分配给最能提高作业性能（Normalized Performance）的作业          Job 升级\n 机会主义工作虽然以best effort执行， 但这是在没有SLA保证的情况下。 全局调度器会在有足够资源的情况下升级这些作业，以快速完成它们。 全局调度器通知本地协调器，将其标记为资源保证作业，并消耗租户的GPU配额来完成作业升级。    对于分布式同步DL训练来说，部分升级没有帮助，因为一个工作者的性能下降可能会广播到整个作业。 因此，全局调度器检查所有GPU是否都被机会主义工作填满。 一旦所有的任务实例都准备好升级，并且资源配额足够，AntMan更愿意将机会主义工作升级，而不是新启一个工作。     Implementation Deep Learning Framework  动态缩放机制在两个流行的深度学习框架中实现  Pytorch Tensorflow   DL框架的修改主要体现在三个部分：  内存分配器  为了实现动态的通用内存，(tensorflow ::BFCAllocator, PyTorch::CUDACachingAllocator）被修改以引入可调整的内存上限。内存分配器会跟踪内存分配的总字节数，并在总字节数超过上限时触发内存不足。 此外，还为内存分配器引入了一个新的接口，允许在任何时候清空缓存内存。 还增加了一个新的通用内存分配器UniversalAllocator，以包裹GPU内存分配器和主机内存分配器（即使用cudaHostMalloc进行内存分配）。  当张量的请求触发了内存分配时，UniversalAllocator试图使用GPU内存分配器来分配内存， 如果GPU内存剩余不足，则将CPU内存分配器作为备份。 🚨UniversalAllocator维护了一个集合数据结构，记录了由GPU分配的内存区域的指针，用来对内存指针进行分类，以便于回收。     执行器  为了实现动态计算单元的扩展，在DL框架中引入了一个带有运算器处理队列的GpuOpManager，它在一个独立的线程中运行。 TensorFlow的运算器被相应地修改，以插入GPU Op到GpuOpManager队列中，从而将GPU运算器的执行专门交给它。 GpuOpManager可能会根据计算能力的有限百分比来延迟GPU运算符的实际执行。   接口   内存使用模式的统计数据和执行信息被汇总到本地协调器上  DL框架和本地协调器通过文件系统进行通信 他们都有一个监控线程来检查文件，以接收工作统计数据或控制信号 为了最大限度地减少内存管理的开销，内存的动态缩放是在mini-batch的边界（session.run()的结束）触发的    Cluster Scheduler  在Kubernetes上实现了一个自定义调度器，作为评估AntMan的原型。  Kubernetes负责集群管理和执行Docker容器中的作业。 我们的全局调度器使用Python APIs来监控Kubernetes的API服务器中的事件，以便进行调度。 本地协调器作为DaemonSet部署在Kubernetes中。每个协调器监控文件系统的某些路径，以收集每个作业的报告信息。 汇总的作业和设备信息存储在ETCD中，这是Kubernetes中内置的分布式键值存储。因此，全局调度器在做调度决策时直接读取ETCD中的状态。    Evaluation Benchmark Trace Experiment Cluster Experiment Ralated Work Conclusion 参考资料\n 论文地址：AntMan: Dynamic Scaling on GPU Clusters for Deep Learning | USENIX 代码地址：https://github.com/alibaba/GPU-scheduler-for-deep-learning 参考：OSDI'20 论文赏：ANTMAN: DYNAMIC SCALING ON GPU CLUSTERS FOR DEEP LEARNING | 高策 (gaocegege.com)  ","date":"2022-08-29T15:41:42+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/20220829154241.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0antman%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】AntMan论文阅读笔记"},{"content":"《GaiaGPU：Sharing GPUs in Container Clouds》论文笔记 Abstract  对于云服务的提供商， 如何在容器间共享GPU， 是一个有吸引力的问题  容器的轻量与伸缩性 GPU强大的并行计算能力 在云环境，容器需要使用一个或多个GPU来满足资源需要的同时， 容器独占式的GPU往往使用率很低   我们提出GaiaGPU，能够在容器间共享显存和算力  将物理GPU划分为多个虚拟GPU 采用弹性资源分配和动态资源分配来提高资源利用率 实验结果显示， 有效的实现了容器间资源的分配和隔离的同时，平均只增加了1.015%的开销。    Introduction  容器化是一种虚拟化技术  涉及到量身定制一个标准操作系统，方便它在一个物理机上运行由多个用户处理的不同应用程序 与VM模拟底层硬件不同  容器模拟的是操作系统 轻量，可伸缩，易部署 微服务打包与发布应用的事实标准   云服务提供商整合容器编排框架（如k8s）到基础架构中来提供容器云   GPU 图像处理单元  有很强的并行处理能力  因为一个芯片上集成了数以千计的计算核 GPU被广泛用于计算密集型任务，以加快计算 随着技术的发展趋势，现代GPU内将集成入越来越多的计算资源   CUDA是多功能GPU最流行的平台，提供了API方便GPU的使用 卓越的性能吸引了很多云提供商将GPU引入云环境  在云环境中，部署在容器中的一个应用程序可能需要一个或多个GPU才能执行， 而另一方面，应用程序的专用GPU资源导致资源不足。 因此，如何在不同的容器中共享GPU对大多数云提供商都非常感兴趣     GPU虚拟化技术是在隔离的虚拟环境（例如VM， 容器）之间共享GPU的技术  多数的GPU虚拟化技术应用于VM， 容器间的虚拟化技术还在起始阶段 现阶段的基于容器的GPU虚拟化技术有以下局限性  需要特定的硬件设备（NVIDIA GRID） 将一整个GPU分配给单个容器， 不能共享 （NVIDIA Docker） 容器间只能共享GPU显存 （ConvGPU） 只支持单个GPU （ConvGPU）     我们提出GaiaGPU，能够在容器间透明地共享显存和算力  用户不用修改容器镜像来共享底层GPU  我们使用k8s的device plugin 框架将物理GPU划分为多个虚拟GPU   每个镜像可以按需分配一个或者多个vGPU 提供了两者方式在运行时更改镜像资源  弹性资源分配：暂时改变资源 动态资源分配：永久改变资源     vGPU包括GPU显存和计算资源  共享显存  容器包含GPU显存的一小部分 vGPU分配的是GPU的物理内存   共享计算资源  共享计算资源意味着每个容器都拥有GPU线程的一部分以并行执行计算。 VGPU的计算资源由GPU的利用率衡量（采样时段内， 容器使用GPU的时间比例）     总结：本文做了如下贡献  提出了GaiaGPU：一种在容器间透明共享显存与算力的方法 采用弹性分配和动态分配的方式提高了资源的利用率 进行了四个实验来验证GaiaGPU的性能。结果：实现了容器间资源的分配和隔离的同时，平均只增加了1.015%的开销。    Related Work GPU虚拟化   被应用于在多个虚拟环境之间分享GPU， 极大地提高了应用性能\n  现存的多数GPU虚拟化技术基于VM，主要有三种虚拟化GPU\n API remoting  vCUDA 和 rCUDA 创建GPU的封装库-\u0026gt;劫持GPU调用-\u0026gt;将重定向到host   para and full virtualization  GPUvm GPUvm将GPU显存和MMIO（存储器映射输入输出）分成几片，将片分给VM   硬件支持的虚拟化  NVIDIA GRID 硬件虚拟化，创建虚拟GPU，挂在容器      与VM相比， 容器使用主机的操作系统-\u0026gt;容器可以直接使用宿主机的GPU驱动-\u0026gt;性能接近原生环境\n NVIDIA GRID  需要特定的硬件和与虚拟GPU要有相同的资源配置 一个容器只能分配一个虚拟GPU   NVIDIA Docker  使得Docker镜像可以使用GPU 允许GPU驱动对CUDA镜像不感知 提供了一个命令行的封装-\u0026gt;当容器启动时可以挂载driver的用户态组件和GPU设备文件 不能共享：只能把一整个GPU分配给一个容器   ConvGPU  容器间共享GPU显存 它拦截了CUDA库来管理每个镜像显存的分配和回收 仅支持分享显存 而且只能虚拟化单个物理GPU   GaiaGPU  软件层虚拟化，没有硬件限制 每个虚拟GPU的资源可以是不一样的 一个容器可以分配多个虚拟GPU 可以同时共享显存和计算资源 可以虚拟化多个物理GPU      Device Plugin  致力于征聘各种计算资源（GPUs， NICs， FPGAs）供集群使用 无需改变k8s的核心代码 工作流  资源发现  实现Device Plugin 通过gRPC将device注册到Kubelet 成功注册后，device plugin发送设备列表 Kubelet负责将这个扩展资源推广给Master   资源分配  用户在容器的specification中请求设备 master的scheduler选择一个k8s节点的device plugin发送device请求 device plugin分配对应的设备给容器   \rimg\r   Vaucher通过利用设备插件框架来实现SGX（Software Guard Extensions）设备虚拟化，  该插件框架修改了Kubelet和SGX代码以限制虚拟SGX设备的设备内存的使用 仅处理内存资源 对内存施加严重限制   GaiaGPU也采用了device plugin框架以在容器之间共享资源  对资源采用弹性限制而不是硬限制-\u0026gt;来改善利用率    Design And Implementation 设计与实现  目标：在容器间共享显存与计算资源，使用最小的成本获得最大的提升 挑战：  透明性：  GaiaGPU不应该修改k8s的代码或者容器镜像 使用共享GPU就如同使用物理GPU一样   低开销  使用共享GPU的性能尽可能接近使用物理GPU   隔离  GaiaGPU应该管理GPU资源在每个容器的分配与回收 共享GPU时容器之间完全隔离     结构：  \rimage-20220828143547571\r 两层资源管理  host层：  GPU Manager负责创建vGPUs GPU Scheduler 负责分配物理GPU资源到vGPUs   容器层  vGPU Library负责管理具体容器的GPU资源     组件  GPU Manager  device plugin：运行在host上负责创建vGPUs，使用gRPC与Kubelet通信 \rimage-20220828171002587\r Register：GPU Manager将自身注册到Kubelet以通知其存在 ListAndWatch：成功注册后，Kubelet调用ListAndWatch获取设备信息List  显存：256M 作为一个单元， 每个单元都被称作一个vmemory设备 计算资源：一个GPU被分作100份vprocessor设备， 每个vprocessor都有百分之一的利用率   Allocate：  映射过程  Kubelet发送随机选择的设备IDs到GPU Manager GPU Manager根据得到的设备IDs计算所需的物理GPU资源 GPU Manage 发送一个请求到 GPU Scheduler GPU Scheduler 返回要分配给容器的物理GPU   映射完成后，GPU manager返回一个allocateResponse， 包含获取分配到的设备的Configurations  容器的环境变量 挂载到容器的目录和文件 分配的设备   Kubelet将这些配置发送到容器运行时     GPU Scheduler  负责处理GPU Manager发来的调度请求 基于拓扑树分配GPU，树的根节点是host， 树的叶节点是物理GPU 当所需要的资源少于一个物理GPU，分配策略会最大程度减少资源碎片 当所需的资源等于一个物理GPU时，采用了最小化单叶节点（即没有兄弟姐妹节点的叶子节点）的分配策略。 当所需的资源不止一个物理GPU时，分配策略的目标是最大程度地降低GPU的通信成本。通信成本取决于两个GPU的连接模式。   vGPU Manager  运行在host，传递容器配置并且监管分配了vGPUs的容器 当容器申请GPU资源时，GPU Manager将配置发送给vGPU Manager，例如  需要的GPU资源 该容器的name   接收到配置后，在主机上为容器命创建一个唯一的路径  路径名为容器名 这个路径包含在allocateResponse里 容器的配置也包含在这个路径里，所以可以通过Kubelet传递到容器   vGPU Manager 和 vGPU Library是 服务器-客户端模式  vGPU Manager 维护一个活着的且分配了GPU资源的容器的list 会定期检查这些容器是否存活 如果容器死了，从list中移除这个容器的信息，并且删除目录     vGPU Library  运行在容器中，管理容器的GPU资源 在容器第一次执行GPU程序时被加载 启动后vGPU Libraray 向 vGPU Manager注册自身 利用LD_LIBRARY_PATH机制拦截CUDA库中内存与计算相关的API  LD_LIBRARY_PATH是一个linux系统的环境变量 可以影响程序的runtime link 允许某些路径先于standard set of directories（？标准库目录？）加载 \rimage-20220828222012346\r   当容器需要的GPU资源超出它申请的资源时，为了避免耗尽整个GPU，有两种资源限制策略  硬限制（hard limiit）：如果容器资源消耗量超过配额，就不再给该容器分配资源 弹性限制（elastic limit）：如果容器资源消耗量超过配额，但是系统中还有空闲资源，那么该容器仍然能够得到更多的资源   内存资源的限制采用硬限制策略， 因为  内存资源大小能决定一个程序能否运行，但对运行的效率影响较小 GPU是计算设备，它们采用一次性资源分配策略。仅在获取所有内存资源后才可以执行应用程序，并且在完成内存之前不会释放。如果使用弹性内存分配，则具有较大内存需求的应用程序将饿死 撤回过度分配的内存资源的唯一方法是通过抛出out-of-memory exception来杀死该过程   计算资源采用弹性限制策略，因为  计算资源对程序运行的影响很大 计算资源（GPU 线程）在执行之后会立刻释放掉         总结：  Step 1：GPU Manager向Kubelet注册自身，并报告vGPU的信息（ListAndWatch） Step 2：Kubelet接收到来自Master的创建一个GPU容器的请求 Step 3：Kubelet发送一个allocateRequest到GPU Manager Step 4：GPU Manager发送一个vGPU调度请求到GPU Scheduler，GPU Scheduler根据调度策略选择实际提供资源的物理GPU。如果调度成功返回一个包含该物理GPU的信息的reponse Step 5：GPU Manager将容器配置信息发送到vGPU Manager Step 6：GPU Manager将容器环境变量，挂载信息和设备信息通过allocateResponse返回给Kubelet Step 7：Kubelet根据allocateResponse创建并且初始化一个容器 Step 8：vGPU Library向vGPU Manager注册自身并且管理其所在容器的GPU资源 Step 9：vGPU Manager持续监控GPU容器状态 \rimage-20220828143547571\r    优化   容器的资源不仅会影响应用程序的性能，而且还会确定应用程序的状态 用户在创建容器时无法准确估算所需的资源 因此，我们提供两种方法来更改运行时容器的资源。弹性资源分配会暂时修改容器的计算资源限制，而动态资源分配永久改变了容器的资源。     弹性分配策略\n 目的是使用闲置的计算资源以提高资源利用率 \rimage-20220828224059056\r nanosleep()是Linux内核函数，会挂起当前线程等待一段时间或者直到接收到调用当前线程的信号（Line 2） 为了防止过载， $GU_{max}$ 默认的最大值是90% （Line 3） 如果物理GPU计算卡仍然有空闲资源，也就是说GU free \u0026gt; 0，即使容器的资源请求已经超出其配额，vGPU Library也会继续给容器分配计算资源（Line 4-5）。如果系统没有剩余的空闲资源（*GU free \u0026lt;= 0*）并且容器的消耗的资源大于其配额，vGPU Library会逐渐收回超额（over-allocated）资源（Line 6-7） 超额资源的回收采用非抢占式策略（non-preemptive strategy），就是说会等到容器中占用线程的核函数执行完后再回收线程资源。CU cores*可以被理解为一种token，容器执行核函数时需要消耗该token，当核函数执行完成释放线程资源时容器又回重新拥有该token。CU cores的初始值等于容器的计算资源配额，CU cores为零时（系统没有空闲的资源，并且该容器的计算资源配额都正在被用于执行核函数），vGPU Library不会再给容器分配任何计算资源，直到容器的CU cores大于零（其他容器释放了空闲资源，或者该容器有核函数完成释放了资源，容器重新获得CU cores*） 弹性分配的样例  \rimage-20220828233026263\r 首先，容器A申请了0.3个GPU，并且被GPU Scheduler调度到了一个完全空闲的GPU上 由于GPU完全空闲，所以容器A会逐渐占用所有的空闲资源，默认最大可占用90% 此时容器B申请了0.7个GPU，也被调度到了此GPU上，但是由于容器A占用了所有的空闲资源，所以需要从容器A回收超额线程资源并分配给容器B 重复经过几次资源的重新分配，容器A和容器B所占用的资源与其资源配额相同      动态分配策略\n 动态资源分配会修改容器资源，包括内存和计算资源，而无需停止容器。 动态资源分配旨在解决两个问题  在硬限制下更新容器的内存和计算资源 在弹性限制下将内存资源添加到容器中   vGPU Library 通过将容器的资源配置与容器的实际使用进行比较来限制容器资源 要永久更改容器的资源：修改容器的资源配置—\u0026gt;通知GPU Scheduler-\u0026gt;更新相应的物理GPU分配    Experiments \u0026hellip;\nConclusion \u0026hellip;\n参考资料   论文地址 https://ieeexplore.ieee.org/abstract/document/8672318\n  Github开源代码 https://github.com/tkestack/gpu-manager\n  ","date":"2022-08-26T17:20:32+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/20220826142857.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0gaiagpu%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】GaiaGPU论文阅读笔记"},{"content":"在ubuntu上k8s集群部署实践  centos 上安装建议看 后端 - CentOS 搭建 K8S，一次性成功，收藏了！_个人文章 - SegmentFault 思否\n 一、机器配置 配置主机名 sudo hostnamectl set-hostname \u0026#34;k8s-master\u0026#34; // Run this command on masternode cat /etc/hostname sudo hostnamectl set-hostname \u0026#34;k8s-node1\u0026#34; // Run this command on node-0 sudo hostnamectl set-hostname \u0026#34;k8s-node2\u0026#34; // Run this command on node-1 配置/etc/hosts sudo vi /etc/hosts 10.1.13.106 k8s-master #10.1.13.107 k8s-node1 #10.1.13.108 k8s-node1 配置免密登录 想让机器 A 访问机器 B，就把机器 A 的公钥放到机器 B 的~/.ssh/authorized_keys 文件里就行了。\n首先我们在worker上生成一个密钥，输入下述命令后一路回车即可：\nssh-keygen 然后登录master，并依次输入下述两条命令将其复制并写入到master的authorized_keys中，注意我下面的scp命令中使用了worker别名，要提前进行配置：\n# 复制到 master 主机 scp root@worker:~/.ssh/id_rsa.pub /home # 写入到 authorized_keys 中 cat /home/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 然后再次使用ssh worker登录就可以发现直接连接上而不需要密码了。\n关闭防火墙 sudo systemctl stop firewalld sudo systemctl disable firewalld 禁用swap 这个swap其实可以类比成 windows 上的虚拟内存，它可以让服务器在内存吃满的情况下可以保持低效运行，而不是直接卡死。但是 k8s 的较新版本都要求关闭swap。所以咱们直接动手，修改/etc/fstab文件：\nsudo vi /etc/fstab 你应该可以看到如下内容，把第二条用#注释掉就好了，注意第一条别注释了，不然重启之后系统有可能会报file system read-only错误。\nUUID=e2048966-750b-4795-a9a2-7b477d6681bf / ext4 errors=remount-ro 0 1 # /dev/fd0 /media/floppy0 auto rw,user,noauto,exec,utf8 0 0 然后输入reboot重启即可，重启后使用top命令查看任务管理器，如果看到如下KiB Swap后均为 0 就说明关闭成功了。\n关闭swap之后使用top命令看到 KiB Swap 全部为0\n\rimage-20220613000133107\r\n上面说的是永久关闭swap内存，其实也可以暂时关闭，使用swapoff -a命令即可，效果会在重启后消失。\n禁用Selinux sudo apt install selinux-utils setenforce 0 确保时区和时间正确 sudo timedatectl set-timezone Asia/Shanghai sudo systemctl restart rsyslog sudo apt-get install ntpdate –y sudo ntpdate time.windows.com 配置net.bridge.bridge-nf-call-iptables cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 设置rp_filter的值 sudo vi /etc/sysctl.d/10-network-security.conf # 将下面两个参数的值从2修改为1 # net.ipv4.conf.default.rp_filter=1 # net.ipv4.conf.all.rp_filter=1 sudo sysctl --system 二、安装docker docker 是 k8s 的基础，在安装完成之后也需要修改一些配置来适配 k8s ，所以本章分为 docker 的安装 与 docker 的配置 两部分。如果你已经安装并使用了一段时间的 docker 了话，建议使用docker -v查看已安装的 docker 版本，并在 k8s 官网上查询适合该版本的 k8s 进行安装。这一步两台主机都需要进行安装。\n安装docker sudo apt install docker.io sudo apt install docker.io=18.09.1-0ubuntu1 启动docker sudo systemctl start docker 配置docker sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://n73pm3wf.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [ \u0026#34;native.cgroupdriver=systemd\u0026#34; ] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 设置开机自启动 $ sudo systemctl enable docker 或 $ sudo systemctl enable docker.service --now 验证docker systemctl status docker docker --version docker info | grep Cgroup 修改后的 docker cgroup 状态，发现变为systemd即为修改成功。\n重启docker（当有配置改动后执行） sudo pkill -SIGHUP dockerd sudo systemctl restart docker 卸载docker sudo apt-get remove docker.io 三、安装k8s 安装k8s # 使得 apt 支持 ssl 传输 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https # 下载 gpg 密钥 这个需要root用户否则会报错 curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - # 添加 k8s 镜像源 这个需要root用户否则会报错 cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF # 更新源列表 apt-get update # 下载 kubectl，kubeadm以及 kubelet # 安装最新版本 apt-get install -y kubelet kubeadm kubectl # 安装指定版本 apt-get install -y kubelet=1.15.1-00 kubeadm=1.15.1-00 kubectl=1.15.1-00 删除已安装的k8s sudo apt-get remove -y --allow-change-held-packages kubeadm kubectl kubelet 查询可安装版本 apt-cache madison kubeadm apt-cache madison kubelet apt-cache madison kubectl apt-cache madison kubernetes-cni 设置不随系统更新而更新 sudo apt-mark hold kubelet kubeadm kubectl 四、初始化master kubeadm init kubeadm init \\ --apiserver-advertise-address=10.10.0.2 \\ --kubernetes-version=1.15.1 \\ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 参数解释：\n \u0026ndash;kubernetes-version：指定kubernetes的版本，与上面kubelet，kubeadm，kubectl工具版本保持一致。 \u0026ndash;apiserver-advertise-address：apiserver所在的节点(master)的ip。 \u0026ndash;image-repository=registry.aliyuncs.com/google_containers：由于国外地址无法访问，所以使用阿里云仓库地址 \u0026ndash;server-cidr：service之间访问使用的网络ip段 \u0026ndash;pod-network-cidr：pod之间网络访问使用的网络ip,与下文部署的CNI网络组件yml中保持一致  初始化成功之后你将看到\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.10.0.2:6443 --token juojiv.q4gwfmkj4cwgscnt \\  --discovery-token-ca-cert-hash sha256:381b61262d3b174cfce0e5cfbd0b4b171e270c506d82c4d82334d0a4e2c2ac47 初始化失败 重置之后重新初始化\nkubeadm reset 记得保存kubeadm join 命令 kubeadm join 10.10.0.2:6443 --token qc7drh.47mloh5ierij84xi \\  --discovery-token-ca-cert-hash sha256:90f147ef85ae06d9a46bed91d64109abbd697b6878ab9ca16a376e11816f9a0d 如果忘记可以使用下面的命令重新生成\nkubeadm token create --print-join-command 配置kubectl 按照上图中的命令配置，可以直接从命令行中复制\n 非root用户  mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 查看已加入的节点 kubectl get nodes # 查看集群状态 kubectl get cs  root用户  export KUBECONFIG=/etc/kubernetes/admin.conf 部署网络 网络插件可以选择calico或flannel。这里使用Flannel作为Kubernetes容器网络方案，解决容器跨主机网络通信。\n 部署flannel网络  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml 也可以把这个文件下载到本地，注意修改下图中的Network，与kubeadm inti中的\u0026ndash;pod-network-cidr 10.244.0.0/16参数保持一致，然后\nkubectl apply -f kube-flannel.yml \rimage-20220613013117789\r\n将会看到\npodsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created 查看pod状态，应该全部为running。可能需要等待一段时间\nkubectl get pod --all-namespaces -o wide \rimage-20220613012658569\r\n 部署calico网络  下载yml文件\nwget https://docs.projectcalico.org/v3.11/manifests/calico.yaml vi calico.yaml #修改CALICO_IPV4POOL_CIDR，为10.244.0.0/16（要与kubeadm inti中的\u0026ndash;pod-network-cidr 10.244.0.0/16参数保持一致，默认为192.168.0.0/16\nkubectl apply -f calico.yaml 五、添加worker节点 使用之前保存的kubeadm join命令\nkubeadm join 10.10.0.2:6443 --token ryyy8k.pr8u7cjm4btqdx4v --discovery-token-ca-cert-hash sha256:6b596c66745b162190475b63686ca46c60cc4f39473 然后在master节点上，查看\nkubectl get nodes 可以看到\n\rimage-20220613010009663\r\n六、验证 创建nginx kubectl create deployment nginx-web --image=nginx 获取pod信息 kubectl get pod -o wide \rimage-20220613013920336\r\n可以看到该节点的ip为10.244.1.2\n测试nginx root@k8s-master:/home# curl http://10.244.1.2 可以看到\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 删除pod kubectl delete pod nginx-web-xxxxxx -n nginx-web 七、删除节点 首先，确定想要清空的节点的名称。可以用以下命令列出集群中的所有节点:\nkubectl get nodes 接下来，告诉 Kubernetes 清空节点：\nkubectl drain \u0026lt;node name\u0026gt; 一旦它返回（没有报错）， 你就可以下线此节点（或者等价地，如果在云平台上，删除支持该节点的虚拟机）。 如果要在维护操作期间将节点留在集群中，则需要运行：\nkubectl uncordon \u0026lt;node name\u0026gt; 然后告诉 Kubernetes，它可以继续在此节点上调度新的 Pods。\n","date":"2022-06-13T14:57:42+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/20220823230439.png","permalink":"https://tweakzx.github.io/p/kubernetesubuntu%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","title":"【Kubernetes】ubuntu安装k8s集群"},{"content":"题目描述 请你来实现一个 myAtoi(string s) 函数，使其能将字符串转换成一个 32 位有符号整数（类似 C/C++ 中的 atoi 函数）。\n函数 myAtoi(string s) 的算法如下：\n 读入字符串并丢弃无用的前导空格 检查下一个字符（假设还未到字符末尾）为正还是负号，读取该字符（如果有）。 确定最终结果是负数还是正数。 如果两者都不存在，则假定结果为正。 读入下一个字符，直到到达下一个非数字字符或到达输入的结尾。字符串的其余部分将被忽略。 将前面步骤读入的这些数字转换为整数（即，\u0026ldquo;123\u0026rdquo; -\u0026gt; 123， \u0026ldquo;0032\u0026rdquo; -\u0026gt; 32）。如果没有读入数字，则整数为 0 。必要时更改符号（从步骤 2 开始）。 如果整数数超过 32 位有符号整数范围 [−231, 231 − 1] ，需要截断这个整数，使其保持在这个范围内。具体来说，小于 −231 的整数应该被固定为 −231 ，大于 231 − 1 的整数应该被固定为 231 − 1 。 返回整数作为最终结果。  注意：\n  本题中的空白字符只包括空格字符 ' ' 。\n  除前导空格或数字后的其余字符串外，请勿忽略 任何其他字符。\n  示例 1：\n 输入：s = \u0026ldquo;42\u0026rdquo; 输出：42 解释：加粗的字符串为已经读入的字符，插入符号是当前读取的字符。 第 1 步：\u0026ldquo;42\u0026rdquo;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026ldquo;42\u0026rdquo;（当前没有读入字符，因为这里不存在 \u0026lsquo;-\u0026rsquo; 或者 \u0026lsquo;+'） ^ 第 3 步：\u0026ldquo;42\u0026rdquo;（读入 \u0026ldquo;42\u0026rdquo;） ^ 解析得到整数 42 。 由于 \u0026ldquo;42\u0026rdquo; 在范围 [-231, 231 - 1] 内，最终结果为 42 。\n 示例 2：\n 输入：s = \u0026quot; -42\u0026quot; 输出：-42 解释： 第 1 步：\u0026quot; -42\u0026quot;（读入前导空格，但忽视掉） ^ 第 2 步：\u0026quot; -42\u0026quot;（读入 \u0026lsquo;-\u0026rsquo; 字符，所以结果应该是负数） ^ 第 3 步：\u0026quot; -42\u0026quot;（读入 \u0026ldquo;42\u0026rdquo;） ^ 解析得到整数 -42 。 由于 \u0026ldquo;-42\u0026rdquo; 在范围 [-231, 231 - 1] 内，最终结果为 -42 。\n 示例 3：\n 输入：s = \u0026ldquo;4193 with words\u0026rdquo; 输出：4193 解释： 第 1 步：\u0026ldquo;4193 with words\u0026rdquo;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026ldquo;4193 with words\u0026rdquo;（当前没有读入字符，因为这里不存在 \u0026lsquo;-\u0026rsquo; 或者 \u0026lsquo;+'） ^ 第 3 步：\u0026ldquo;4193 with words\u0026rdquo;（读入 \u0026ldquo;4193\u0026rdquo;；由于下一个字符不是一个数字，所以读入停止） ^ 解析得到整数 4193 。 由于 \u0026ldquo;4193\u0026rdquo; 在范围 [-231, 231 - 1] 内，最终结果为 4193 。\n 提示：\n 0 \u0026lt;= s.length \u0026lt;= 200 s 由英文字母（大写和小写）、数字（0-9）、\u0026rsquo; \u0026lsquo;、'+'、'-\u0026rsquo; 和 \u0026lsquo;.\u0026rsquo; 组成  来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/string-to-integer-atoi\n题解 大家可以自行去看leetcode的官方题解，就是用一个自动机来解决这个问题。我第一次用Go来解决这个问题，所以决定记录下来\ntype Automaton struct{ state string sign int ans int table map[string][]string } func(a *Automaton) init(){ //因为Go不能设置类型变量的初始值（我没找到相关代码），所以我用了初始化函数来代替  a.state = \u0026#34;start\u0026#34; a.sign = 1 a.ans = 0 a.table = map[string][]string{ \u0026#34;start\u0026#34; : []string{\u0026#34;start\u0026#34;,\u0026#34;signed\u0026#34;,\u0026#34;in_number\u0026#34;,\u0026#34;end\u0026#34;}, \u0026#34;signed\u0026#34; : []string{\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;in_number\u0026#34;,\u0026#34;end\u0026#34;}, \u0026#34;in_number\u0026#34; : []string{\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;in_number\u0026#34;,\u0026#34;end\u0026#34;}, \u0026#34;end\u0026#34; : []string{\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;}, } } func(a *Automaton) get(c byte){ //实现状态转移的方法  a.state = a.table[a.state][a.get_col(c)] switch a.state{ case \u0026#34;in_number\u0026#34;: a.ans = a.ans*10 + int(c-\u0026#39;0\u0026#39;) if a.sign\u0026gt;0{ a.ans = min(a.ans, math.MaxInt32) }else{ a.ans = min(a.ans, -math.MinInt32) } case \u0026#34;signed\u0026#34;: if c == \u0026#39;-\u0026#39;{ a.sign = -1 } } } func(a *Automaton) get_col(c byte ) int{//获得输入字符的类型  switch { case c==\u0026#39; \u0026#39;: return 0 case c==\u0026#39;+\u0026#39;||c==\u0026#39;-\u0026#39;: return 1 case \u0026#39;0\u0026#39;\u0026lt;=c\u0026amp;\u0026amp;c\u0026lt;=\u0026#39;9\u0026#39;: return 2 default: return 3 } } func(a *Automaton) get_ans() int{ //获得结果  return a.sign * a.ans } func myAtoi(s string) int { a := \u0026amp;Automaton{} a.init() for i:=0; i\u0026lt;len(s); i++{ a.get(s[i]) } return a.get_ans() } func min(a,b int) int{ if a\u0026lt;b{ return a } return b } ","date":"2022-03-12T16:59:39+08:00","permalink":"https://tweakzx.github.io/p/golang%E8%87%AA%E5%8A%A8%E6%9C%BA%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"【Golang】自动机的实现"},{"content":"旅行起点 Go 语言之旅 (go-zh.org)\n上方链接是一个Go语言学习的Playground，快点击它，开启一场Go语言之旅吧\n旅行开始 练习：循环与函数 为了练习函数与循环，我们来实现一个平方根函数：用牛顿法实现平方根函数。\n计算机通常使用循环来计算 x 的平方根。从某个猜测的值 z 开始，我们可以根据 z² 与 x 的近似度来调整 z，产生一个更好的猜测：\nz -= (z*z - x) / (2*z) 重复调整的过程，猜测的结果会越来越精确，得到的答案也会尽可能接近实际的平方根。\n在提供的 func Sqrt 中实现它。无论输入是什么，对 z 的一个恰当的猜测为 1。 要开始，请重复计算 10 次并随之打印每次的 z 值。观察对于不同的值 x（1、2、3 \u0026hellip;）， 你得到的答案是如何逼近结果的，猜测提升的速度有多快。\n提示：用类型转换或浮点数语法来声明并初始化一个浮点数值：\nz := 1.0 z := float64(1) 然后，修改循环条件，使得当值停止改变（或改变非常小）的时候退出循环。观察迭代次数大于还是小于 10。 尝试改变 z 的初始猜测，如 x 或 x/2。你的函数结果与标准库中的 math.Sqrt 接近吗？\n（注： 如果你对该算法的细节感兴趣，上面的 z² − x 是 z² 到它所要到达的值（即 x）的距离， 除以的 2z 为 z² 的导数，我们通过 z² 的变化速度来改变 z 的调整量。 这种通用方法叫做牛顿法。 它对很多函数，特别是平方根而言非常有效。）\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func Sqrt(x float64) (z float64) { z = float64(1) for math.Abs(z*z-x)\u0026gt;0.000001 { z -= (z*z-x)/(z*2) } return } func main() { fmt.Println(Sqrt(2)) } 练习：切片 实现 Pic。它应当返回一个长度为 dy 的切片，其中每个元素是一个长度为 dx，元素类型为 uint8 的切片。当你运行此程序时，它会将每个整数解释为灰度值（好吧，其实是蓝度值）并显示它所对应的图像。\n图像的选择由你来定。几个有趣的函数包括 (x+y)/2, x*y, x^y, x*log(y) 和 x%(y+1)。\n（提示：需要使用循环来分配 [][]uint8 中的每个 []uint8；请使用 uint8(intValue) 在类型之间转换；你可能会用到 math 包中的函数。）\npackage main import \u0026#34;golang.org/x/tour/pic\u0026#34; func Pic(dx, dy int) [][]uint8 { picture := make([][]uint8,dy) for x:=range picture{ line := make([]uint8,dx) for y:=range line{ line[y] = uint8((x+y)/2) } picture[x] = line } return picture } func main() { pic.Show(Pic) } 练习：映射 实现 WordCount。它应当返回一个映射，其中包含字符串 s 中每个“单词”的个数。函数 wc.Test 会对此函数执行一系列测试用例，并输出成功还是失败。\n你会发现 strings.Fields 很有帮助。\npackage main import ( \u0026#34;golang.org/x/tour/wc\u0026#34; \u0026#34;strings\u0026#34; ) func WordCount(s string) map[string]int { words := strings.Fields(s) ans := make(map[string]int) for _,w :=range words{ //v,ok := ans[w] \tans[w] = ans[w] + 1 } return ans } func main() { wc.Test(WordCount) } 练习：斐波纳契闭包 让我们用函数做些好玩的事情。\n实现一个 fibonacci 函数，它返回一个函数（闭包），该闭包返回一个斐波纳契数列 (0, 1, 1, 2, 3, 5, ...)。\npackage main import \u0026#34;fmt\u0026#34; // 返回一个“返回int的函数” func fibonacci() func() int { first := 2 second := 1 //根据公式倒推出的first和second \treturn func() int{ first = second - first second = second + first //斐波那契公式 \treturn second } } func main() { f := fibonacci() for i := 0; i \u0026lt; 10; i++ { fmt.Println(f()) } } 练习：Stringer 通过让 IPAddr 类型实现 fmt.Stringer 来打印点号分隔的地址。\n例如，IPAddr{1, 2, 3, 4} 应当打印为 \u0026quot;1.2.3.4\u0026quot;。\npackage main import \u0026#34;fmt\u0026#34; type IPAddr [4]byte // TODO: 给 IPAddr 添加一个 \u0026#34;String() string\u0026#34; 方法 func (ip IPAddr) String() string{ return fmt.Sprintf(\u0026#34;%v.%v.%v.%v\\n\u0026#34;,ip[0],ip[1],ip[2],ip[3]) } func main() { hosts := map[string]IPAddr{ \u0026#34;loopback\u0026#34;: {127, 0, 0, 1}, \u0026#34;googleDNS\u0026#34;: {8, 8, 8, 8}, } for name, ip := range hosts { fmt.Printf(\u0026#34;%v: %v\\n\u0026#34;, name, ip) } } 练习：错误 从之前的练习中复制 Sqrt 函数，修改它使其返回 error 值。\nSqrt 接受到一个负数时，应当返回一个非 nil 的错误值。复数同样也不被支持。\n创建一个新的类型\ntype ErrNegativeSqrt float64 并为其实现\nfunc (e ErrNegativeSqrt) Error() string 方法使其拥有 error 值，通过 ErrNegativeSqrt(-2).Error() 调用该方法应返回 \u0026quot;cannot Sqrt negative number: -2\u0026quot;。\n注意: 在 Error 方法内调用 fmt.Sprint(e) 会让程序陷入死循环。可以通过先转换 e 来避免这个问题：fmt.Sprint(float64(e))。这是为什么呢？\n修改 Sqrt 函数，使其接受一个负数时，返回 ErrNegativeSqrt 值。\npackage main import ( \u0026#34;fmt\u0026#34; ) type ErrNegativeSqrt float64 func (e ErrNegativeSqrt) Error() string{ return fmt.Sprintf(\u0026#34;cannot Sqrt negative number: %v\u0026#34;,float64(e)) } func Sqrt(x float64) (float64, error) { if x\u0026gt;0{ return 0, nil }else{ var e ErrNegativeSqrt e = ErrNegativeSqrt(x) return x,e } } func main() { fmt.Println(Sqrt(2)) fmt.Println(Sqrt(-2)) } 练习：Reader 实现一个 Reader 类型，它产生一个 ASCII 字符 'A' 的无限流。\npackage main import \u0026#34;golang.org/x/tour/reader\u0026#34; type MyReader struct{} // TODO: 给 MyReader 添加一个 Read([]byte) (int, error) 方法 func (mr MyReader) Read(buf []byte) (int, error) { for i :=range buf{ buf[i] = \u0026#39;A\u0026#39; } return 1, nil } func main() { reader.Validate(MyReader{}) } 练习：rot13Reader 有种常见的模式是一个 io.Reader 包装另一个 io.Reader，然后通过某种方式修改其数据流。\n例如，gzip.NewReader 函数接受一个 io.Reader（已压缩的数据流）并返回一个同样实现了 io.Reader 的 *gzip.Reader（解压后的数据流）。\n编写一个实现了 io.Reader 并从另一个 io.Reader 中读取数据的 rot13Reader，通过应用 rot13 代换密码对数据流进行修改。\nrot13Reader 类型已经提供。实现 Read 方法以满足 io.Reader。\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) type rot13Reader struct { r io.Reader } func ( rot rot13Reader) Read(buf []byte) (int, error){ len,ok := rot.r.Read(buf) for i,v := range buf{ switch{ case (\u0026#39;a\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;m\u0026#39;)||(\u0026#39;A\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;M\u0026#39;): buf[i] = v+13 case (\u0026#39;n\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;z\u0026#39;)||(\u0026#39;N\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;Z\u0026#39;): buf[i] = v-13 default: } } return len,ok } func main() { s := strings.NewReader(\u0026#34;Lbh penpxrq gur pbqr!\u0026#34;) r := rot13Reader{s} io.Copy(os.Stdout, \u0026amp;r) } 练习：图像 还记得之前编写的图片生成器 吗？我们再来编写另外一个，不过这次它将会返回一个 image.Image 的实现而非一个数据切片。\n定义你自己的 Image 类型，实现必要的方法并调用 pic.ShowImage。\nBounds 应当返回一个 image.Rectangle ，例如 image.Rect(0, 0, w, h)。\nColorModel 应当返回 color.RGBAModel。\nAt 应当返回一个颜色。上一个图片生成器的值 v 对应于此次的 color.RGBA{v, v, 255, 255}。\npackage main import ( \u0026#34;golang.org/x/tour/pic\u0026#34; \u0026#34;image\u0026#34; \u0026#34;image/color\u0026#34; ) type Image struct{ w,h int pixels [][]uint8 } func (self Image) Bounds()(image.Rectangle){ return image.Rect(0, 0, self.w, self.h) } func (self Image) ColorModel()(color.Model){ return color.RGBAModel } func (self Image) At(x int ,y int)(color.Color){ v := self.pixels[y][x] return color.RGBA{v,v, 255, 255} } func Pic(dx, dy int) [][]uint8 { img := make([][]uint8, dy) for y := 0; y \u0026lt; dy; y++ { img[y] = make([]uint8, dx) for x := 0; x \u0026lt; dx; x++ { img[y][x] = (uint8)(x^y) } } return img } func main() { m := Image{256,256,Pic(256,256)} pic.ShowImage(m) } 练习：等价二叉查找树 1. 实现 Walk 函数。\n2. 测试 Walk 函数。\n函数 tree.New(k) 用于构造一个随机结构的已排序二叉查找树，它保存了值 k, 2k, 3k, \u0026hellip;, 10k。\n创建一个新的信道 ch 并且对其进行步进：\ngo Walk(tree.New(1), ch) 然后从信道中读取并打印 10 个值。应当是数字 1, 2, 3, ..., 10。\n3. 用 Walk 实现 Same 函数来检测 t1 和 t2 是否存储了相同的值。\n4. 测试 Same 函数。\nSame(tree.New(1), tree.New(1)) 应当返回 true，而 Same(tree.New(1), tree.New(2)) 应当返回 false。\nTree 的文档可在这里找到。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/tour/tree\u0026#34; ) // Walk 步进 tree t 将所有的值从 tree 发送到 channel ch。 func Walk(t *tree.Tree, ch chan int) { dfs(t, ch) close(ch) } func dfs(t *tree.Tree, ch chan int) { if t == nil { return } dfs(t.Left, ch) ch \u0026lt;- t.Value dfs(t.Right, ch) } // Same 检测树 t1 和 t2 是否含有相同的值。 func Same(t1, t2 *tree.Tree) bool { ch1, ch2 := make(chan int), make(chan int) go Walk(t1, ch1) go Walk(t2, ch2) for i := range ch1 { // ch1 关闭后 for循环自动跳出 \tif i != \u0026lt;-ch2 { return false } } return true } func main() { fmt.Println(Same(tree.New(1), tree.New(1))) } 练习：Web 爬虫 在这个练习中，我们将会使用 Go 的并发特性来并行化一个 Web 爬虫。\n修改 Crawl 函数来并行地抓取 URL，并且保证不重复。\n提示：你可以用一个 map 来缓存已经获取的 URL，但是要注意 map 本身并不是并发安全的！\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) type Fetcher interface { // Fetch 返回 URL 的 body 内容，并且将在这个页面上找到的 URL 放到一个 slice 中。 \tFetch(url string) (body string, urls []string, err error) } // Crawl 使用 fetcher 从某个 URL 开始递归的爬取页面，直到达到最大深度。  type CrawlRecord struct{ m map[string]int mux sync.Mutex wg sync.WaitGroup } var\tcr = CrawlRecord{m: make(map[string]int)} func Crawl(url string, depth int, fetcher Fetcher) { defer cr.wg.Done() // TODO: 并行的抓取 URL。 \t// TODO: 不重复抓取页面。  // 下面并没有实现上面两种情况： \tif depth \u0026lt;= 0 { return } cr.mux.Lock() cr.m[url]++ cr.mux.Unlock() body, urls, err := fetcher.Fetch(url) if err != nil { fmt.Println(err) return } fmt.Printf(\u0026#34;found: %s %q\\n\u0026#34;, url, body) for _, u := range urls { cr.mux.Lock() if _,ok := cr.m[u]; !ok{ cr.wg.Add(1) go Crawl(u, depth-1, fetcher) } cr.mux.Unlock() } return } func main() { cr.wg.Add(1) Crawl(\u0026#34;https://golang.org/\u0026#34;, 4, fetcher) cr.wg.Wait() } // fakeFetcher 是返回若干结果的 Fetcher。 type fakeFetcher map[string]*fakeResult type fakeResult struct { body string urls []string } func (f fakeFetcher) Fetch(url string) (string, []string, error) { if res, ok := f[url]; ok { return res.body, res.urls, nil } return \u0026#34;\u0026#34;, nil, fmt.Errorf(\u0026#34;not found: %s\u0026#34;, url) } // fetcher 是填充后的 fakeFetcher。 var fetcher = fakeFetcher{ \u0026#34;https://golang.org/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;The Go Programming Language\u0026#34;, []string{ \u0026#34;https://golang.org/pkg/\u0026#34;, \u0026#34;https://golang.org/cmd/\u0026#34;, }, }, \u0026#34;https://golang.org/pkg/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;Packages\u0026#34;, []string{ \u0026#34;https://golang.org/\u0026#34;, \u0026#34;https://golang.org/cmd/\u0026#34;, \u0026#34;https://golang.org/pkg/fmt/\u0026#34;, \u0026#34;https://golang.org/pkg/os/\u0026#34;, }, }, \u0026#34;https://golang.org/pkg/fmt/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;Package fmt\u0026#34;, []string{ \u0026#34;https://golang.org/\u0026#34;, \u0026#34;https://golang.org/pkg/\u0026#34;, }, }, \u0026#34;https://golang.org/pkg/os/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;Package os\u0026#34;, []string{ \u0026#34;https://golang.org/\u0026#34;, \u0026#34;https://golang.org/pkg/\u0026#34;, }, }, } 旅行终点 你可以从安装 Go 开始。\nwget -c https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz -O - | sudo tar -xz -C /usr/local vim /etc/profile export PATH=$PATH:/usr/local/go/bin export GOPROXY=https://goproxy.cn,direct export GO111MODULE=on source ~/.profile go version 一旦安装了 Go，Go 文档是一个极好的 应当继续阅读的内容。 它包含了参考、指南、视频等等更多资料。\n了解如何组织 Go 代码并在其上工作，参阅此视频，或者阅读如何编写 Go 代码。\n如果你需要标准库方面的帮助，请参考包手册。如果是语言本身的帮助，阅读语言规范是件令人愉快的事情。\n进一步探索 Go 的并发模型，参阅 Go 并发模型(幻灯片)以及深入 Go 并发模型(幻灯片)并阅读通过通信共享内存的代码之旅。\n想要开始编写 Web 应用，请参阅一个简单的编程环境(幻灯片)并阅读编写 Web 应用的指南。\n函数：Go 中的一等公民展示了有趣的函数类型。\nGo 博客有着众多关于 Go 的文章和信息。\nmikespook 的博客中有大量中文的关于 Go 的文章和翻译。\n开源电子书 Go Web 编程和 Go 入门指南能够帮助你更加深入的了解和学习 Go 语言。\n访问 go-zh.org 了解更多内容。\n","date":"2022-02-22T17:20:32+08:00","image":"https://go.dev/images/gophers/biplane.svg","permalink":"https://tweakzx.github.io/p/golanggo%E8%AF%AD%E8%A8%80%E4%B9%8B%E6%97%85/","title":"【Golang】Go语言之旅"},{"content":"PolarDB Serverless论文阅读报告 摘要 数据库管理系统的上云是近期很火的研究趋势，因为这样可以获得更高的弹性，可用性以及更低的成本，传统的独块的数据库架构很难满足这样的要求。高速网络与新的内存技术（例如RDMA）的发展，给分散式数据库带来了可能：它将原先的独块的服务器资源分离解耦到资源池中，再通过高速网络连接起来。下一代的云原生数据库就是为了分散化的数据中心而设计。\nPolarDB Serverless 遵循分散式的设计范式而设计，解耦了计算节点的CPU，内存，存储资源。\n 每种资源可以随需求而独立的增长或缩小，保障可靠性的同时，可以进行多维度的按需供应。 同时采用了优化策略如乐观锁和预取策略来改善性能。 还可以实现更快的故障恢复。  介绍 使用云数据库的三个好处：  按需付费可以使得用户减小开支。 更高的资源弹性可以应对短暂的资源使用高峰期。 更快的升级与更快的错误修复。快速的升级迭代可以保证产品竞争力，错误修复可以在不影响产品可用性的前提下进行。  经典的云数据库的架构  monolithic machine 独块的机器  特点：所有的资源都是紧耦合的 问题：  在进行数据库实例到机器的分发过程中要解决一个装箱问题 总是有碎片化的资源，难以达到高的使用率 运行时不能根据负载调整资源 资源间的命运共享，一个资源的故障会导致其他资源的故障，不能独立透明地修复故障，导致修复时间很长     存算分离的架构：  两种：  virtual machine with remote disk 搭载远程硬盘的虚拟机 shared storage 共享存储   优点  DBaaS可以提高存储池的使用率 共享存储可以进一步减少成本：原数据与只读备份可以共享存储   问题  CPU和内存的装箱问题依旧存在 缺乏灵活可放缩的内存资源 每个只读备份在内存中都要有冗余的内存开销      本文提出了一种新架构，分散架构（disaggragation architecture)  运行在分散的数据中心（DDC），CPU、内存和存储解耦，资源间通过高速网络连接 效果  每一种资源都可以提高其利用率，可以独立地放缩其资源量 解决了资源间的命运共享问题，资源故障可以被独立修复 数据页可以在远程内存池中被共享，所以解决了备份的内存冗余问题    云原生数据库 多数的云数据库是基于共享存储的架构，内存和CPU绑成最小的资源单元，只能按照最小资源单元的粒度来增长和释放资源，这会带来很多的资源浪费。\nPolarDB Serverless则是遵循分散架构的一个云原生数据库\n  引入了多租户可放缩的内存池，可以进行内存页的分配与生命周期管理\n  节点组成：\n 一个 RW node(存原数据页节点) 多个 RO node（存只读备份节点）    面临的挑战\n 添加远程内存之后，系统可以正确地处理事务  当写后读时，系统不应该错过任何一个update。使用缓存失效机制来保证写后所有地节点会更新 在更新B+树地索引的时候，使用global latch全局页锁来保证RO节点不能看到不一致的索引结构 使用读视图来确保RO节点不会读到未提交的事务   高效地执行事务  广泛使用了RDMA操作，尤其是one-sided RDMA verbs 为了提高并发，使用了乐观锁 存储方面使用了page materialization offloading技术，使用redo日志生成page 利用了预取来提高本地命中率   构建可信系统  对不同的节点都设计了策略来处理单点故障 因为内存和存储的状态是解耦的，所以修复崩溃的速度比独块架构要快5.3倍      背景 PolarDB PolarDB是一个基于共享内存架构云原生数据库\n PolarFS是一个持久化的原子操作的可伸缩的分布式共享存储。是统一的存储资源池，其提供虚拟的volume，每个volume划分为10GB大小的chunk分布在不同的节点。每个volume最多10000个chunk，即100T的容量。每个chunk三副本，用parallel raft提供线性一致性。 RW和RO节点之间通过redo日志来同步内存状态，通过LSN（log sequence number）来协调一致性。RW和RO节点上有负责处理SQL语句的处理器和事务引擎（InnoDB，X-Engine），以及一个缓存池来服务查询与事务。 有多个无状态的代理节点负责透明的负载均衡  \rimage-20211222201006368\r\n分散化的数据中心DDC   连接技术\n  在分散化的数据中心，计算节点、内存结点以及存储节点都是由高速网络连接。\n  RDMA（Remote Direct Memory Access）技术给大型的DDC带来了可能\n    层级结构\n 分为三层：spine layer、leaf layer、ToR layer 每一个ToR节点链接48台主机，ToR的交换机分别连接Leaf节点的交换机，然后Leaf的交换机又去链接Spine层的交换机。 每台主机都会配有双端口的RDMA网卡，用于链接两个ToR节点来避免单点故障。 一个leaf 交换机组包含互为备份的同时工作的多个leaf交换机 由一个leaf交换机组管理控制的所有的交换机与服务器称为一个PoD（Point of Delivery）。一个PoD最多有32个leaf交换机。    资源部署方式\n 单个数据库实例所需的内存和存储会部署在一个PoD下 不同实例部署在不同的PoD下 计算和内存资源总是倾向于部署在同一个ToR下，以获得更低的延迟与更少的页抖动    \rimage-20211222201043429\r\n无服务数据库 无服务数据库是云原生数据库的高弹性变种，主要目的是为了实现资源的按需分配\n 自动扩缩 auto-scaling  现存的无服务数据库的扩缩容因为是基于共享存储的架构，所以扩缩容受到限制。由于内存和CPU资源总是深度绑定在一个资源单位上，扩缩容也只能按照资源单位作为调动粒度。所以CPU和内存资源总是不能得到充分利用。 分析性数据库对内存的要求比较高，只需要少量的CPU资源来定期同步更新数据 事务性数据库少量的内存就可以保证缓存命中率，但需要更多的CPU资源来应对访问的高峰时刻   自动暂停 auto-pause  现存的无服务数据库的自动暂停也是受限的，CPU和内存资源必须被同时释放 在分散式架构下，CPU和内存不再共享命运。业务低峰期，内存可以不必释放，避免了重新加载。   扩容透明性 scaling transparency  透明性即在扩缩容的时候需要保证客户的场景不能中断或者性能出现严重影响 分散式架构下，中间临时状态如脏页、事务状态如版本信息timestamp、逻辑锁、query中间结果都可以保存在共享内存层，给实现扩缩容的透明性提供了更好的条件。 PolarDB Serverless 目前将脏页存在共享内存里，其他的临时状态期待后续的工作。    设计 PolarDB Serverless 基于PolarDB开发的分散式架构的云原生数据库。\n 组成：多个代理节点，一个RW节点，多个RO节点 使用PolarFS来做共享存储 和PolarDB的最大不同在于使用了远程内存池（共享内存）  使用共享内存\n 好处  RW和RO共用数据页，省去了每个节点自己保存副本，提高了内存的使用效率   坏处（performance penalty）  远程内存的访问速度远远低于本地内存；解决方案：分层内存系统和预取技术 私有数据放在公共资源上，需要有跨节点的互斥保护；解决方案：广泛使用单边的RDMA谓词和乐观协议来避免使用全局锁（global latch） 页的传输和网络带来了负担。解决方案：先将redo log写到存储层，再异步地通过日志将页物质化    分散化内存 远程内存访问接口  page_register: 页的引用计数增一 page_unregister: 页的引用计数减一 page_read: 使用单边RDMA谓词将页从远程内存读取到本地 page_write: 使用单边RDMA谓词将页从本地写到远程内存 page_invalidation: 使所有RO节点的本地的内存副本失效  远程内存管理 内存的分配单元是一个slab，一个slab的大小是1GB\n几种数据结构如下：\n Page Array（PA）  每一个slab由页数组组成，页数组是物理地址连续的由16KB的页组成的数组。 PA的地址会注册到RDMA的网卡上，所以可以被RDMA远程访问 负责提供slab访问的节点也叫slab node，第一个slab node 也叫home node   Page Address Table（PAT）  一个哈希表，存储每一页的位置（slab id 与物理地址）   Page Invalidation Bitmap (PIB)  一个位图，对应PAT的每一项记录invalidation bit，0表示内存中的是最新版本，1表示不是最新版本   Page Reference Directory(PRD)  一个map，对应于每个PAT项，记录引用了这个页的节点   Page Latch Table(PLT)  对于PAT中的每一项，管理其页锁（page latch）。 是一个全局的物理锁，用于多个数据库节点之间进行保护与同步读写 尤其用于保护B+树的结构一致性    page分配过程：\n 数据库向home node 发送page register请求 如果不存在，则遍历slab找到有空闲的slab，如果都没有空闲则使用LRU算法淘汰掉一些page 写入后，将page的为位置信息写入到PAT，返回page的远程地址和page latch  扩缩容：\n 扩容：home node 请求DBaaS分配新的slab，扩展buffer pool，PAT，PIB和PRD 缩容：page通过LRU进行淘汰，无用的slab将被回收  本地缓存  以page为单位将远程内存缓存到本地 如果page不在远程内存上，进程会从Polar FS读取内容到本地缓存，然后再写回到远程内存。（存储和内存不发生直接接触） 不是所有的page都要写入远程内存，例如全表扫描，这些page不太会再次访问 本地发生miss，进程要等待读取远程内存（可以利用预取技术） 本地的缓存写满后使用LRU算法进行淘汰。如果page没有修改过可以直接释放。如果page修改过，那就写回后再释放。引用计数减一。  缓存一致性  RO节点不能直接访问RW节点的本地缓存，只能通过获取最新的redo log来在本地物质化page来获取最新的数据。 如果RW节点将修改过的数据写回远程内存后，RO节点自然是不需要读取redo log，可是系统如果每次修改都要立刻写回，网络开销会很大。所以RW节点不会立刻写回，而是使用了页失效的方法。  \rimage-20211225120332727\r\n 具体过程如下：  RW修改page后，调用page_invalidation 发送请求到home node PIB中对应页项设置为1 查询PRD，查看page在哪些RO节点上存在 向所有的存在这个page的RO发送请求，将PIB设置为1 设置PIB过期是一个同步阻塞操作 只有全部的RO节点设置成功才返回成功 如果有个RO节点超时，那么就把他踢出集群来保证该操作成功   在写回到Polar FS（远程存储）的时候，同样会使用page_invalid来保证远程内存中不会存在比远程存储中更老版本的数据。  B+树结构一致性   物理一致性\n问题是：多线程访问B+树的Index的时候如何做到并发控制\n解决方案：\n  只有RW节点可以修改page，所以不需要管理多节点的写冲突。但是SMO（结构修改操作）会同时修改多个page，其他节点在遍历B+树的时候，不同的RO节点可能看到不一致的物理B+树结构。\n 我们使用全局页锁来解决这个问题，使用共享锁（S）和排他锁（X）。 区别于本地页锁，全局页锁用于保证多节点下的B+树索引结构一致性。使用crabbhing/lock coupling算法实现 所有参与SMO的节点会加上X锁，直到SMO结束。RO节点读取的时候要检查PLT，查看是否有X锁，并且向被读的页上加S锁。    对于RW节点要进行的insert和delete操作，我们采取两阶段做法\n 采用乐观并发控制，假设没有SMO操作，那就只需要本地的锁来作单节点并发控制。如果确实不需要进行SMO，那就顺利插入或者删除。 如果发现B+树的叶子节点相对空或者相对满，即很有可能要发生SMO操作，就启用悲观策略，这时候会对所有可能参与SMO的page加上X锁，直到SMO结束释放锁。这样锁的排序可以保证RO节点只能看到SMO之前或者之后的树结构，而不会是SMO的中间状态。      逻辑一致性\n 相同的数据可能会有多个事务进行并发处理，如何保证满足不同的快照隔离级别。    快照隔离 PolarDB Serverless 提供基于MVCC的快照隔离。事务的实现机制，是由快照的时间戳来控制事务能看到数据的版本信息。RW节点会维护一个中心化的的时间戳叫CTS，来为所有数据库节点分配全局单调递增的时间戳。\n一次读写事务需要获取两次时间戳（cts_read和cts_commit）；提交事务的时候所有记录和undo log中的记录都需要额外维护一个列来保存cts_commit;一个读写事务内的read总会返回一个cts_commit比cts_read要小的记录;每个记录都会有一个事务id字段来记录修改该记录的事务。\n一个只读事务则只需要在事务开始的时候获取cts_read时间戳即可。\n但是对于一些比较heavy的事务，无法迅速对所有记录更新cts_commit，因为这会在提交事务的时候带来大量的随机写。因此这个cts_commit的更新是异步执行的。这就导致了在并发事务处理的时候，无法得知该记录的cts_commit是否已经被写或者是否要被写，即无法得知先前的事务是否已经完成或者该记录是否与先前事务有关。\n解决方案：通过查询RW上的CTS Log数据结构，其是一个环形数组：记录着最近若干个的读写事务的提交时间戳(cts_commit)。如果先前事务没有完成，这个数组上的commit timestamp就会为null，每当一个节点读取到一个cts_commit为空的记录，就可以去查询这个全局的环形数组来得知这个记录对应的事务是否已经提交了。\n同样要去全局查这个时间戳要对应一次network request，本系统还是采用RDMA 来加速环形数组的访问获取时间戳的过程，RDMA CAS技术能够原子递增获取时间戳，并且这个数组的地址也会被注册到RDMA 的网卡上。和用RPC相比，直接走网卡不需要占用RW节点的CPU资源。\nPage Materialization Offloading 传统的DB会周期性的将脏页写入到持久性的存储中。Aurora提出了日志即数据库的概念，通过使用redo log来物质化页数据来获取最新版本的数据。Socrates在此基础上，则做到了将log与数据分离存储。\nPolarrDB Serverless和Socrates类似，将logs和pages分开存储在不同的chunks中。\n redo logs先存储在logs chunks中 异步发送到page chunks 为了PolarFS组件的重用和最小化更改，logs仅仅送到leader node。 然后leader node 重做并使用ParallelRaft保证副本的一致性  自动扩缩 在版本的升级和重启过程中，用户对Serverless服务是无感知的，断开连接，事务中断，请求超时是不允许的。\n所以在本系统中，当发生版本升级和跨节点迁移的时候，代理节点负责保持客户端连接，发送请求后会等待旧的RW节点处理掉正在进行的事务。之后把就节点的脏页flush到共享的内存池中，再关闭就RW节点。新的RW节点连接内存池，预热缓存，重做undo log来构建事务的列表。之后代理节点将连接还给新的RW节点。\n性能优化 使用分散式架构会带来性能的损失，所以要使用一些性能优化手段来进行优化。\n乐观锁 使用乐观锁可以尽可能避免并发操作对全局锁的获取，进而提高并行效率。\nRW节点会维护一个SMO counter，每次发生SMO的时候counter++；并且所有被修改的Page也会维护这个更新后的counter（SMO RW）\n每次query执行的时候去拿这个SMO counter（SMO query），一旦RO上的query发现某个Page的SMO counter比SMO query还要大，说明在query执行过程中发生了SMO。这个时候就要回滚到悲观并发控制，即获取全局PL来锁住整个B+ Tree的SMO更新。\n预取 在PolarDB Serverless中，我们提出了批处理密钥预处理(BKP)。BKP从分解的内存和存储中预取包含有趣的元组的页面，以隐藏远程I/O延迟。BKP的接口接受一组要预取的密钥。当调用该接口时，引擎将启动一个后台预取任务，从目标索引中检索所需的密钥，并在必要时从远程内存/存储中获取相应的数据页。BKP还可以优化分析工作负载。\n容错与恢复策略 数据库节点恢复 本系统采用的是ARIES的恢复算法（Algorithm for Recovery and Isolation Exploiting Semantics. ）\n RO节点的恢复：由于页数据再远程内存上，所以可以轻松地使用新的RO节点代替旧的RO节点 RW节点的恢复  无预期的节点故障  RW节点挂掉之后，集群的manager（CM）会通过心跳信号探测到，然后RO节点就可以晋升为RW节点。   有预期的节点故障  例如版本升级，前文已经讲过，不再赘述。      内存结点恢复 内存节点的数据缓存Page，在把dirty page写到内存节点前，对应的redo log已经flush到存储层了，因此内存节点重启可以用PolarFS上的redolog来进行恢复数据。\nhome node上因为包含重要的metadata如PAT，PIB，PRD和PLT；这些数据会同步备份在从副本；home node负责检测slab node上的故障，然后home node根据PAT上的信息来重建重启的slab node即可。\n集群恢复 在极少数情况下，当主节点的所有副本都不可用时，需要通过集群恢复来恢复服务。所有数据库节点和内存节点将从清除状态重新启动，所有内存状态将从存储重新启动。初始化后（连接到远程内存和存储器等），RW节点执行之前所述的并行REDO恢复，然后扫描撤销头以查找所有未完成的事务。之后，RW节点将启动服务，并在后台回滚未提交的事务。在集群恢复过程中，缓存在远程内存中的页面将被清除，因此它将忍受冷缓存问题。\n","date":"2021-12-22T14:17:12+08:00","image":"https://2021.sigmod.org/images/2021sigmod-logo1.jpg","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0polardb-serverless%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/","title":"【论文笔记】PolarDB-Serverless论文阅读报告"},{"content":"1 HPL（High Performance Linpack) 假设要使用HPL程序在4个进程上解一个4096 * 4096的方程组（4096 * 4096的矩阵加一列方程组的右端项b），按照讲义第14页所示的block-cyclic方式对数据进行分配，NB=512。4个进程按1 * 4和4 * 1两种方式排布。那么，在HPL的回代部分（讲义48到55页），X的各个元素分别是由哪些进程算出的？例如，X[0..512]由进程(3, 0)求出。写出两种排布方式下X的各部分分别由哪些进程计算得到。（5分）\n 1 * 4 排布  \rimage-20211214170301242\r\n   X 进程     X[0..511] (0,0)   X[512..1023] (0,1)   X[1024..1535] (0,2)   X[1536..2047] (0,3)   X[2048..2559] (0,0)   X[2560..3071] (0,1)   X[3072..3583] (0,2)   X[3584..4095] (0,3)     4 * 1 排布  \rimage-20211214170325158\r\n   X 进程     X[0..511] (0,0)   X[512..1023] (0,1)   X[1024..1535] (0,2)   X[1536..2047] (0,3)   X[2048..2559] (0,0)   X[2560..3071] (0,1)   X[3072..3583] (0,2)   X[3584..4095] (0,3)    ","date":"2021-12-14T13:55:35+08:00","image":"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpmob0455c.pic27.websiteonline.cn%2Fupload%2Felinpack3000-1200_dps0.jpg\u0026refer=http%3A%2F%2Fpmob0455c.pic27.websiteonline.cn\u0026app=2002\u0026size=f9999,10000\u0026q=a80\u0026n=0\u0026g=0n\u0026fmt=jpeg?sec=1642054231\u0026t=55c0893a251868314596e23dae76a843","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E4%BD%9C%E4%B8%9A-3/","title":"【分布式与并行计算】并行作业-3"},{"content":"1 加速向量加法 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;assert.h\u0026gt; inline cudaError_t checkCuda(cudaError_t result) { if (result != cudaSuccess) { fprintf(stderr, \u0026#34;CUDA Runtime Error: %s\\n\u0026#34;, cudaGetErrorString(result)); assert(result == cudaSuccess); } return result; } void initWith(float num, float *a, int N) { for(int i = 0; i \u0026lt; N; ++i) { a[i] = num; } } __global__ void addVectorsInto(float *result, float *a, float *b, int N) { int initIndex = threadIdx.x + blockIdx.x * blockDim.x; int gridStride = gridDim.x * blockDim.x; for(int i = initIndex;i\u0026lt;N;i+=gridStride){ result[i] = a[i] + b[i]; } } void checkElementsAre(float target, float *array, int N) { for(int i = 0; i \u0026lt; N; i++) { if(array[i] != target) { printf(\u0026#34;FAIL: array[%d] - %0.0f does not equal %0.0f\\n\u0026#34;, i, array[i], target); exit(1); } } printf(\u0026#34;SUCCESS! All values added correctly.\\n\u0026#34;); } int main() { const int N = 2\u0026lt;\u0026lt;20; size_t size = N * sizeof(float); float *a; float *b; float *c; cudaMallocManaged(\u0026amp;a, size); cudaMallocManaged(\u0026amp;b, size); cudaMallocManaged(\u0026amp;c, size); initWith(3, a, N); initWith(4, b, N); initWith(0, c, N); size_t threads_per_block = 1024; size_t number_of_blocks = (N+threads_per_block-1)/threads_per_block; addVectorsInto\u0026lt;\u0026lt;\u0026lt;32,1024\u0026gt;\u0026gt;\u0026gt;(c, a, b, N); //addVectorsInto\u0026lt;\u0026lt;\u0026lt;number_of_blocks,threads_per_block\u0026gt;\u0026gt;\u0026gt;(c, a, b, N);  checkCuda(cudaGetLastError()); checkCuda(cudaDeviceSynchronize()); checkElementsAre(7, c, N); cudaFree(a); cudaFree(b); cudaFree(c); } 2 加速SAXPY #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;assert.h\u0026gt;#define N 2048 * 2048 // Number of elements in each vector  /* * Optimize this already-accelerated codebase. Work iteratively, * and use nsys to support your work. * * Aim to profile `saxpy` (without modifying `N`) running under * 20us. * * Some bugs have been placed in this codebase for your edification. */ inline cudaError_t checkCuda(cudaError_t result) { if (result != cudaSuccess) { fprintf(stderr, \u0026#34;CUDA Runtime Error: %s\\n\u0026#34;, cudaGetErrorString(result)); assert(result == cudaSuccess); } return result; } __global__ void saxpy(float * a, float * b, float * c) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int stride = gridDim.x * blockDim.x; for(int i = tid; i\u0026lt;N; i+=stride){ c[tid] = 2 * a[tid] + b[tid]; } } int main() { int deviceId; int numberOfSMs; cudaGetDevice(\u0026amp;deviceId); cudaDeviceGetAttribute(\u0026amp;numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); float *a, *b, *c; int size = N * sizeof (float); // The total number of bytes per vector  cudaMallocManaged(\u0026amp;a, size); cudaMallocManaged(\u0026amp;b, size); cudaMallocManaged(\u0026amp;c, size); // Initialize memory  for( int i = 0; i \u0026lt; N; ++i ) { a[i] = 2.0; b[i] = 1.0; c[i] = 0.0; } cudaMemPrefetchAsync(a, size, deviceId); cudaMemPrefetchAsync(b, size, deviceId); cudaMemPrefetchAsync(c, size, deviceId); int threads_per_block = 256; int number_of_blocks = numberOfSMs * 32 ; saxpy \u0026lt;\u0026lt;\u0026lt; number_of_blocks, threads_per_block \u0026gt;\u0026gt;\u0026gt; ( a, b, c ); checkCuda(cudaGetLastError()); checkCuda(cudaDeviceSynchronize()); // Print out the first and last 5 values of c for a quality check  for( int i = 0; i \u0026lt; 5; ++i ) printf(\u0026#34;c[%d] = %f, \u0026#34;, i, c[i]); printf (\u0026#34;\\n\u0026#34;); for( int i = N-5; i \u0026lt; N; ++i ) printf(\u0026#34;c[%d] = %f, \u0026#34;, i, c[i]); printf (\u0026#34;\\n\u0026#34;); cudaFree( a ); cudaFree( b ); cudaFree( c ); } 3 N-body #include \u0026lt;math.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026#34;timer.h\u0026#34;#include \u0026#34;files.h\u0026#34; #define SOFTENING 1e-9f  #include \u0026lt;assert.h\u0026gt;inline cudaError_t checkCuda(cudaError_t result) { if (result != cudaSuccess) { fprintf(stderr, \u0026#34;CUDA Runtime Error: %s\\n\u0026#34;, cudaGetErrorString(result)); assert(result == cudaSuccess); } return result; } /* * Each body contains x, y, and z coordinate positions, * as well as velocities in the x, y, and z directions. */ typedef struct { float x, y, z, vx, vy, vz; } Body; /* * Calculate the gravitational impact of all bodies in the system * on all others. */ __global__ void bodyForce(Body *p, float dt, int n) { int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = gridDim.x * blockDim.x; for (int i = index; i \u0026lt; n; i+=stride) { float Fx = 0.0f; float Fy = 0.0f; float Fz = 0.0f; for (int j = 0; j \u0026lt; n; j++) { float dx = p[j].x - p[i].x; float dy = p[j].y - p[i].y; float dz = p[j].z - p[i].z; float distSqr = dx*dx + dy*dy + dz*dz + SOFTENING; float invDist = rsqrtf(distSqr); float invDist3 = invDist * invDist * invDist; Fx += dx * invDist3; Fy += dy * invDist3; Fz += dz * invDist3; } p[i].vx += dt*Fx; p[i].vy += dt*Fy; p[i].vz += dt*Fz; } } int main(const int argc, const char** argv) { // The assessment will test against both 2\u0026lt;11 and 2\u0026lt;15.  // Feel free to pass the command line argument 15 when you gernate ./nbody report files  int nBodies = 2\u0026lt;\u0026lt;11; if (argc \u0026gt; 1) nBodies = 2\u0026lt;\u0026lt;atoi(argv[1]); // The assessment will pass hidden initialized values to check for correctness.  // You should not make changes to these files, or else the assessment will not work.  const char * initialized_values; const char * solution_values; if (nBodies == 2\u0026lt;\u0026lt;11) { initialized_values = \u0026#34;files/initialized_4096\u0026#34;; solution_values = \u0026#34;files/solution_4096\u0026#34;; } else { // nBodies == 2\u0026lt;\u0026lt;15  initialized_values = \u0026#34;files/initialized_65536\u0026#34;; solution_values = \u0026#34;files/solution_65536\u0026#34;; } if (argc \u0026gt; 2) initialized_values = argv[2]; if (argc \u0026gt; 3) solution_values = argv[3]; const float dt = 0.01f; // Time step  const int nIters = 10; // Simulation iterations  int bytes = nBodies * sizeof(Body); float *buf; cudaMallocManaged(\u0026amp;buf, bytes); Body *p = (Body*)buf; read_values_from_file(initialized_values, buf, bytes); double totalTime = 0.0; /* * This simulation will run for 10 cycles of time, calculating gravitational * interaction amongst bodies, and adjusting their positions to reflect. */ int deviceId; int numberOfSMs; cudaGetDevice(\u0026amp;deviceId); cudaDeviceGetAttribute(\u0026amp;numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); int threads_per_block = 128; int number_of_blocks = numberOfSMs * 25 ; for (int iter = 0; iter \u0026lt; nIters; iter++) { StartTimer(); /* * You will likely wish to refactor the work being done in `bodyForce`, * and potentially the work to integrate the positions. */ bodyForce\u0026lt;\u0026lt;\u0026lt;threads_per_block,number_of_blocks\u0026gt;\u0026gt;\u0026gt;(p, dt, nBodies); // compute interbody forces  checkCuda(cudaGetLastError()); checkCuda(cudaDeviceSynchronize()); /* * This position integration cannot occur until this round of `bodyForce` has completed. * Also, the next round of `bodyForce` cannot begin until the integration is complete. */ for (int i = 0 ; i \u0026lt; nBodies; i++) { // integrate position  p[i].x += p[i].vx*dt; p[i].y += p[i].vy*dt; p[i].z += p[i].vz*dt; } const double tElapsed = GetTimer() / 1000.0; totalTime += tElapsed; } double avgTime = totalTime / (double)(nIters); float billionsOfOpsPerSecond = 1e-9 * nBodies * nBodies / avgTime; write_values_to_file(solution_values, buf, bytes); // You will likely enjoy watching this value grow as you accelerate the application,  // but beware that a failure to correctly synchronize the device might result in  // unrealistically high values.  printf(\u0026#34;%0.3f Billion Interactions / second\u0026#34;, billionsOfOpsPerSecond); cudaFree(buf); } 4 证书 \rimage\r\n","date":"2021-12-13T19:03:20+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/image-20211215144717123.png","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97cuda%E5%8A%A0%E9%80%9F%E8%AF%BE%E7%A8%8B%E9%A2%98%E7%9B%AE/","title":"【分布式与并行计算】CUDA加速课程题目"},{"content":"1 矩阵向量乘法（6分） 矩阵向量乘法(gemv)如何用OpenMP或pthread对其并行化(OpenMP和pthread任选一种即可)？假设矩阵按行存储（每一行数据是连续的），处理器有32个核。如果矩阵是按列存储呢？具体实现如何修改？\n可用语言详细描述或写出伪代码\n  按行存储\nvoid *worker1(int row, int N, int** A, int* vec, int* result){ for(int i = row; i\u0026lt;N; i += 32){ result[i] = innerProduct(A[i],Vec); } } int main(){ ... for(int i=0;i\u0026lt;32;i++){ data_array[i].row = i; pthread_create(\u0026amp;threads[i], NULL, worker1, (void *)\u0026amp;data_array[i]) } ... }   按列存储\nvoid *worker2(int column, int N, int M, int** A, int* vec, int* result){ for(int j = column; j\u0026lt;M; j += 32){ for(int i = 0; i\u0026lt;N; i++){ result[i] += A[j][i]*Vec[j]; } } } int main(){ ... for(int i=0;i\u0026lt;32;i++){ data_array[i].column = i; pthread_create(\u0026amp;threads[i], NULL, worker1, (void *)\u0026amp;data_array[i]) } ... }   2 程序分析（4分） 以下程序运行时会出现什么现象？可以如何改写来避免此现象发生？\nint selected_thread; sem_t start1, start2, stop1, stop2; void* worker1() { sem_wait(\u0026amp;start1); sem_post(\u0026amp;stop1); } void *worker2(){ sem_wait(\u0026amp;start2); sem_post(\u0026amp;stop2); } int main(int argc, char *argv[]) { selected_thread = 2; sem_init(\u0026amp;start1); sem_init(\u0026amp;start2); sem_init(\u0026amp;stop1); sem_init(\u0026amp;stop2); pthread_create(worker1); pthread_create(worker2); sem_post(\u0026amp;start1); sem_wait(\u0026amp;stop1); sem_wait(\u0026amp;stop2); return 0; } 答： worker2会一直等待，程序无法结束。所以在主函数里加上sem_post(\u0026amp;start2)。\n","date":"2021-12-11T18:01:26+08:00","image":"https://pic1.zhimg.com/v2-f9d2f5486443cc045996753d59f80a7e_1440w.jpg?source=172ae18b","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E4%BD%9C%E4%B8%9A-2/","title":"【分布式与并行计算】并行作业-2"},{"content":"1矩阵向量乘法（4分） 讲义55页所示结果Y，如果要作为下一次矩阵向量乘法的输入X，切分到不同的列进程，并且复制到每一行进程，应如何操作？可写出伪代码，或用语言描述。\n即图(a)中的Y，变成下图(b)中的X。假设每行有P个进程，每列也是P个进程，一共P*P个进程。\n\rimage-20211210120943046\r\n答：进程$P_{ij}$和所控制的矩阵进行矩阵向量乘之后，将结果存入进程$P_{ji}$。\n2 代码填空（3分） 在测量程序性能时，我们经常要记录整个程序或程序中某一部分的运行时间。在MPI程序中，由于每个进程的运行时间不同，一般需要取各个进程运行时间的最大值，然后由0号进程保存和打印（其他进程不需要保存）。以下程序完成了这个功能，请在横线处填上函数调用语句。\nint main(int argc, char * argv) { double total_time; double time0, time1; int procs, rank; MPI_init(argc, argv); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;procs); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); time0 = MPI_Wtime(); //do some computation \ttime1 = MPI_Wtime() – time0; _______________________________________________________________ if(rank == 0) { Printf(“Total execution time is %f seconds\\n”, total_time); } } 答：\nMPI_Reduce(\u0026amp;time1,\u0026amp;total_time,1,MPI_DOUBLE,MPI_MAX,0,MPI_COMM_WORLD) 3 以下程序相当于哪个MPI聚合操作？（3分） #define N 16384 double *send_buff, *recv_buff; MPI_Status status; int i, nprocs, myid, count=N/num_procs; send_buff = (double*)malloc(N*sizeof(double)); recv_buff = (double*)malloc(N*sizeof(double)); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;nprocs); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;myid); //此处省略send_buff中数据的初始化 memcpy((void*)(recv_buff + myid * count), (void*)(send_buff + myid * count), count*sizeof(double)); for(int i = 0; i \u0026lt; nprocs;i++) { if(i!=myid) { MPI_Sendrecv(send_buff+i*count, count, MPI_DOUBLE, i, 400, recv_buff+i*count, count, MPI_DOUBLE, i, 400, MPI_COMM_WORLD, \u0026amp;status); } } 答：MPI_Alltoall()\n","date":"2021-12-10T11:28:33+08:00","image":"https://img2.baidu.com/it/u=2900542081,1673808678\u0026fm=26\u0026fmt=auto","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E4%BD%9C%E4%B8%9A-1/","title":"【分布式与并行计算】并行作业-1"},{"content":"1 逻辑时钟与一致割集 下图中，直线上小黑点给出了时钟计数，请分别用Lamport 逻辑时钟和向量时钟给图上的事件设置时间戳，并给出一致割集和非一致割集的例子。\n\r事件时钟计数\r\n答：\n  设置时间戳\n  Lamport逻辑时钟\n\rLamport逻辑时钟\r\n  向量时钟\n\r向量时钟\r\n    割集的例子\n  一致割集\n\r一致割集\r\n  非一致割集\n\r非一致割集\r\n包含$e_1^2$这个接收事件但是不包含$e_3^1$这个发送事件。\n    2 异步分布式系统的故障类型 考虑在异步分布式系统中使用的两个通信服务。在服务A中，消息可能丢失、被复制或延迟，校验和仅应用到消息头。在服务B中，消息可能丢失、延迟或发送地太快以致接收方无法处理它，但到达目的地的消息其内容一定正确。描述每个服务会有的故障类型。根据对有效性和完整性的影响将故障分类。服务B能被描述成一个可靠的通信服务吗？\n答：\n  服务A会有的故障类型\n 遗漏故障  消息丢失   拜占庭故障  checksum仅仅应用到消息的head，消息的body可能发生损坏 消息重复   因为这是异步分布式系统，所以不会有时序故障。  遗漏故障的消息丢失破坏了有效性。拜占庭故障的可能损坏的消息以及重复的消息破坏了完整性。\n  服务B会有的故障类型\n 遗漏故障  消息丢失 发送地太快以致接收方无法处理它，接收遗漏   因为这是异步分布式系统，所以不会有时序故障。  服务B满足完整性，但是服务B消息发生的遗漏故障，不满足有效性，所以不是可靠的通信服务。\n  3 Ricart and Agrawala 算法 请证明Ricart-Agrawala的互斥算法满足ME2和ME3。\n ME2:进入或退出临界区的请求最终都会成功\nME3:如果一个进入临界区的请求发生在先，那么进入临界区也按此顺序\nRicart-Agrawala算法：\n 一个进程申请资源时向所有其他进程发出带有时间戳的申请报文； 一个进程收到申请报文后，答复这个申请，当且仅当：1）若不在临界区并且自己未申请进入临界区,或者2)自己虽然发出了申请报文，但自己的报文的时间戳大于收到的申请报文。如果已经在临界区，或者自己的申请发送在前，则在出临界区之前将所有的申请挂起。 申请资源的进程仅在收到其它所有进程的回答报文后才进入临界区使用资源； 一个进程使用完资源后，它向所有挂起的申请发送回答报文。   证明：\n 证明满足ME2  一个进程$p_i$要进入临界区向其他进程发送请求，不想进入临界区的进程，或者已经发送了请求但是发送时间戳大于接收请求时间戳$T_i$的进程都会回复。所以进程$p_i$要等待逻辑时间戳$T_i$之前发送了请求的进程以及正在临界区的进程的答复。 在逻辑时间戳$T_i$之前发出请求的进程所等待进程的数量依次递减到1。等正在临界区的进程使用完资源并退出后，所有在等待的进程所需等待进程数量全都减1。此时有一个进程得到了全部的答复，进入临界区。 按照这个过程，只要前边的进程依次进入临界区并退出之后，进程$p_i$就可以成功进入临界区，毕竟没有进程可以插队，所需等待进程数量只能递减。 由于使用完资源后退出临界区不需要等待答复，所以可以成功退出。所以，满足ME2。   证明满足ME3  当进$p_i$给其他进程发送进入临界区的申请时 如果进程$p_j$也想进入临界区，发送的申请的时间戳小于收到的$T_i$，那么$p_j$不会发送答复给$p_i$,这样$p_i$就必须等待$p_j$进入并退出临界区之后才能得到答复，才有可能进入临界区。 如果进程$p_j$也想进入临界区，发送的申请的时间戳大于收到的$T_i$，那么他会回复$p_i$的申请，$p_i$无需等待$p_j$，且当$p_i$收到$p_j$的请求之后不会回复，$p_j$等待$p_i$。 所以，满足ME3    4 改进Ricart and Agrawala 算法 在Ricart-Agrawala的互斥算法中，原始假定系统的进程是不出故障的。请修改算法增加处理一个进程崩溃的情况。\n答：如果有进程崩溃，那么它永远不会回复，则发送请求的进程需要一直盲等。\n 所以每当接收到消息之后要做出答复，回复同意，当且仅当：1）若不在临界区并且自己未申请进入临界区,或者2)自己虽然发出了申请报文，但自己的报文的时间戳大于收到的申请报文。或者回复拒绝，当1）自己处在临界区，或者2）自己的临界区申请的逻辑时间戳小于收到的申请，把回复拒绝的进程缓存起来，等待回复。 如果超出timeout没有收到答复则认为机器故障，只需等待其余的机器全部回复同意 当一个进程退出临界区之后，要向所有回复拒绝的进程回复同意。  5改进基于环的互斥算法 改进基于环的互斥算法使得它能检测权标的丢失并重新生成权标。\n答：\n 一个进程发出申请之后，如果长时间没有拿到权标，则向下一个节点发送一个请求reqest确认权标是否还在。 一个进程收到了确认权标存在的请求request，如果权标在自己手里则向下一个节点发送一个exist，如果权标不在自己手里，则将requst传递给下一个节点 一个进程收到了一个exist消息则将它传递到下一个节点 如果发出申请的进程收到request请求，则认为权标丢失，重新生成权标。如果收到exist，则说明一切正常。  6 双向环结构的选举算法 基于环的选举算法是建立在单向环的假设之上的，为了获得更快的选举速度，现采用双向环结构，即每个节点可以同时向顺时针和逆时针两个方向发送选举消息，请列出新算法的高层描述，并用一个四节点的双向环来说明你的方法。\n答：\n 最初，所有进程标记为非参与者，任意一个进程发起选举，发起选举后，置自己为参与者（$elected_i = ⊥$)，向上下游发送一个选举消息，包含自己的标识符。 一个进程收到选举消息后，那么比较自己的标识符与收到消息中的标识符，  如果自己的标识符小于消息中的标识符，那么顺消息来的方向传递选举消息； 如果自己的标识符大于消息中的标识符  如果自己是非参与者，将消息中的标识符改为自己标识符，顺消息来的方向传递选举消息。 如果自己是参与者，则不转发消息。   如果自己的标识符等于消息中的标识符，那么说明自己的标识符最大，如果还未当选，则当选为协调者，向上游和下游发送一个当选消息，包含自己的标识符P   一个进程收到一个当选消息，如果自己是参与者，设置$elected_i = P$，置自己为非参与者，并且按照消息发送的方向传递给自己的邻居；如果自己已经知晓当选消息，则不转发。  Init elected != ⊥ for process i (i = 1,2,...N) in the process which start an election: function startElection(): electingMSG \u0026lt;- MSG(type = \u0026quot;electing\u0026quot;, value = Local.id) Local.elected = '⊥' send(msg = electingMSG ,direction = clockwise) send(msg = electingMSG , direction = counterclockwise) in process i: function handleMSG(MSG msg, Direction direction): if msg.type = \u0026quot;electing\u0026quot;: if Local.id \u0026lt; msg.id: send(msg,direction) Local.elected = '⊥' else if Local.id \u0026gt; msg.id: if Local.elected != '⊥': msg.value = Local.id send(msg, direction) Local.elected = '⊥' end else if(process i is not Coordinator) setCoordinator(process i) Local.elected = '⊥' electdeMSG \u0026lt;- MSG(type = \u0026quot;elected\u0026quot;, value = Local.id) send(msg = electedMSG, direction = clockwise) send(msg = electedMSG, direction = counterclockwise) end end end if msg.type = \u0026quot;elected\u0026quot; if Local.elected == '⊥': Local.elected = msg.value send(msg, direction) end end \rimage-20211209165145519\r\n图中:\t白色表示初始非参与者，黄色表示参与者，红色表示知道当选结果。蓝色是选举消息，绿色是当选消息。从13号开始发起选举，最终选出23为协调者。\n 网络带宽：找到最大标识符最多需要N个消息，确认最大标识符最多需要2N个消息，通知当选最多需要N+1个消息，则最多需要消息为 4N+1 回转时间：第一轮寻找花费最多(N/2)个消息，第二轮确认需要最多花费N个消息，第三轮通知最多需要花费(N/2)+1个消息，所以最多需要花费2N+1个消息的回转时间。  7 基于生成树的选举算法 节点之间按照生成树方式连接，仅有边相连的节点能通信，请基于此网络拓扑，设计一个选举算法，给出其伪码。当仅有一个进程发起选举，你的选举算法所需的消息量是多少？\n答：假定是有向生成树，每个节点有父节点和子节点，每个节点可以向子节点或者父节点发送消息。任何一个进程都可以发起选举。算法如下：\n  收到选举消息，将发送消息的标识符和自己的标识符作比较，更改消息中的信息为较大值；如果有子节点，则向子节点发送这个选举消息；如果是叶子节点，则父亲节点返回一个回复消息，包含当前的较大标识符。\n  等到收到所有子节点的回复消息，选出最大的标识符，返回给父亲节点\n  当根节点收到最大的 标识符，则向所有的子节点发送当选信息，直到叶节点。如果有子节点发现自己的标识符等于消息中的的标识符，则成为协调者。\n  我的选举算法所需的消息量选举有N-1个，回复有N-1个，当选有N-1个，所以总共有3N-3个。\nInit Local.elected != ⊥ for process i (i = 1,2,...N) Local.count =0 for process i (i = 1,2,...N) in the process p0 which start an election: function startElection(): electingMSG \u0026lt;- MSG(type = \u0026quot;electing\u0026quot;, value = Local.id) Local.elected = '⊥' for i in son(p0): send(electingMSG, dest = i) end in process p: function handleMSG(MSG msg): if msg.type = \u0026quot;electing\u0026quot;: Local.elected = '⊥' if !isEmpty(son(p): msg.value = max(Local.id,msg.value) for i in son(p): send(msg, dest = i) end else replyMSG = MSG(type = \u0026quot;reply\u0026quot;, value = max(Local.id,msg.value) send(relpyMSG, dest = father(p)) end if msg.type = \u0026quot;reply\u0026quot;: Local.count++; msg.value = max(Local.id, msg.value) if(Local.count == son(p).size()): if !isEmpty(father(p)): send(msg, dest = father(p)) else electedMSG = MSG(type = \u0026quot;elected\u0026quot;,value = msg.value) for i in son(p): send(electedMSG, dest = i) end Local.elected = msg.value if Local.ip == msg.value: setCoordinator(process = p) end end end end if msg.type = \u0026quot;elected\u0026quot;: if Local.ip == msg.value: setCoordinator(process = p) end if !isEmpty(son(p)): for i in son(p): send(msg, dest = i) end end 8 法定数共识复制 在服务器X、Y和Z上使用法定数共识进行复制，这些服务器都有数据项A和B的副本。A和B副本的初始值是100，并且在X、Y和Z上A和B的选票是1。同样对于A和B，R＝W＝2。一个客户读A的值然后将它写到B上。\n1）当客户执行这些操作时，出现了一个分区，将服务器X和Y与服务器Z分隔开了。描述当客户能访问服务器X和Y时，获得的法定数和发生的操作。\n2）描述当客户仅能访问服务器Z时，获得的法定数和发生的操作。\n3）分区修复了，然后另一个分区发生了，结果X和Z与Y分隔开了。描述当客户能访问服务器X和Z时，获得的法定数和发生的操作。\n答：\n1）在数据的v0版本时，A和B副本的初始值是100，出现了一个分区，将服务器X和Y与服务器Z分隔开。\n   X Y Z     A = 100（v0) A = 100（v0) A = 100（v0)   B = 100（v0) B = 100（v0) B = 100（v0)    ​\t客户可能从X或者Y上读取A，R = 1+1 =2，读取成功。\n​\t客户需要在X和Y上写B，W = 1+1 = 2， 写成功。\n2)客户只能访问服务器Z，R = 1，客户无法读取；W = 1，客户无法写。\n3)当分区被修复后，因为之前的分区导致X和Y的数据要比Z上的数据更新，例如\n   X Y Z     A = 200（v1) A = 200（v1) A = 100（v0)   B = 300（v1) B = 300（v1) B = 100（v0)    此时，另一个分区出现，X和Z与Y分隔开，当客户试图获取法定数的时候，发现Z的数据版本过时，于是Z根据X上的最新数据更新自己。之后客户获取读法定数，R = 1+1 =2，然后读成功。客户获取写法定数，W = 1+1 =2，写成功。\n9 串行等价的交错执行 一个服务器管理对象a1, a2, \u0026hellip; an ，它为客户提供下面两种操作：read (i)返回对象ai的值。write(i, Value)将对象ai设置为值Value。\n事务T和U定义如下：\nT: x = read(j); y = read (i); write(j, 44); write(i, 33)\nU: x = read(k); write(i, 55); y = read (j); write(k, 66)\n请给出事务T和U的3个串行化等价的交错执行。\n答：我们给每一步分别表上序号：\nT: ①x = read(j); ②y = read (i); ③write(j, 44); ④write(i, 33)\nU: ⑤x = read(k); ⑥write(i, 55); ⑦y = read (j); ⑧write(k, 66)\n如果按照先T后U的顺序依次执行一个事务的话，我们发现①和⑤，②和⑦，④和⑥有写后写依赖；③和⑦有写后读依赖，①和③，②和④，②和⑥，⑤和⑧有读后写依赖，所以，我们要保证这些依赖的逻辑顺序，串行化等价的交错执行如下：\n①⑤②③④⑥⑦⑧\n①③⑤②④⑦⑧⑥\n①⑤③②⑧⑦④⑥\n10 乐观并发控制 考虑将乐观并发控制应用于下列事务T和U的情况：\n​ T: x = read(i); write(j, 44);\n​ U: write(i, 55); write(j, 66);\n如果事务T和U同时处于活动状态，试描述以下几种情况的结果如何：\n 服务器首先处理T的提交请求，使用向后验证方式。 服务器首先处理U的提交请求，使用向后验证方式。 服务器首先处理T的提交请求，使用向前验证方式。 服务器首先处理U的提交请求，使用向前验证方式。  对于上面的每种情况，描述事务T和U的操作顺序，注意写操作在验证通过之后才真正起作用。\n答：\n  服务器首先处理T的提交请求，使用向后验证方式。\nT先开始所以T的验证阶段无事发生，U进入验证阶段之后，U没有读集，所以没有冲突，可以顺利提交。\n  服务器首先处理U的提交请求，使用向后验证方式。\nT进入验证阶段，发现T的读集{i}与U的写集{i, j}有冲突，T事务被放弃。\n  服务器首先处理T的提交请求，使用向前验证方式。\nU进入验证阶段，发现U的写集{i,j}与T的读集{i}有冲突，所以推迟验证，等T的读集执行完毕之后再提交U。\n  服务器首先处理U的提交请求，使用向前验证方式。\nT进入验证阶段，发现U无读集，所以可以通过验证。\n  ","date":"2021-12-07T14:21:24+08:00","image":"https://img2.baidu.com/it/u=1163044150,1040852846\u0026fm=26\u0026fmt=auto","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A/","title":"【分布式与并行计算】分布式作业"},{"content":" 时间：2021-12-2 下午5点半\n方式：阿里会议视频面试\n岗位：研究型实习生-智能存储\n 面试过程 这次面试主要是在聊项目，聊兴趣啥的。没有什么算法或者知识点的提问。\n 先做一下自我介绍。 问我觉得自己在学习上的最大优势是什么。 问我大三实习的主要工作，这一部分我还是忽略的带过了。 讲了自己的大二时的比赛。 问了可以实习的时间。 目前在学校里都学了什么课程。 面试官问我大四在做什么，我回答自己保研后在课题组里做结项的事情。面试官比较好奇我自己都做了什么，我回答东西都是别人做的，我就是提供一些辅助性的工作。 然后询问我课题组的主要研究是什么，我回答云计算，docker，k8s之类的。面试官问那为啥你做的东西和云计算一点关系都没有，又去做了算法NLP，我回答这是老师安排的。 问我对docker，k8s了解多少。我回答仅仅会简单的使用。 后来面试官解释NLP之类的工作一般是达摩院内边来做，意思可能是这边还是偏重于开发。 问我对这个部门的工作了解吗，一面有介绍吗？我回答和存储，预训练模型好像有关。 面试官说你要自己想好自己要做什么方向，兴趣是最好的老师啥的。（我也想啊） 面试官问我对NLP了解多少，我说仅仅毕业设计和这个有关，知道预训练和蒸馏。 他说感觉你对这些方面都有一点了解，但又都不深，不知道你将来要做什么。我回答自己目前还在探索，打算先开始做在去感受自己想要什么。 感觉聊到这里，其实已经无话可聊了。  下面是反问环节\n 我说自己可能问的问题比较大不好回答：如果我有机会得到这个offer，那我需要弥补的差距在什么地方？ 面试官从两个方面建议，一个就是如果计算想搞算法的话，blabla我忘了。如果想做开发的话，可以学习一下云原生，云计算之类的知识。 面试官分析了一下之后，我回答自己的理解与看法 ，主要说了人工智能是一种拿来应用的技术，做工程的过程中拿来使用，是工程的一部分。不是所有人都要去研究模型，我们能拿来即用即可，还是要去提升工程上的开发能力。  面试总结 我其实看不出来这次的结果好坏，面试官也确实提到了欢迎来去做一些尝试，可以先进来，实习的过程中摸索自己的兴趣，感受一下各个方向是什么样的，但更像是一些 说法正确的客套话。\n目前还在等结果。（希望结果是好的吧）\n","date":"2021-12-04T18:31:16+08:00","image":"https://img2.baidu.com/it/u=3334504742,937198599\u0026fm=26\u0026fmt=auto","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%98%BF%E9%87%8C%E5%AE%9E%E4%B9%A0%E4%BA%8C%E9%9D%A2/","title":"【实习面试】阿里实习二面"},{"content":" 时间：2021-11-30 下午2点\n方式：阿里会议视频面试\n岗位：研究型实习生-智能存储\n 为什么会参加这次面试？ 因为想要亲身参与真实的科研活动或者一个真实企业内的做工程的过程，所以投递了这份简历，我也没有想到会给我安排面试。其实面试的时候我已经忘记自己投递的是哪个课题了，现在推测一下可能是大规模预训练模型的迁移啥的。\n因为面试的前一天参加了字节的面试，十分挫败，所以对面试可以说十分没有信心，加上收到的邮件里没有会议链接。无论面与不面，迟早要打电话告知，所以索性决定打电话推辞掉这个面试。打了电话，估计中午面试联系人可能在休息，所以过了一会才接到，面试联系人劝说我面试没有坏的影响，就当锻炼自己，只有好处没有坏处，不如一试。所以没有推脱，反正就一个小时，一个小时之后我的生活还会恢复原样。\n面试过程 面试的时候发现刚刚接电话的人应该就是面试官，面试官给我详细介绍了一下什么是研究型实习生以及它和其他的实习生的区别，以及对招聘的影响。然后就开始面试，先是自我介绍，还是介绍了本科，研究生的学校和专业，实习与比赛。问了我大概想做科研还是偏工程的方向，我回答工程，但后来想想应该回答我都行的其实。\n  面试了一道算法题：\n 将一个字符串按单词反转，但是对空间的开销有限制，最好是在原地址上直接修改，如果用栈，或者切割单词成数组之类的方式都是不符合空间开销要求的。\n 刚开始我想的是将单词先切出来，面试官发现我可能没有理解题目就又说了一下。\n我之后想说判断空格 然后做首尾交换，面试官提醒我单词的长度可能是不一致的，让我再想想。\n面试官说可以先说一下思路，再写代码。（说实话这点还挺赞的。）\n最后我想了想，说实话因为没有什么信心，我都想放弃算了。\n但偏偏还是想到了先把每个单词都先在局部反转，然后全局一起反转就不会产生大的空间开销。\n面试官说这个想法是对的，然后让我自己实现一下，就大概写了写代码。\n又问了我这个算法的时间复杂度是多少，我说是O(n)。\n  问平时怎么做测试，我回答用自己设计测试样例，然后print的方法和编辑器调试。\n他问我有么有用什么测试工具之类的，我说在课上学过UnitTest4，但实际上没用过。\n  知不知道多线程，pthread之类的。\n  之后聊了实习，实习时写的Json工具，日志接口开发。我都回答其实没有什么含金量。\n  然后是机器人大赛，路径规划算法用的是A* ,问我为什么用A* 而不是用机器人走迷宫的方式来操控机器人。和视觉算法的设计，以及OpenCV啥的。\n  最后聊了本科毕设，介绍了自己的毕设内容，问我觉得最有挑战的部分是啥。我回答是loss函数的设计，三段蒸馏，每段的不同的损失计算方法。\n  然后就是反问环节，我没问，确实不知道问啥。\n面试总结 大概就只记得这些了，好像忘记了很多其他的细节，但是面试完心情也比之前好了一些。\n阿里巴巴的这次面试给我最大的感受就是，面试官会确认自己的意思有没有准确传达给我，这个细节还挺令我感动的。面试官会把自己的问题或者很多要考虑的情况讲的很细致，确认我理解之后再让我思考并回答，这样其实对我这样不太熟悉面试的人十分友好的。\n面试的结果还不知道，但是无论好与坏，这次面试都给了我一些鼓舞，即便是结果不太好，我也不会气馁，不管怎么样都要继续努力。（希望结果是好的吧）\n后来接到了二面的电话，所以一面应该是过了。\n","date":"2021-12-01T21:49:57+08:00","image":"https://img2.baidu.com/it/u=3334504742,937198599\u0026fm=26\u0026fmt=auto","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%98%BF%E9%87%8C%E5%AE%9E%E4%B9%A0%E7%94%9F%E9%9D%A2%E8%AF%95/","title":"【实习面试】阿里实习生面试"},{"content":" 时间：2021-11-26 下午六点\n方式：飞书视频面试\n岗位：后端开发\n 为什么会面试蓝湖 心血来潮想要出去实习，在校友群内发了求助。24号中午，本科的同班同学涛神问我想不想试一试蓝湖，我想要一份实习来提升自己的代码水平，所以当然要抓住这次机会。\n所以中午抓紧时间写了一份简历交给了涛神帮忙内推。涛神问我要不要先准备一下，我说不了，早解决早轻松，可能是犯蠢了，也可能是太了解自己，我不是会好好规划复习的人，所以不如趁着热情直接上。\n面试的过程 面试官还是很温柔的，上来先让我做了一下自我介绍，我介绍了一下本科和硕士，以及一段实习经历，一段比赛经历。让说一下觉得最有有挑战性的工作？实习和比赛都很水，所以实话实说没啥亮点。\n算法题一道，写一下快排。当时代码没有跑起来，因为vscode好像更新了code runner的配置，所以没有跑起来，不过写的代码大概率全是bug，代码能力也是硬伤。\nclass Solution { public: int qsort(vector\u0026lt;int\u0026gt;\u0026amp; nums,int left,int right){ int l = left; int r = right; if(l\u0026gt;=r){ return l; } int randNum = rand()%(r-l+1)+l; int temp = nums[randNum]; nums[randNum] = nums[l]; nums[l] = temp; int pivot = nums[l]; while(l\u0026lt;r){ while(l\u0026lt;r\u0026amp;\u0026amp;nums[r]\u0026gt;pivot){ r--; } nums[l] = nums[r]; while(l\u0026lt;r\u0026amp;\u0026amp;nums[l]\u0026lt;=pivot){ l++; } nums[r] = nums[l]; } nums[l] = pivot; qsort(nums,left,l-1); qsort(nums,l+1,right); return l; } vector\u0026lt;int\u0026gt; sortArray(vector\u0026lt;int\u0026gt;\u0026amp; nums) { srand((unsigned)time(NULL)); qsort(nums,0,(int)nums.size()-1); return nums; } }; 然后问了一些基础问题：\n  Java和C++的区别有哪些\n  类的重写和重载\n  TCP通信的三次握手和四次挥手，为什么不能多一次或者少一次？\n  TCP长连接和短链接\n  TCP和UDP的区别\n  了解Cache吗？我答非所问，回答了操作系统的cache。\n  其实有可能是问http缓存\n  输入url地址浏览器的变化\n  问有么有用过数据库，对数据库了解多少，回答用过MySQL\n  为什么要用数据库？\n  简单介绍一下云计算是什么，为什么要用云计算\n  面试总结 面完就知道自己应该是凉了，果然12月1号收到了感谢信。其实还是老问题，自己的基础知识还是要在巩固一下可能要多看看面经，另外算法什么的还是要多加练习。\n后来涛神告诉我是HR觉得只有一个月多的实习时间太短了，不好安排。\n","date":"2021-12-01T20:56:54+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/image-20211201214555354.png","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E8%93%9D%E6%B9%96%E9%9D%A2%E8%AF%95%E5%87%89%E7%BB%8F/","title":"【实习面试】蓝湖面试凉经"},{"content":" 时间：2021-11-29下午三点\n方式：飞书视频面试\n岗位：后端开发实习生-业务中台职位\n 为什么会投递字节实习 ​\t因为中科院的研究所大多不让实习，研二想去实习是一定不行的。但是研一在雁栖湖，课题组在海淀，我研一基本不参与科研，所以想要趁没人管的时候出去实习。因为将来大概率是做软开，所以想找一份后端开发的工作。在校友群里问了一下，有学长给了一个内推的机会，所以就有了这次一面。\n面试过程 ​\t面试官上来让我做了自我介绍之后就让我写代码做题。\n 题目一：链表1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;6 转变成 链表1-\u0026gt;6-\u0026gt;2-\u0026gt;5-\u0026gt;3-\u0026gt;4\n大概意思就是将链表平分成两部分，前一部分顺序不变，后一部分反转后插空插入前一部分链表。\n ​\t说实话，自己的代码能力确实有点差，原本打算使用c++来写后来改成了Java，因为我想起Java里有LinkedList这个数据结构。理由有点荒诞哈，其实是我没有意识到要自己定义节点来实现链表，所以在想要写指针的时候就卡住了，完全不知道怎么给LinkedList的链表写指针。然后就尬住了，面试官无奈说那就换一道吧。\n 题目二：判断一个二叉树是否是平衡树\n ​\t题目不是很难，但是自己太久不写代码有些生疏了，写出来的程序不知道为什么没有编译通过。说实话，面试官让我自己实现一个单例来测试程序，其中爆出的各种错误，无一不揭示了我完全没有什么开发经验的事实，例如内部类放错了位置，static的编译问题，还有一个空指针异常。无疑是再次尬死了。\n​\t之后面试官让我讲一讲自己的项目经历，主要问了一下大三在华为的实习。可惜自己虽然实习了，但实际的工作很少，含金量也不高，面试官兴趣不大。\n​\t之后就是问我有没有什么想问他的，我已经不想说话了，就没问。\n​\t草草结束。\n总结 首先还是把这两道题的代码写写吧 题目一 /** * @author lizhixuan * @version 1.0 * @date 2021/12/1 15:50 */ public class ReverseList2 { public static class ListNode { int val; ListNode next; ListNode() {} ListNode(int val) { this.val = val; } ListNode(int val, ListNode next) { this.val = val; this.next = next; } } public static ListNode createInstance(int n){ ListNode head = new ListNode(1); ListNode current = head; for (int i = 2; i \u0026lt;= n; i++) { current.next = new ListNode(i); current = current.next; } return head; } public static void printList(ListNode head){ ListNode current = head; while(current.next!=null){ System.out.print(current.val); System.out.print(\u0026#34;-\u0026gt;\u0026#34;); current = current.next; } System.out.println(current.val); } public static ListNode reverse(ListNode head){ ListNode pre = null; ListNode current = head; ListNode next; while(current!=null){ next = current.next; current.next = pre; pre = current; current = next; } return pre; } public static ListNode mergeList(ListNode l1,ListNode l2){ ListNode head = l1; ListNode l1Next; ListNode l2Next; while(l1!=null\u0026amp;\u0026amp;l2!=null){ l1Next = l1.next; l2Next = l2.next; l1.next = l2; l2.next = l1Next; l1 = l1Next; l2 = l2Next; } return head; } public static ListNode reverseHalf(ListNode head){ ListNode fast = head; ListNode slow = head; while(fast != null){ if(fast.next == null){ break; } if(fast.next.next == null){ break; } fast = fast.next.next; slow = slow.next; } ListNode half = reverse(slow.next); slow.next = null; return mergeList(head,half); } public static void main(String[] args) { ListNode head = createInstance(7); printList(head); head = reverseHalf(head); printList(head); } } 题目二 import java.util.LinkedList; import java.util.Queue; /** * @author lizhixuan * @version 1.0 * @date 2021/12/1 17:24 */ public class BalanceCheck { public static class TreeNode{ int val; TreeNode left; TreeNode right; TreeNode(int val){this.val = val;} } public static TreeNode listToTree(String src){ src = src.substring(1,src.length()-1); String[] strList = src.split(\u0026#34;,\u0026#34;); TreeNode root ; TreeNode result = null; Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); for (int i =0 ; i\u0026lt; strList.length ; i++){ if (i == 0){ root = new TreeNode(Integer.parseInt(strList[i])); result = root; queue.add(root); } if (!queue.isEmpty()){ root = queue.poll(); }else { break; } if ( i+1 \u0026lt; strList.length \u0026amp;\u0026amp; !strList[i+1].equals( \u0026#34;null\u0026#34;)){ root.left = new TreeNode(Integer.parseInt(strList[i +1])); queue.add(root.left); } if ( i + 2 \u0026lt; strList.length \u0026amp;\u0026amp; !strList[i+2].equals( \u0026#34;null\u0026#34;)){ root.right = new TreeNode(Integer.parseInt(strList[i +2])); queue.add(root.right); } i = i +1; } return result; } public static int getHeight(TreeNode root){ if(root==null){ return 0; } return Math.max(getHeight(root.left),getHeight(root.right))+1; } public static boolean isBalanced(TreeNode root){ if(root==null){ return true; } int leftHeight = getHeight(root.left); int rightHeight = getHeight(root.right); return Math.abs(leftHeight-rightHeight)\u0026lt;=1 \u0026amp;\u0026amp; isBalanced(root.left) \u0026amp;\u0026amp; isBalanced(root.right); } public static void main(String[] args) { String tree = \u0026#34;[3,9,20,null,null,15,7]\u0026#34;; TreeNode root = listToTree(tree); System.out.println(isBalanced(root)); } } 一些知识点   java内部类：\n 成员内部类中不能存在任何static的变量和方法 成员内部类是依附于外围类的，所以只有先创建了外围类才能够创建内部类    java的static关键字\n static变量也称作静态变量，静态变量被所有的对象所共享，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化。 为什么说static块可以用来优化程序性能，是因为它的特性:只会在类加载的时候执行一次。 在C/C++中static是可以作用域局部变量的，但是在Java中切记：static是不允许用来修饰局部变量。    反思  自己还是缺乏代码经验，代码写不出来，面试官明显十分失望，失去耐心之后多次叹气捂脸，说实话压力还是挺大的，毕竟确实挺丢脸的。以后也要多写一点代码。 打算把自己的学习总结下来，于是也有了这篇博客，希望自己可以坚持写博客，说实话，面试完压力挺大，感觉自己就是一个无敌铁废物，写博客写出来感觉好一些了。  ","date":"2021-11-29T16:52:26+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/bytedance.jpg","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AD%97%E8%8A%82%E9%9D%A2%E8%AF%95%E5%87%89%E7%BB%8F/","title":"【实习面试】字节面试凉经"}]