[{"content":"HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees 摘要\n 生产多租户集群中存在严重的共享异常现象  在这种情况下，一些租户中的作业所经历的排队延迟比它们在拥有其分配的 GPU 的私有集群中所经历的排队延迟更为严重。 这是因为租户使用配额(GPU 的数量)来保留资源，而深度学习作业通常使用具有理想 GPU 关联的 GPU，而配额不能保证这一点   HiveD 是第一个安全共享 GPU 集群的框架，因此这种异常从设计上来说永远不会发生。  在 HiveD 中，每个租户通过虚拟专用集群(Virtual Private Cluster，VC)保留资源。 虚拟专用集群是根据集群中对应于不同级别的 GPU 亲和力的多阶储存细胞结构来定义的。 这种设计允许 HiveD 在每个 VC 中合并任何现有的调度程序，以实现各自的设计目标，同时安全地共享集群。   HiveD 开发了一种优雅的伙伴细胞分配算法，  通过有效地管理来自 VC 的细胞与物理集群中的细胞之间的动态结合来确保共享安全。 伙伴细胞分配的直接扩展可以进一步支持低优先级作业，以清理未使用的 GPU 资源，从而提高集群利用率。   结合实际部署和跟踪驱动的仿真，我们发现  (i)共享异常存在于三个最先进的深度学习调度器中，导致额外的队列延迟长达1000分钟 (ii) HiveD 可以合并这些调度器，消除所有调度器中的共享异常，实现关注点分离，使调度器能够专注于自己的调度目标，而不违反共享安全。    引言   常见的做法\n 每个租户使用由 GPU 数量和其他相关资源(如 CPU 和内存)组成的配额来保留资源。 异常情况: DDL等待 GPU的时间明显长于在一个大小等于租户配额的私有集群中进行。 这是因为当前的资源预留机制是基于配额的，即 GPU 的数量。  配额不能捕获训练作业的 GPU 亲和性需求: 例如，一个节点上的8-GPU 作业通常比8个节点上的运行速度快得多。 配额不能像承租者的私有集群那样保证承租者的 GPU 关联性。   因此，多 GPU 作业通常必须在队列中等待或以放松的亲和力运行，这两种情况都会导致更差的性能(更长的队列延迟或更慢的训练速度)。    HiveD，一个共享 GPU 集群的资源预留框架，用于深度学习训练，通过完全消除共享异常来保证共享安全。\n HiveD 不使用配额，而是为每个租户提供一个由一个新的抽象: cell 定义的虚拟私有集群(简称 VC)。  Cell 使用多级结构来捕获一组 GPU 可以满足的不同级别的亲和力。   这些细胞结构自然地在典型的 GPU 集群中形成一个层次结构;  例如，从单个 GPU，到连接到 PCIe 交换机的 GPU，到连接到 CPU 套接字的 GPU，到节点中的 GPU，再到机架中的 GPU，等等。      Cell\n HiveD 将物理 GPU 集群虚拟化为每个租户的 VC，其中 VC 保留了物理集群中必要的亲和结构。  这使得任何一个最先进的深度学习调度器都可以在 VC 定义的边界内做出调度决策，而不会影响其他 VC 的亲和力需求，从而确保共享安全。 通过这种方式，HiveD 实现了关注点分离：它专注于资源预留机制，并将其他资源分配目标留给 VC 调度程序(例如，集群利用率和作业完成时间)。      HiveD 开发了一种优雅高效的伙伴细胞分配算法，可以将细胞从 VC 绑定到物理集群。\n 伙伴细胞分配倡导动态细胞绑定，而非静态绑定，以获得灵活性。  它动态地创建和释放 VC 中的细胞到物理集群中的 GPU 的绑定，同时在不可预测的工作负载下提供经过验证的共享安全性。   此外，该算法可以自然地扩展到支持抢占的低优先级作业，以机会性地清除未使用的细胞，从而提高总体利用率。 结合起来，HiveD 实现了私有集群(保证独立于其他租户的细胞的可用性)和共享集群(在其他租户不使用资源时提高利用率和对更多资源的访问)的最佳性能    我们评估 HiveD 使用实验的96-GPU 实际集群和跟踪驱动的模拟。评估表明:\n 共享异常存在于所有评估的SOTA深度学习调度器中 HiveD 消除了所有共享异常，将过度队列延迟从1000分钟减少到零，同时保留了这些调度器的设计目标; HiveD 保证了无论集群负载如何，都可以共享安全，而基于配额的集群可以导致高负载下租户的7倍过度队列延迟。    总之，本文作出了以下贡献:\n 我们是第一个观察和识别生产多租户 GPU 集群中共享异常的深度学习训练。 我们定义了针对异常的共享安全的概念，并提出了一种新的资源抽象，即多级细胞，用于建模虚拟专用集群。 我们开发了一个优雅和高效的好友小区分配算法来管理具有被证明的共享安全性的小区，并支持低优先级作业。 我们在一个实际的集群上进行了广泛的评估，并通过仿真，在产品跟踪的驱动下，表明 HiveD 在共享安全性、队列延迟和利用率方面达到了设计目标。    背景与动机   当前管理多租户 GPU 集群的方法。\n 在大型企业中，大型 GPU 集群通常由多个业务团队共享，每个业务团队都是贡献其资源(预算或硬件)的租户。  租户共享 GPU 集群的方式类似于共享 CPU 集群：  每个租户都被分配了许多令牌作为其配额。 每个令牌对应于使用 GPU 和其他类型资源的权利。 配额表示期望承租者能够“至少”访问其贡献的资源份额     为了提高集群中的培训速度  用户通常为深度学习任务指定 GPU 亲和力需求[52,86]。  例如，一个64-GPU 的作业通常需要以8 × 8的亲和力运行，即在8个节点上运行作业，每个节点使用8个 GPU，而不是64 × 1，即每个节点使用1个 GPU。   给定关联需求，资源管理器将以保证(硬)或最佳努力(软)的方式满足它们。  如果没有满足作业关联性要求的放置，则作业将在队列中等待亲和力要求，或如果要求是软性的(例如，64 × 1相对于8 × 8) ，则会以放松亲和力的方式安排。        共享异常\n 在生产 GPU 集群中，我们观察到用户投诉的异常情况:  一个租户被分配了64个 GPU 的配额，但是报告说它不能运行一个8 × 8深度学习作业。   原因：  这种异常现象之所以出现，是因为承租者所分配的亲和力已经被其他承租者的工作分割   结果  即使租户有足够的 GPU 配额，64-GPU 作业也必须在队列中等待，或者以放松关联的方式执行性能下降的作业。 向承租者保证它至少可以访问其所占的资源的承诺被打破了。   分析  如果我们将租户比作程序，那么共享异常看起来类似于内存管理中的外部碎片。 然而，重要的区别在于，在共享的 GPU 集群中，租户希望他们的资源共享得到保证 显然，配额只能保留资源的数量，而不能保留资源的亲和力。      减少共享异常的一种方法是设计一种调度策略，以尽量减少全局资源分散。\n 这使得深度学习调度器的设计更加复杂，它已经需要管理复杂的多目标优化。  例如，最小化全局碎片化可能会由于工作间干扰的增加而降低工作绩效[86]。   因此，我们建议将共享异常的关注与其他资源分配目标分开[47]。  我们没有开发一个能够实现所有可能目标的复杂调度程序，而是设计了 HiveD，一个侧重于消除共享异常的资源预留框架 并提供了一个干净的界面，以纳入任何最先进的深度学习调度程序，以解决诸如集群利用率[86]、作业完成时间[41,66]和公平性[29,60]等问题      HiveD设计 系统总览  HiveD 提出保证消除共享异常，作为共享 GPU 集群的先决条件。  具体来说，如果可以在私有集群中满足具有亲和需求的 GPU 请求序列，那么应该在相应的虚拟私有集群和共享物理集群中满足这些请求序列。    \rimage-20230529130308520\r\n  图2说明了整个系统架构。\n HiveD 对 GPU 资源的抽象分为两层  虚拟专用集群(VCs)层 物理集群层   HiveD为每个租户提供一个 VC。  每个 VC 都预先分配了一组细胞格，这是一种新的资源抽象不仅捕获配额，而且捕获 GPU 的亲和结构(图中每个细胞内的数字显示细胞的亲和 GPU 的数目)。 分配给 VC 的细胞形成一个 VC 视图，其 GPU 亲和结构与相应的私有集群相同。  任何第三方调度程序都可以合并到 VC 视图中，以实现一定的资源分配目标。 此外，HiveD 保证任何调度决策都被限制在 VC 视图定义的边界内，就像在它的私有集群上发生一样，从而保证了共享的安全性。        VC 中的细胞是逻辑的\n 当作业在逻辑细胞中使用 GPU 时，  例如，图2中 Tenant A 的 VC 视图中的4-GPU 细胞中的一个 GPU，逻辑细胞将绑定到从物理集群分配的物理细胞，如图2底部所示。 如果没有使用任何 GPU，则逻辑细胞将从物理集群解除绑定。   为了提高利用率，可抢占的低优先级作业可以机会性地清除空闲的 GPU。  这种动态绑定比静态绑定更加灵活:  动态绑定可以避免硬件故障的物理细胞 它可以避免低优先级作业使用的细胞以减少抢占 它还可以包装细胞以最小化 GPU 亲和力的碎片        为了实现这一点，HiveD 采用伙伴细胞分配(一种高效而优雅的算法)来处理动态绑定。\n 动态绑定的一个关键挑战是保证响应动态工作负载的安全性，即作业不可预测地到达并请求不同级别的细胞。 证明了伙伴细胞分配算法能够保证共享的安全性:  在 VC 中任何合法的细胞请求都能够得到满足。   该算法还可以支持低优先级作业。  图2显示了一个可能的细胞分配，其中物理集群中的细胞绑定到两个 VC 中定义的细胞，并且还绑定到一个低优先级作业。      细胞结构的虚拟私有集群  为了建模一个(私有的)图形处理器集群，HiveD 定义了一个多阶储存细胞结构的层次结构。  一个细胞在某个级别上是相应的关联 GPU 集合，它们具有互连拓扑。 然后将每个虚拟私有集群(VC)定义为每个级别上的细胞数，并以相应的私有集群为模型。    \rimage-20230529130450884\r\n  图3显示了一个示例\n 其中有4个级别的细胞结构:  1：GPU 2：PCIe 开关 3：CPU 套接字 4：节点级别   集群有一个机架，由四个8-GPU 节点组成，由三个租户 A、 B 和 C 共享。  图3中的表格总结了每个租户的 VC 的细胞格分配。  租户 A 和 B 的 VC 都保留一个第3级细胞(同一 CPU 套接字下的4个 GPU) ，一个第2级细胞(同一 CPU 套接字下的2个 GPU PCIe 开关)和一个1级细胞(单个 GPU)。 租户 C 是一个较大的租户，它保留两个4级细胞(节点级)和一个2级细胞。   给定图3中定义的 VC 视图，HiveD 可以采用第三方调度程序来处理视图。  从第三方调度器的角度来看，VC 视图与由不同大小(即不同细胞级别)的节点组成的私有集群没有什么不同。  例如，调度程序可以将租户 C 视为一个私有集群，其中包含两个8-GPU 节点和一个2-GPU 节点，尽管2-GPU 节点实际上是一个2级细胞。   请注意，第三方调度程序可以使用分配细胞中的任何 GPU。  例如，它可以将两个2-GPU 作业调度到一个4-GPU (level-3)细胞: 细胞是 VC 和物理集群中资源预留的粒度，但不一定是第三方调度程序的作业调度粒度。          在细胞格层次结构中，一个级别 k 细胞格 c 由一组级别(k-1)细胞格 S 组成。\n S 中的细胞格称为伙伴细胞格; 伙伴细胞格可以合并到下一个更高级别的细胞格中。 我们假设细胞格具有分层的统一可组合性  (i)所有级别 k 细胞格在满足租户对级别 k 细胞格的请求方面是等价的， 并且(ii)所有级别 k 细胞格可以分割成相同数量的级别(k-1)细胞格      异质性。\n 一个异构集群可以划分为多个具有层次一致可组合性的同构集群。 这在实践中是合乎逻辑的，因为一个产品集群通常由足够大相同的子集群组成  为了获得更好的性能，用户通常使用同质的 GPU，并指定所需的 GPU/拓扑类型      初始细胞分配。\n 集群提供者必须计算出在每个级别上分配给每个租户的 VC 的细胞数。  如果 VC 分配能够容纳分配给所有 VC 的所有细胞，那么 VC 分配在物理集群中是可行的; 也就是说，存在从每个 VC 中的逻辑细胞到物理集群中的物理细胞的一对一映射。   最初的细胞VC 的分配取决于预算、业务优先级和工作量等因素，因此它是在 HiveD 之外处理的  集群可能比分配的细胞节省更多的物理资源来处理硬件故障。      伙伴细胞分配算法   HiveD 管理 VC 中的逻辑细胞和物理集群中的物理细胞之间的动态绑定，并处理分配和释放细胞的请求。\n 这是由好友细胞分配算法完成的。 该算法为每个 VC 维护以下信息:  (i)每个分配的逻辑细胞对应的物理细胞(即绑定) ; (ii)每个细胞级别 k 的全局空闲列表，以跟踪该级别的所有未分配的物理细胞。  该算法始终将可用细胞保持在尽可能高的级别:  例如，如果级别-(k-1)的所有伙伴细胞都可用于级别-k 的细胞，则只记录级别-k 的细胞。   该算法旨在保持尽可能多的高级细胞可用。算法1显示了算法的伪代码。        为了在 VC 中分配一个 level-k 细胞，算法从 level-k 开始，如果需要则向上分配:\n 它首先检查一个空闲 level-k 细胞是否可用，如果可用则分配一个。 如果没有，算法将一级一级地向上移动，直到有一个空闲的级别 -l 细胞可用，其中 l \u0026gt; k。 然后，该算法将递归地将一个空闲的1级细胞划分为多个低级细胞，直到有一个 k 级细胞可用。 每次分割都会在下一个较低级别产生一组好友细胞格，这些好友细胞格将被添加到该较低级别的空闲列表中。 其中一个新的低能级细胞再次分裂，直到产生游离的 k 能级细胞。    细胞释放过程也是以自底向上的方式进行的。\n 当释放一个 level-k 细胞格 c 时，算法将 c 添加到检查 C 的好友细胞的状态。 如果 c 的好友细胞格都是空闲的，那么算法将把 c 和它的好友细胞格合并到一个级别为(k + 1)的细胞格中。 合并过程在上升级别时递归地继续，直到没有细胞格可以合并为止。 这样，好友细胞分配算法减少了 GPU 碎片，并创建了调度需要更高级别细胞的作业的机会。    在处理分配请求之前，算法确保请求是合法的，因为它在这个细胞级别的 VC 的分配配额内。HiveD 将细胞格分配存储在一个表 r 中，其中一个租户 t 为 level-k 细胞格预先分配的编号存储在 rt，k 中。伙伴小区分配算法保证在一个可行的初始 VC 分配下满足所有合法小区的请求，这在定理1中有正式描述。\n  定理1。在层次一致可组合的条件下，如果原来的 VC 分配是可行的，那么伙伴细胞分配算法可以满足任何合法的细胞分配。\n\rimage-20230529130905146\r\n实现 评估 ","date":"2023-05-29T10:37:17+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202305291403418.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0hived%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】HiveD论文阅读笔记"},{"content":"A reading list for machine learning systems Frameworks  [VLDB \u0026lsquo;20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training [NeurIPS \u0026lsquo;19] PyTorch: An Imperative Style, High-Performance Deep Learning Library [OSDI \u0026lsquo;18] Ray: A Distributed Framework for Emerging AI Applications [OSDI \u0026lsquo;16] TensorFlow: A System for Large-Scale Machine Learning  Parallelism \u0026amp; Distributed Systems  [NSDI \u0026lsquo;23] ARK: GPU-driven Code Execution for Distributed Deep Learning [OSDI \u0026lsquo;22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization [EuroSys \u0026lsquo;22] Varuna: Scalable, Low-cost Training of Massive Deep Learning Models [SC \u0026lsquo;21\u0026rsquo;] Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines [ICML \u0026lsquo;21] PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models [OSDI \u0026lsquo;20] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters [ATC \u0026lsquo;20] HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism [NeurIPS \u0026lsquo;19] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [SOSP \u0026lsquo;19] A Generic Communication Scheduler for Distributed DNN Training Acceleration [SOSP \u0026lsquo;19] PipeDream: Generalized Pipeline Parallelism for DNN Training [EuroSys \u0026lsquo;19] Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks [arXiv \u0026lsquo;18] Horovod: fast and easy distributed deep learning in TensorFlow [ATC \u0026lsquo;17] Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters [EuroSys \u0026lsquo;16] STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning [EuroSys \u0026lsquo;16] GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter Server [OSDI \u0026lsquo;14] Scaling Distributed Machine Learning with the Parameter Server [NIPS \u0026lsquo;12] Large Scale Distributed Deep Networks  GPU Cluster Management   [NSDI'23] Transparent GPU Sharing in Container Clouds for Deep Learning Workloads\n 容器广泛用于数据中心中的资源管理。  支持容器云中的深度学习(DL)训练的一个常见实践是静态地将 GPU 完全绑定到容器上。 由于生产中 DL 作业的资源需求多种多样，大量的 GPU 未得到充分利用。 因此，GPU 集群具有较低的 GPU 利用率，由于排队，导致作业完成时间较长。   我们提出 TGS (透明 GPU 共享) ，一个系统，提供透明的 GPU 共享到集装箱云中的 DL 训练。  与最近 GPU 共享的应用层解决方案形成鲜明对比的是，TGS 在容器下的 OS 层运行。 透明性允许用户使用任何软件来开发模型并在其容器中运行作业。 TGS 利用自适应速率控制和透明统一内存，同时实现高 GPU 利用率和性能隔离。 它确保生产作业不会受到共享 GPU 上机会作业的严重影响。 我们已经建立了 TGS，并将他与docker和kubernetes整合。实验结果表明:  TGS 对生产作业的吞吐量影响不大 TGS 为机会作业提供了与最先进的应用层解决方案 AntMan 相似的吞吐量，并且与现有的 OS 层解决方案 MPS 相比提高了15倍的吞吐量。        [ASPLOS \u0026lsquo;23] Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs\n 虽然最近的深度学习工作负载调度器表现出很好的性能，但在实际应用中很难部署它们，是由于  缺乏灵活的入侵方式、 过高的集成和维护成本 有限的可伸缩性 不透明的决策过程   Lucid: 基于可解释模型的非侵入式深度学习工作负载调度器 。  它由三个创新模块组成。  首先，为了有效地收集作业度量和及时调试作业反馈，引入了一个二维优化剖析器。 其次，Lucid 利用一种惰性包装策略来规避干扰。 第三，Lucid 基于估计的作业优先级值和共享分数来编排资源，以实现有效的调度。   此外，Lucid 通过设计良好的系统优化器促进模型性能维护和系统透明调整。 我们的评估表明，与最先进的抢占式调度器 Tiresias 相比，Lucid 将平均作业完成时间减少了1.3倍。此外，它为实际部署提供了明确的系统解释和优秀的可伸缩性。      [EuroSys \u0026lsquo;23] Lyra: Elastic Scheduling for Deep Learning Clusters\n 训练和推理中的问题:  当流量负载较低时，推理集群的 GPU 利用率较低 由于缺乏资源，训练作业往往需要较长的排队时间   我们引入了 Lyra ，一个新的集群调度器来解决这些问题。  Lyra 引入了容量贷款，将空闲推理 GPU 服务器贷款给训练工作。它进一步利用弹性扩展来扩展培训作业的 GPU 分配，以更好地利用借出的资源。 容量借贷和弹性扩展为集群管理带来了新的挑战。  当需要返回借出的服务器时，我们需要最小化作业抢占的数量 当更多的 GPU 可用时，我们需要将它们分配到弹性作业，并最小化作业完成时间(JCT)   Lyra 使用基于原则的启发式方法来解决这些组合问题。  它引入了服务器抢占成本的概念，并在服务器回收期间使用贪婪的方法降低这一成本。 它进一步依赖于为弹性工作的每个额外工人定义的 JCT 缩减值，以多选择背包问题解决调度问题。   在64-GPU 测试平台上的原型实现和超过50,000个生产作业的15天跟踪的大规模模拟表明  Aryl 在平均排队时间和 JCT 方面带来了1.53 x 和1.50 x 的减少 集群调度器提高了高达26.9% 的集群使用率        [OSDI \u0026lsquo;22] Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters\n 此文作者发现，  尽管 GPU 是 DNN 训练所需要的最主要的资源， 但是 CPU 和 MEM 的分配策略同样也会显著影响到集群的资源利用效率和训练任务的吞吐量。   Synergy  针对不同类型的 DNN，深入分析了 CPU 和 MEM 的不同组合对其吞吐量的影响 然后将最优的组合方案应用到了调度策略中      [NSDI \u0026lsquo;22] MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters\n 随着机器学习 (ML) 技术的持续进步和最近大量数据集的可用性，科技公司正在部署大型 ML 即服务 (MLaaS) 云，通常使用异构 GPU 来提供大量 ML 应用程序。然而，在异构 GPU 集群中运行不同的 ML 工作负载会带来许多挑战。 在本文中  我们对从阿里巴巴拥有 6,000 多个 GPU 的生产 MLaaS 集群中收集的为期两个月的工作负载跟踪进行了表征研究。 我们解释了集群调度所面临的挑战  GPU 利用率低 排队延迟长 需要高端 GPU 调度要求苛刻的任务难以调度 异构机器之间的负载不平衡 潜在的CPU瓶颈   我们描述了我们当前的解决方案，并呼吁进一步调查仍未解决的挑战。 我们已经发布了公共访问的跟踪，就工作负载和集群规模而言，这是最全面的。      [arXiv \u0026lsquo;22] Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads\n Singularity  微软的全球分布式调度服务，高效和可靠地执行深度学习训练和推理工作负载 核心：是一个新颖的、工作负载感知的调度器，它可以透明地抢占和弹性地扩展深度学习工作负载  以提高利用率 而且不会影响它们在全球 AI 加速器(GPU/FPGA)中的正确性或性能   所有作业都是可抢占的、可迁移的，并且在默认可以动态调整大小(弹性) : 一个活动的作业可以被动态和透明地  被抢占和被迁移到不同的节点集、集群、数据中心或者区域，并且可以从执行被抢占的地方精确地恢复 在给定类型的一组不同的加速器上调整大小   机制透明：不要求用户对代码进行任何更改，也不要求使用任何可能限制灵活性的自定义库。 可靠：利用Singularity可以获得效率和可靠性增益，而对稳态性能的影响可以忽略不计。 我们的设计方法是对DNN 网络架构不感知的，并且可以处理各种并行策略(数据/流水线/模型并行)      [OSDI \u0026lsquo;21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning\n Pollux通过在每个作业层面和整个集群层面自适应地共同优化相互依赖的因素，改善深度学习（DL）集群的调度性能。  在训练过程中监测每个作业的状态，Pollux建模了他们的有效吞吐量随着添加或删除资源而发生的变化 Pollux动态地重新分配资源，以提高整个集群的好产量，同时尊重公平性，不断优化每个DL作业，以更好地利用这些资源。 在真实的DL作业和跟踪驱动的模拟实验  Pollux相对于最先进的DL调度器将平均作业完成时间减少了37-50% Pollux促进了竞争资源的DL作业之间的公平性        [NSDI \u0026lsquo;21] Elastic Resource Sharing for Distributed Deep Learning\n  [OSDI \u0026lsquo;20] Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads\n 背景  加速器，如 GPU、 TPU、 FPGA 和定制 ASIC 表现出跨模型架构的异构性能行为。 现有的针对加速器集群的调度器(用于在许多用户之间仲裁这些昂贵的培训资源)已经展示了如何针对各种多任务、多用户目标(如公平性和完成时间)进行优化。不幸的是，现有的调度程序基本上不考虑性能异构性。   异构感知调度器 Gavel  它系统地推广了大量现有的调度策略。Gavel 将这些策略表示为优化问题，然后使用我们称为有效吞吐量的抽象将这些问题系统地转换为能够识别异构性的版本。 然后，Gavel 使用一种基于循环的调度机制，以确保在给定目标调度策略的情况下，作业能够得到理想的分配。 Gavel 的异质性感知策略允许异质集群维持更高的输入负载，并且与异质性不可知策略相比，最终目标如完成时间和平均工作完成时间分别提高了1.4倍和3.5倍。      [OSDI \u0026lsquo;20] AntMan: Dynamic Scaling on GPU Clusters for Deep Learning\n 如何在大规模GPU集群上有效调度深度学习工作， 对于工作性能，系统吞吐量和硬件利用率至关重要。  随着深度学习的工作量变得更加复杂，它变得越来越具有挑战性。   本文将介绍AntMan， 这是一种深入学习的基础设施，该基础架构共同设计了集群调度程序，并已在阿里巴巴部署在生产中，以管理数以万计的每日深度学习工作。  AntMan适应深度学习训练的波动资源需求。因此，它利用备用GPU资源在共享GPU上共同执行多个作业。 AntMan利用深度学习训练的独特特征，在深度学习框架内为显存和计算资源引入动态缩放机制。这允许job之间的细粒度协调并防止工作干扰。 评估表明，AntMan在我们的多租户集群中不损害公平性的情况下，整体将显存利用率提高了42％，计算资源利用率提高了34％，为有效利用大规模的GPU提出了新方法。      [NSDI \u0026lsquo;20] Themis: Fair and Efficient GPU Cluster Scheduling\n ML 训练工作负载通常是需要批量调度的长时间运行的作业，并且它们的性能对任务的相对位置很敏感 Themis 是一种ML 训练工作负载的新调度框架。  该机制是一种以完成时间进行公平调度的GPU分配策略(在一个共享的集群中有N个应用程序的运行时间与在一个1/N集群中单独运行的运行时间的比率)。 Themis 的目标是在有效利用集群 GPU 的同时，保证各工作负载调度的公平性并最大限度地减少所有 ML 应用程序的完成时间。 Themis 包含两级调度架构  其中 ML 作业在调度引擎中对可用资源进行投标，这样可以使调度引擎可以捕获布局敏感性并确保效率。 调度引擎的分配是通过在短期内将GPU 分配给中标者以换取更多效率，但在长期内仍然确保所有工作完成时间的公平性。   Themis 在 Apache YARN 3.2.0 上实现，并通过重放大型企业跟踪中的工作负载进行评估，公平性提高了 2.25 倍以上，集群效率提高了 ~5% 到 ~250%。      [EuroSys \u0026lsquo;20] Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning\n Gandiva-fair  效率与公平的平衡  效率：Gandiva-fair 提供用户之间的性能隔离，使多个用户可以共享一个集群，从而最大限度地提高集群效率 公平：在活跃用户之间公平分配集群范围 GPU 时间   集群异质性  背景：新一代用户面临更高的需求，老一代 GPU 的利用率很低，从而降低了集群效率 解决方案： Gandiva-fair 描述了来自较新 GPU 的各种作业的可变边际效用，并通过一种新颖的资源交易机制透明地激励用户使用较旧的 GPU 效果：该机制不影响任何用户的公平性保证的情况下最大限度地提高集群效率        [NSDI \u0026lsquo;19] Tiresias: A GPU Cluster Manager for Distributed Deep Learning\n 深度学习 (DL) 训练作业给现有的集群管理器带来了一些独特的挑战，例如  不可预测的训练时间 全有或全无的执行模型 GPU 共享的不灵活性   我们对生产中的大型 GPU 集群的分析表明，现有的大数据调度程序会导致  较长的排队延迟 较低的整体性能   我们介绍了 Tiresias  这是一个为分布式 DL 训练作业量身定制的 GPU 集群管理器，它可以有效地安排和放置 DL 作业以减少它们的作业完成时间 (JCT)。 鉴于 DL 作业的执行时间通常是不可预测的，我们提出了两种调度算法——  离散化二维Gittins索引：基于部分信息 离散化二维 LAS： 与信息无关，旨在最小化平均 JCT   此外，我们描述了何时可以放宽合并放置约束，并提出了一种放置算法来利用这些观察结果而无需任何用户输入。   在具有 60 个 P100 GPU 的密歇根 ConFlux 集群上进行的实验和大规模跟踪驱动模拟表明，  与生产中使用的基于 Apache YARN 的资源管理器相比，Tiresias 将平均 JCT 提高了 5.5 倍。 更重要的是，Tiresias 的性能与假设完美知识的解决方案的性能相当。      [ATC \u0026lsquo;19] Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads\n  调度框架旨在提供如下特性\n 高效率 资源隔离 用户间公平共享    然而，基于深度神经网络 (DNN) 的工作负载主要在 GPU 上训练，与传统的大数据有两个显着差异数据分析工作负载\n 首先，从集群利用率的角度来看，GPU 代表了无法在用户之间以细粒度共享的单一资源 其次，从工作负载的角度来看，深度学习框架需要gang schedule，这降低了调度的灵活性，并使作业本身在运行时对故障没有弹性    在本文中\n 我们展示了来自 Microsoft 多租户 GPU 集群的长达两个月的跟踪的详细工作负载特征 通过将调度程序日志与来自各个作业的日志相关联，我们研究了影响多租户集群上 DNN 训练工作负载的集群利用率的三个不同问题：  (1) 队列调度和位置约束的影响 (2) 位置的影响关于 GPU 利用率 (3) 训练期间的失败。   根据我们运行大规模操作的经验，我们提供了与用于 DNN 训练工作负载的下一代集群调度器相关的设计指南      [OSDI \u0026lsquo;18] Gandiva: Introspective cluster scheduling for deep learning\n Gandiva: 一个集群调度框架，使用特定领域知识，优化了GPU集群训练深度学习模型的延迟与效率   深度学习job的特征\n 1）反馈驱动的探索：  一个用户经常运行一组作业(或 a multi-job)来获得特定任务的最佳结果 并使用关于准确性的早期反馈来动态优先考虑或杀死一个作业子集 同步发生的多个作业的早期反馈是至关重要的   2）深度学习工作在资源使用方面的异构，这使得它很难实现最适合的先验。 3）作业内可预测性：因为作业会重复执行叫做mini-batch的迭代  Gandiva利用这个特征解决了1）2）两个问题 利用可预测性对GPU进行多个job间进行时分复用， 这提供了低延迟 这种预测性还可以用于内省job性能并动态迁移到最合适的GPU上，提高了集群效率      我们通过一个原型实现和微基准测试表明\n Gandiva 可以在深度学习过程加快超参数搜索一个数量级 并通过透明迁移和job时分实现更好的利用，使job与资源的更好地匹配。 在一个运行在180-GPU 集群中的实际工作负载中，Gandiva 将集群的总利用率提高了26% 这为深度学习提供了一种管理大型 GPU 集群的新方法。        Memory Management for Machine Learning  [ASPLOS \u0026lsquo;23] DeepUM: Tensor Migration and Prefetching in Unified Memory [ATC \u0026lsquo;22] Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory [MobiSys \u0026lsquo;22] Memory-efficient DNN Training on Mobile Devices [HPCA \u0026lsquo;22] Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems [ASPLOS \u0026lsquo;20] Capuchin: Tensor-based GPU Memory Management for Deep Learning [ASPLOS \u0026lsquo;20] SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping [ISCA \u0026lsquo;19] Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory [ISCA \u0026lsquo;18] Gist: Efficient Data Encoding for Deep Neural Network Training [PPoPP \u0026lsquo;18] SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks [MICRO \u0026lsquo;16] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design  Scheduling \u0026amp; Resource Management  [ASPLOS \u0026lsquo;23] ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning [arXiv \u0026lsquo;22] EasyScale: Accuracy-consistent Elastic Training for Deep Learning  分布式同步GPU训练通常被用于深度学习。  使用固定GPU的资源约束  使得大规模的深度学习训练工作受到影响 降低了集群的利用率   纳入资源弹性  往往会引入模型精度的非确定性\u0026lt;\u0026mdash;\u0026ndash;缺乏隔离能力     本文介绍EasyScale，  这是一个弹性框架  可以在异构GPU上扩展分布式训练 同时产生确定性的深度学习模型   实现了弹性的精度一致的模型训练。  EasyScale严格遵循数据并行训练流程 仔细追踪与精度相关的因素 有效利用深度学习特性进行上下文切换   为了使异构GPU的计算能力达到饱和  EasyScale根据我们的作业内和作业间调度策略动态地分配工人 最大限度地减少GPU的空闲时间 并相应地提高综合作业的吞吐量。   实验  部署在CompanyA的一个在线服务集群中 EasyScale为弹性深度学习训练作业提供动力，使其适时地利用空闲的GPU 在不违反SLA的情况下将集群的整体利用率提高了62.1%       [MLSys \u0026lsquo;22] VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware [SIGCOMM \u0026lsquo;22] Multi-resource interleaving for deep learning training [EuroSys \u0026lsquo;22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning [ATC \u0026lsquo;21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training [NeurIPS \u0026lsquo;20] Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning [OSDI\u0026rsquo; 20] KungFu: Making Training in Distributed Machine Learning Adaptive [OSDI \u0026lsquo;20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications [MLSys \u0026lsquo;20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications  GPU 利用率不足  现代 GPU 本身不支持细粒度共享原语 分时和抢占是代价昂贵的 DL 应用程序不能完全使用 GPU 的资源， 且这些资源无法被共享   Salus  支持两个 GPU 共享原语，以实现多个 DL 应用程序之间的细粒度 GPU 共享， 原语是：  快速作业切换 内存共享   Salus 实现了一个高效的、统一的执行服务，将 GPU 共享给不同的 DL 应用程序，并通过执行迭代调度和解决相关的内存管理问题来实现细粒度共享。 我们展示了这些原语可以用来实现灵活的共享策略，比如公平性、优先级排序和为各种用例打包。 将 Salus 与 TensorFlow 相结合，对流行的 DL 工作进行评估，结果表明 Salus 可以提高 DL 培训工作的平均完成时间3.19 × ，超参数调整的 GPU 利用率2.38 × ，DL 推理应用的 GPU 利用率42 × 以上，不共享 GPU 和7 × 以上 NVIDIA MPS，且开销较小。     [SOSP \u0026lsquo;19] Generic Communication Scheduler for Distributed DNN Training Acceleration [EuroSys \u0026lsquo;18] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters  Optimus  一个为深度学习集群定制的作业调度器，它基于在线资源性能模型使作业训练时间最小化。 Optimus使用在线拟合来预测训练期间的模型收敛，并设置性能模型来准确估计训练速度，作为每个作业分配资源的函数。 基于这些模型，我们设计了一个简单而有效的方法，用于动态分配资源和放置深度学习任务，以尽量减少作业完成时间。 实验  我们在Kubernetes（一个用于容器编排的集群管理器）之上实现了Optimus 并在一个有7个CPU服务器和6个GPU服务器的深度学习集群上进行了实验 使用MXNet框架运行9个训练作业 结果显示，Optimus在作业完成时间和makespan方面分别比有代表性的集群调度器高出约139%和63%       [HPCA \u0026lsquo;18] Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective  Serving Systems (\u0026amp; inference acceleration)  [EuroSys \u0026lsquo;23] Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access [MICRO \u0026lsquo;22] DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation [ATC \u0026lsquo;22] Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing [OSDI \u0026lsquo;22] Orca: A Distributed Serving System for Transformer-Based Language Generation Tasks [OSDI \u0026lsquo;22] Achieving μs-scale Preemption for Concurrent GPU-accelerated DNN Inferences [ATC \u0026lsquo;21] INFaaS: Automated Model-less Inference Serving [OSDI \u0026lsquo;20] Serving DNNs like Clockwork: Performance Predictability from the Bottom Up [ISCA \u0026lsquo;20] MLPerf Inference Benchmark [SOSP \u0026lsquo;19] Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis [ISCA \u0026lsquo;19] MnnFast: a fast and scalable system architecture for memory-augmented neural networks [EuroSys \u0026lsquo;19] μLayer: Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization [EuroSys \u0026lsquo;19] GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks [OSDI \u0026lsquo;18] Pretzel: Opening the Black Box of Machine Learning Prediction Serving Systems [NSDI \u0026lsquo;17] Clipper: A Low-Latency Online Prediction Serving System  Deep Learning Compiler  [PLDI \u0026lsquo;21] DeepCuts: A Deep Learning Optimization Framework for Versatile GPU Workloads [OSDI \u0026lsquo;18] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning  Very Large Models  [ASPLOS \u0026lsquo;23] Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression [arxiv \u0026lsquo;21] ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning [ATC \u0026lsquo;21] ZeRO-Offload: Democratizing Billion-Scale Model Training [FAST \u0026lsquo;21] Behemoth: A Flash-centric Training Accelerator for Extreme-scale DNNs  Deep Learning Recommendation Models  [OSDI \u0026lsquo;22] FAERY: An FPGA-accelerated Embedding-based Retrieval System [OSDI \u0026lsquo;22] Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update [EuroSys \u0026lsquo;22] Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations [ASPLOS \u0026lsquo;22] RecShard: statistical feature-based memory optimization for industry-scale neural recommendation [HPCA \u0026lsquo;22] Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation [MLSys \u0026lsquo;21] TT-Rec: Tensor Train Compression for Deep Learning Recommendation Model Embeddings [HPCA \u0026lsquo;21] Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training [HPCA \u0026lsquo;21] Understanding Training Efficiency of Deep Learning Recommendation Models at Scale [ISCA \u0026lsquo;20] DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference [HPCA \u0026lsquo;20] The Architectural Implications of Facebook’s DNN-based Personalized Recommendation [MICRO \u0026lsquo;19] TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning  Hardware Support for ML  [ISCA \u0026lsquo;18] A Configurable Cloud-Scale DNN Processor for Real-Time AI [ISCA \u0026lsquo;17] In-Datacenter Performance Analysis of a Tensor Processing Unit  ML at Mobile \u0026amp; Embedded Systems  [MobiCom \u0026lsquo;20] SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud [RTSS \u0026lsquo;19] Pipelined Data-Parallel CPU/GPU Scheduling for Multi-DNN Real-Time Inference [ASPLOS \u0026lsquo;17] Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge  ML Techniques for Systems  [ICML \u0026lsquo;20] An Imitation Learning Approach for Cache Replacement [ICML \u0026lsquo;18] Learning Memory Access Patterns  ","date":"2023-05-21T21:38:00+08:00","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0mlsys-paper-list/","title":"【论文笔记】MLsys Paper List"},{"content":"Shockwave: Fair and Efficient Cluster Scheduling for Dynamic Adaptation in Machine Learning 摘要\n  动态自适应已经成为加速分布式机器学习(ML)训练的关键技术\n 最近的研究表明，动态调整模型结构(例如，彩票假设[16])或超参数(例如，批量大小[1])可以显着加快训练而不牺牲准确性 然而，现有的机器学习集群调度器并不是为处理动态适应而设计的    我们发现，当训练吞吐量在动态适应下随时间变化时，现有的方案不能提供公平性，降低系统效率\n  Shockwave，一个基于两个关键思想的具有未来规划的调度器。\n 首先，Shockwave 将经典市场理论从静态设置扩展到动态设置，从而实现效率和公平的协同优化 第二，Shockwave利用随机动态规划来处理动态变化    我们建立了一个Shockwave系统，并通过跟踪驱动仿真和聚类实验对其性能进行了验证\n 结果表明，与现有的公平调度方案相比，对于具有动态自适应的 ML 作业跟踪  Shockwave 方案的完成时间提高了1.3倍 公平性提高了2倍      引言   GPU 驱动的深度神经网络(DNN)训练正迅速成为数据中心的核心工作负载。\n 由于训练数据量庞大，模型规模不断增大，许多 DNN 模型无法在单个 GPU 设备上进行训练，分布式多 GPU 训练已成为常态。 对 GPU 设备日益增长的需求促使企业整合硬件资源，并在共享的 GPU 集群中运行工作负载。 因此，建立能够公平地仲裁 GPU 资源竞争任务之间的调度机制，并有效地为高集群利用率调度这些任务是非常重要的。    虽然在为 DNN 工作负载设计调度程序方面有大量的工作，但是他们并没有使用严格的方法来协同优化系统的效率和公平性。\n 像 Gandiva [41]和 Tiresias [21]这样的系统使用诸如动态缩放、时间分片和超额订阅等技术优化完成时间和平均 JCT (作业完成时间) ，但不考虑公平性。 基于处理器共享的方法，如 DRF [17]和 Gavel (加权极大极小公平)[33]在每个调度周期中提供(主导)资源的即时公平共享，但这可能会显着损害效率[20,35]。 基于 Stride [39]的调度方法，如 Gandiva-Fair [10]要求集群运营商明确指定单个作业的份额(例如，A20% 和 B80% 的 GPU) ，手动指定的固定份额可能违反机器学习作业的长期公平性[29]。 最后，AlloX [28]和 Themis [29]旨在通过采用基于过滤器的方法来提供长期的公平性，其中在每一轮中，距离公平份额最远的作业子集被过滤，并且在过滤的作业中，最大效率的作业由调度器选择。然而，过滤器值需要繁重的手工调整; 此外，即使仔细调整，使用固定的过滤器可能导致次优效率和公平性(2)。    我们设计了 Shockwave，一个利用市场理论的调度器，以一种系统的和有原则的方式，共同优化机器学习训练工作的效率和公平性。\n 我们制定了一个费舍尔市场[5] ，其中每个工作收到一个平等的预算购买资源的中央仲裁人。 仲裁者然后计算价格，使市场达到一个均衡; 也就是说，每个工作的预算都被用于最大化其性能(例如，训练产量) ，所有的资源都被完全出售。 运用市场理论制定资源配置方案是有力的，因为实现市场均衡既保证了公平，又保证了效率。  每项工作在获取资源方面具有同等的购买力，确保了公平性。 此外，市场清算均衡确保了工作的节约和每个工作的表现是最大限度地给予其预算。      虽然经济理论是许多先前系统的基础(例如 DRF [17]、 Themis [29]和 REF [43]) ，\n 但它们都假设作业具有已知的静态资源请求。这个假设对于弹性机器学习训练工作来说不再正确  其资源需求随时间动态变化 进一步，资源需求的变化依赖于模型更新模式，因此它们是未知的先验  例如，训练作业可以通过计算梯度噪声标度(GNS)来动态调整它们的批量大小[30,31]。  OpenAI 使用批量大小缩放(从32到32M)将 GPT-3训练加速500倍[7] ， 同样，BERTLarge 训练使用动态批量大小(256到4096)来实现2.5倍的加速[37]。       本文将市场理论推广到具有弹性资源需求的机器学习作业调度问题    现有的调度器要么是不可知的，要么是对动态变化作出反应\n  它们不能保证公平性或显著降低效率。\n 关键原因是，当前的最优时间表或权重分配在未来可能是次优的，被动地重新排列工作的优先级可能为时已晚，无法弥补早期阶段的优先级不足    最先进的调度器可以适应动态性\n 例如，Pollux 可以代表作业自动调度，例如，可以自动调整批量大小。 我们发现这会影响训练的准确性    我们的目标是让用户根据算法的需要执行弹性变化。\n 在动态条件下实现公平分配而不需要对动态进行任何控制是具有挑战性的 现有的研究也没有对此进行研究。      为了支持资源需求随时间的动态变化，我们扩展了经典的静态费舍尔市场，并提出了一个新的离散的动态市场，可以确保长期的效率和公平性。\n 使用离散时间可以帮助我们捕捉多轮重复运行市场的效果，而动态市场可以帮助我们捕捉作业的时变效用。  例如，考虑一个场景，其中我们为一个作业运行20轮调度。 如果一个作业的 perGPU的batchsize在10轮后由于 GNS （梯度噪声规模）扩展而翻倍，那么在10轮后，他在某个GPU上的利用率分配也会翻倍（$u_1$ = 2 $u_0$)。 一个静态市场将假设利用率随时间不变, 20轮的累计利用率率为20$u_0$; 一个动态市场可以捕捉的变化效用的工作随着时间的推移，20轮的累计效用将是30$u_0$   准确计算效用可以使动态市场随着时间的推移优化公平和效率。  我们证明了我们的动态市场公式(4.2)保证了长期的效率和公平性，例如  随时间推移的纳什社会福利最大化， 随时间推移的帕累托最优最大化， 以及共享激励。        在实际系统中实施动态市场公式具有挑战性，主要原因有两个。\n 首先，市场公式需要知道未来的效用值来计算市场均衡。  作业中的动态适应是不确定性触发的，因为它们依赖于不同模型和数据集的梯度值，这使得预测未来的效用是很有挑战性的。   第二，在(无限)长的时间范围内求解动态市场均衡是困难的和不切实际的。  它在计算上是不可行的，并且需要预测每个作业未来的性能特征   此外，当作业到达并在线完成时，我们需要周期性地解决市场均衡问题，同时保持较低的调度开销。    为了弥合理论与系统之间的鸿沟，Shockwave 解决了这些挑战，实现了一个动态适应预测器和一个近似动态市场。\n 首先，我们观察到现实世界中机器学习工作负载的动态适应遵循一些模式，这些模式可以用贝叶斯统计来预测。 然后，我们发展方法，以整合这些预测到我们的动态市场公式。 其次，在执行基于循环的调度时，我们发现为(无限)长的时间范围规划一个调度可能会引入大量的开销。  为了保持较低的计划开销，Shockwave 只计划有限长度的时间窗(例如，30-60分钟) 我们设计的估计器可以捕捉到短期计划对长期公平性和长期效率的影响 这种设计帮助我们在不牺牲长期目标的情况下平衡系统开销      我们在32-GPU 集群试验台上评估了Shockwave，并使用模拟器研究了大规模的 GPU 集群。\n 使用来自以前的现实世界系统的多个工作负载[33,36]  我们发现与现有的公平的 DNN 集群调度器(包括 Themis [29] ，Gavel [33] ，AlloX [28]等)相比，Shockwave 提高了1.3倍的完成时间和2倍的公平性   我们进一步评估不同大小的集群上的Shockwave。使用与物理集群相同的调度器构建的模拟器  我们发现 Shockwave 可以在256个 GPU 上调度900个活动 DNN 训练作业，并且与现有的调度器相比，保持了最大完成时间(1.26-1.37 ×)和公平性(2.5-3.1 ×)的优势 我们的解决方案开销仍然很低，不到两分钟循环周期的12.5%   Shockwave是开源的 https://github.com/uw-mad-dash/Shockwave。    动机 ","date":"2023-05-19T17:55:06+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202305191757609.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0shockwave%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Shockwave论文阅读笔记"},{"content":"摘要\n  背景\n 为了加速深度学习(Deep Learning，DL)模型的培训，使用配备 GPU 等硬件加速器的机器集群来减少执行时间。 需要最先进的资源管理器来提高 GPU 的利用率和最大化吞吐量。 虽然在同一 GPU 上共享 DL 作业已被证明是有效的，但这可能会引起干扰，从而导致速度减慢。    Horus: 一个用于 DL 系统的干扰感知和基于预测的资源管理器。\n Horus 主动预测了从 DL 模型的计算图特征推断出的异构 DL 作业的 GPU 利用率，从而消除了在线剖析和隔离保留 GPU 的需要。 通过跨异构 GPU 硬件的微基准测试和工作共享  我们将 GPU 利用率确定为一个通用的代理度量，以确定良好的安置决策 与目前的方法相反，去除了GPU在线剖析，不直接测量每个提交的工作的 GPU 利用率。   我们的方法促进了高资源利用率和完成时间的减少; 通过真实世界的实验和大规模的跟踪驱动模拟，我们证明  Horus 在 GPU 资源利用率方面优于其他 DL 资源管理器高达61.5% 在完成时间减少方面优于23.7-30.7% 在作业等待时间减少方面优于68.3%      引言  由于现有的资源管理器，如 Kubernetes和 YARN禁止明确使用 GPU 共享(即，只允许将单个 DL 作业分配给每个 GPU)。  这种利用率不足降低了性能、资源效率和服务可用性 导致排队时间延长 需要额外的 GPU 设备来满足需求   共享GPU已经成为解决利用率不足的一种方法  这种共存位置的有效性基于对 DL 工作负载 GPU 利用模式的良好理解 对于提供商来说，这可以实现高质量的 DL 系统调度和同位置决策。从而减少 GPU 资源利用不足的情况。 了解和利用 DL 工作负载利用率来改善同一地点对于设计资源节约型 DL 系统至关重要   但是，通过 DL 工作负载描述 GPU 利用率的已有方法在执行过程中利用了在线分析。  在线分析需要在独立的 GPU (或专用机器)上执行每个惟一的 DL 作业，以确保准确的度量收集[21] ，[22]。由于需要保留 GPU 设备，这种在线分析导致服务可用性和资源效率降低: 随着不同模型架构和配置数量的增加，这是一个日益严重的问题[8]。虽然同位可以提高 GPU 的利用率，但它也可能引起性能干扰(我们称之为干扰) ，导致不同同位组合的 DL 作业平均减速18% [8]。虽然 DL 资源管理器现在允许同位[6] ，[8] ，[10] ，但较少注意积极解决 DL 作业之间的干扰共享相同的 GPU 期间安置决定。差的 DL 工作安排导致更高的完成时间，增加的作业完成时间(JCT) ，作业驱逐，以及 GPU 内存不足(OOM)错误造成的作业失败[9]。   在本文中，我们提出了 Horus: 一个基于预测的干扰感知的 DL 系统资源管理器。与现有的方法相比，Horus 根据 DL 作业的模型特征主动预测 GPU 对未知 DL 作业的利用率，我们的调度器利用这些模型特征来确定合适的 DL 作业同位组合以最小化干扰。我们的方法避免了剖析内核模式[10] ，[13] ，[21] ，[22] ，修改底层 DL 框架，也不需要在调度器运行时需要一个孤立的 GPU 的作业执行的大量在线剖析ーー所有这些都是昂贵的和时间的  同一地点所造成的数据传输角色塑造工作量干扰。我们已经描述了超过600个跨异构 GPU 硬件架构的 DL 作业的独特组合的干扰概况。研究结果表明，DL 工作同位干扰导致2.4 x-3.4 x 的速度减慢，与分布式培训的网络局部性相当。 DL 工作负载的 GPU 利用率分析与预测引擎。通过一系列的基准测试，我们分析和识别了 DL 模型的关键特性及其与 GPU 利用率的关系。其中包括每秒浮点运算(FLOPs)、输入数据大小和卷积层数目等 DL 计算图结构。我们提出的预测引擎允许分秒的 DL 作业 GPU 利用率预测，而不需要在线剖析。 支持干扰的 DL 资源管理器。利用我们的预测引擎，我们提出了一个支持协同定位和最小化 GPU 过度使用的干扰软件资源管理器。我们的方法提供了两种可供选择的调度算法，优先考虑最小化作业完成时间或提高公平性以避免作业匮乏ーー降低中位作业等待时间，代价是作业完成时间和利用率的边际降低。资源管理器被集成到 Kubernetes，部署在一个数字用户线集群中，并通过对一个生产数字用户线集群的跟踪驱动模拟进行大规模评估。结果表明，与现有方法相比，我们的方法在 GPU 集群利用率方面实现了32-61.5% 的增长，并且最多可以减少23.7-30.7% 的工作时间。   我们扩展了我们以前的工作[23] ，通过将 DL 工作负载角色塑造研究的范围从81个增加到292个模型，获取更多的图形处理器架构和600多个用于分析和建模的同位概况，提高图形处理器预测模型的准确性，并通过追踪驱动的生产集群模拟来评估 Horus 的规模。Horus 框架也进行了重新设计，包括一个精确的公平排队调度算法，以最小化成本目标。最后，通过一组额外的工作负载组合和一个额外的共同定位算法进行了评估，以进行比较[8]。  动机 背景 DL 利用率与推理 CO-LOCATION关系研究 剖析设定 GPU利用率与JCT减缓的关系 Horus  Horus 是一个基于预测的、支持干扰的 DL 系统资源管理器，它被设计成一组组件，可以作为现有集群资源管理器框架(如 Kubernetes)的一部分部署。  图5描述了 Horus 架构，它包括三个主要组件:  预测引擎 度量库 应用程序控制器。   \rimage-20230515124449778\r 预测引擎  在作业提交时，应用程序控制器通过检查工作负载定义向预测引擎发送一个请求，以估计 DL 作业 GPU 的使用情况，即 GPU 利用率和 GPU 内存利用率。 具体来说，预测引擎需要一种访问 DNN 图并模拟运行模型的方法   度量库  集群视图通过基础设施更新和监控代理来维护，从每个节点收集基础设施数据，包括 GPU 使用情况和系统使用情况(主机内存使用情况和 CPU 使用情况)。 N 个代理部署在每个单独的节点上，报告应用程序和系统利用率指标，这些指标最终被收集到度量存储库中。   应用控制器  然后，调度程序通过计算 DL 作业的适用性将其分配给 GPU ーー最小化成本函数目标，以支持同地共存的 缓存集群状态。 我们的方法旨在最大限度地提高 GPU 的利用率，并通过降低同一地点放置决策的优先级来最小化完成时间，这将导致严重的干扰和通信延迟导致 JCT 减速(第4.2节)。      预测引擎 预测GPU利用率  总览  预测引擎通过迭代 DL 模型的开放神经网络交换(ONNX)图表表示来提取表3中描述的关键 DL 工作负载特征。我们可以通过迭代每个运算符，并根据其输入、输出形状和参数进行计算，获得 FLOP 等聚合特征。 然后将特征标准化并用作机器学习模型的数字输入，以预测 GPU 的利用率(GUtilj)。我们在离线培训阶段基于一组历史 DL 工作负载概况微基准来训练预测模型，类似于现有的基于预测的方法[14] ，[29]。这些配置文件通常是通过运行微基准测试的开发人员获得的，或者是通过监视孤立 GPU 上现有的非共存 DL 工作负载获得的。 至关重要的是，在成功的预测模型训练之后，不需要为进入系统的唯一 DL 工作负载进行隔离剖析。值得注意的是，这种方法也可以与反应性方法[8] ，[10] ，[15] ，[17] ，[18]相结合。机器学习模型可以在收集了额外的配置文件(例如，当发现新的模型)之后周期性地重新训练。   特征重要性  为了进一步了解是什么促进了 GPU 的利用，我们调查了每个基于树的回归特征权重，通过提取权重和平均它们，如图6所示。 \rimage-20230515125327336\r 这些特性是明确的指标，并遵循现有的模型压缩和神经结构搜索的文献，其中减少参数和中间激活的数量可以节省硬件的计算和内存消耗[37] ，[45]。令人惊讶的是，我们发现卷积的数量和剩余的特性对回归器的影响很小，甚至没有影响，我们计划利用编译器中间表示来获得更多的硬件特性特征[25]。   模型评估  模型的准确性是通过测量回归平方平均数对数误差(RMSLE)来确定的。 这种方法对于利用率预测是有用的:  虽然高估 GPU 利用率在最大化资源效率方面并不理想 但是低估可能导致意外的 GPU 过度分配和我们试图避免的干扰   表4显示，所有的预测模型实现了相对较低的 RMSLE 得分为0.133。 \rimage-20230515125313318\r    预测GPU内存利用率 与 GPU 利用率相比，估计 GPU 内存利用率更为复杂，因为总的作业内存大小(MiB)受各个 DL 库的初始化和优化控制。在不研究内核实现的情况下，可以通过考虑正向 Mf 和反向传递 Mb 中的以下四个因素来估计最小预期内存使用量(以字节为单位) : (i)数据 B 的批量大小，(ii)激活次数 A，(iii)梯度 G 的数量和(iv)参数 P 的数量。除了初始化开销 d 之外，给定 DL 作业 j 的总体估计内存需求将是\n\rimage-20230515125749843\r\nGPU 利用率(GUtilj)和 GPU 内存(GMemj)估计值将用于调度器中的节点容量检查，以防止处理传入作业。\n推理感知的任务调度 Gandiva [8]布局策略监控应用程序吞吐量，当使用未定义的阈值和时间段进行减速检测时，一个作业被随机杀死或迁移到另一个节点。在这种方法中，随机作业迁移可能被分配到另一个不兼容的作业，从而导致相同或更大的性能下降。\nAntman [10]通过监视 DL 作业，雇用本地协调器和修改底层 DL 框架来实现共同定位，以允许对 DL 作业内核进行细粒度控制，在 GPU 上注入空闲时间以减轻共同定位作业之间的干扰。然而，这种方法需要理解和分析内核在运行时的执行顺序，以确定适当的空闲时间。我们的干扰感知调度的核心是在作业执行之前了解计算资源需求，并以尽可能少的成本为作业分配相应的资源。这与现有的 DL 系统调度程序形成对比，后者在获得工作负载利用模式之后作出反应。\n任务调度计划  问题建模  \rimage-20230515130031081\r \rimage-20230515125939659\r   成本明细  \rimage-20230515125953903\r \rimage-20230515130047005\r \rimage-20230515130058309\r \rimage-20230515130130804\r    基于有权公平队列的运行时作业调度 \rimage-20230515130353224\r\n 作业聚合  在实际调度作业之前，我们对所有作业执行一个聚合过程。具体来说，L1距离度量用于鉴别类似的工作，考虑到以下特征:  (1)任务数量 (2)预测的 GPU 利用率 (3)每个任务的 GPU (4)估计的 GPU 内存   在实践中，我们对所有待执行作业运行 k-means 算法来识别相似的作业，并将它们放入相应的队列，即 Q1.41.Q1; \u0026hellip; ; Qk (第2行)。 我们设置 k = 3，因为我们发现，从图1a，利用模式有3个不同的 CDF。   基于加权公平排队的作业选择  **line 4：**基于加权公平排队的作业选择。据推测，b 作业被允许作为批处理进入每个调度周期。公平地说，horus拿起一定数量的未决根据队列权重从每个队列中删除作业，即作业挂起的程度。 我们期望从等待时间更长、队列长度更大的队列中选择和处理更多的作业，  我们测量作业的每个队列的中值等待时间和队列长度的乘积的权重，其中最大运算是为了保证一个非零值一旦中值等待时间为零时，所有作业都是新到达系统。  中值具有统计特性，受偏斜数据的影响较小，可以更准确地反映一类作业的排队时间。   最后，计算从 Qx 中选择的作业数。  这种设计可以积极地避免任何特定类别的工作饥饿——当某个类别的工作开始饥饿时，将会选择更多的工作。我们还允许对饥饿作业进行预留，因为加权公平排队算法会选择等待时间最长的作业。       资源分配  由于所有待完成的任务都按照加权公平性排序到 e J 中，因此调度器将尽最大努力为每个任务分配可用资源，同时将性能干扰降到最低。 **line 8：**具体来说，对于每个作业，我们检查资源容量， 并且选择所有可以满足作业 j 在 CPU、内存和 GPU 内存方面的所有需求的节点。 **line 9-12：**GPU 内存需求是通过方程推断出来的。(2)第4.1条。基于作业所需的 GPU 总数和每个节点的 GPU 数，我们计算出满足作业 j 需求的最小节点数 line 14: 用方程式(7) ，然后我们可以计算在 N (第14行)中每个节点的每个 GPU 上调度作业的成本 line 16: 以最小的成本(第16行)选择顶部的 GPU (G)。 line 18: 并在最终资源分配和作业调度(第18行)    ","date":"2023-05-15T09:45:37+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202305151014810.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0horus%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Horus论文笔记"},{"content":"Transparent GPU Sharing in Container Clouds for Deep Learning Workloads 摘要\n  容器广泛用于数据中心中的资源管理。\n 支持容器云中的深度学习(DL)训练的一个常见实践是静态地将 GPU 完全绑定到容器上。 由于生产中 DL 作业的资源需求多种多样，大量的 GPU 未得到充分利用。 因此，GPU 集群具有较低的 GPU 利用率，由于排队，导致作业完成时间较长。    我们提出 TGS (透明 GPU 共享) ，一个系统，提供透明的 GPU 共享到集装箱云中的 DL 训练。\n 与最近 GPU 共享的应用层解决方案形成鲜明对比的是，TGS 在容器下的 OS 层运行。 透明性允许用户使用任何软件来开发模型并在其容器中运行作业。 TGS 利用自适应速率控制和透明统一内存，同时实现高 GPU 利用率和性能隔离。 它确保生产作业不会受到共享 GPU 上机会作业的严重影响。 我们已经建立了 TGS，并将他与docker和kubernetes整合。实验结果表明:  TGS 对生产作业的吞吐量影响不大 TGS 为机会作业提供了与最先进的应用层解决方案 AntMan 相似的吞吐量，并且与现有的 OS 层解决方案 MPS 相比提高了15倍的吞吐量。      引言   问题：\n  大型企业构建多租户 GPU 集群，许多团队共享这些集群来开发和训练 DL 模型。\n  在容器云中支持 DL 训练的一个常见实践是静态地将完整的 GPU 绑定到容器。\n 当一个 GPU 被分配给一个容器时，容器对 GPU 有独占访问权，这为生产作业提供了性能隔离。 但这意味着其他集装箱在同一台机器上不能使用 GPU 时，GPU 未得到充分利用，甚至完全空闲。    这种方法的主要限制是资源利用率低。\n 微软最近对一个生产型 GPU 集群的研究表明，GPU 的平均利用率只有52% [5]。 另一项针对阿里巴巴一个生产型图形处理器集群的测量显示，该集群的图形处理器利用率甚至更低ーー图形处理器利用率的中值不超过10% 。 然而，由于独占 GPU 分配，即使许多 GPU 没有得到充分利用，传入作业也必须在队列中等待调度。这会导致后续作业的作业完成时间较长。      解决方案：\n 这个问题可以通过 GPU 共享来解决，从而提高 GPU 的利用率。 在生产环境中，DNN 训练作业通常分为两类:  生产作业：其他作业不会对他造成太多的性能下降 机会作业：利用空余资源   在两类作业之间共享 GPU 以提高 GPU 利用率是很自然的。然而，对于生产环境来说，确保 GPU 共享对生产作业的影响最小化是至关重要的。 GPU 共享解决方案可以在应用层或操作系统层实现。  AntMan是最先进的应用程序层解决方案。虽然 AntMan 可以提供高 GPU 利用率和性能隔离，但它对 DL 框架进行了重大修改，并限制用户使用给定框架的特定版本。 NVIDIA MPS是一个操作系统层的解决方案。MPS 需要应用程序知识来设置性能隔离的资源限制，并且不支持 GPU 内存超订情况下的 GPU 共享。它将多个流程合并到一个 CUDA 上下文中，从而导致作业之间的命运共享。      我们提出 TGS，一个系统，提供透明的 GPU 共享到集装箱云中的 DL 训练。\n 特点  与应用层解决方案不同，TGS 在操作系统层工作，并在操作系统层实现应用层解决方案的好处，而不受现有操作系统层解决方案的限制。 透明性允许用户选择任何 DL 框架的任何版本(TensorFlow、 PyTorch 或自定义框架)来开发模型并在容器中运行作业。   核心：TGS 的核心是容器和 GPU 之间的轻量级间接层。  它拦截从容器到 GPU 的系统调用，并调节并发作业的 GPU 资源使用。 TGS 支持生产作业和机会作业之间的 GPU 共享，但是在很大程度上将生产作业与争用隔离开来。   在实现具有性能隔离的 OS 层 GPU 共享解决方案时，存在两个主要的技术挑战。  第一个挑战是在没有应用程序知识的情况下自适应地在容器之间共享 GPU 计算资源。  不正确地为每个容器设置资源限制会降低作业性能或使资源闲置。 MPS 和 MIG 需要应用程序知识来手动设置资源限制。TGS 在没有应用知识的情况下应用自适应速率控制方法来解决这一挑战。 它在运行时监视生产作业的性能，并自适应地更新资源分配到机会作业。 控制回路自动收敛到这样一个点，即机会作业利用尽可能多的资源，而不会对生产作业产生太大影响。   第二个挑战是启用透明的 GPU 内存超订。  GPU 有自己的内存来保持应用程序状态。当容器所需的 GPU 内存总量超过 GPU 内存大小时，MPS 失败。 AntMan 使用 DL 框架中的自定义内存管理组件来管理应用层 GPU 内存和主机内存之间的内存交换。 设计了一种基于 CUDA 统一存储器的透明统一存储机制，实现了操作系统层的统一存储，避免了显式修改应用程序的需要。 这种机制在 GPU 内存超额预订时管理底层的内存交换。TGS 利用位置偏好来确保 GPU 内存优先用于生产作业，以保护其性能。        总之，我们做出了以下贡献。\n 我们提出了 TGS 系统，为容器云中的 DL 训练提供透明的GPU共享。 我们设计自适应速率控制和透明的统一内存机制，以同时实现高 GPU 利用率和性能隔离。 我们实施 TGS，并与 Docker 和 Kubernetes 整合。实验表明:  TGS 对生产作业的吞吐量影响不大 TGS 为机会作业提供了与现有 OS 层解决方案 AntMan 相似的吞吐量，并且与现有 OS 层解决方案 MPS 相比提高了15倍的吞吐量。      背景和动机 容器云   容器被广泛用于管理资源和在数据中心部署工作负载，并提供可移植性和隔离性。\n 可移植性  容器是一个独立的软件包，包括运行应用程序所需的所有内容。 容器化的应用程序可以在各种环境中运行，而不需要进行任何修改。 这种可移植性使开发人员能够使用自己选择的工具和应用程序堆栈来开发和运行自己的应用程序，而无需担心部署环境。   隔离性  不同容器中的应用程序通过使用独立的命名空间进行隔离。      与虚拟机相比，容器是轻量级的。\n 虚拟机使用guest操作系统，但容器使用主机操作系统核函数。 因此，应用程序可以在容器中运行时实现裸金属性能。 云运营商使用容器编排平台在数据中心的许多机器上提供、管理和更新容器。    DL训练负载  训练作业包含许多迭代。  每次迭代使用来自数据集的一批样本来训练 DNN 模型。 迭代包括向前传递和向后传递。  前向传递使用 DNN 模型来计算批中样品的标签。根据输出标签和实际标签使用损失函数计算损失。 反向传递将损失从 DNN 模型的最后一层传递到第一层，并计算每个权重的梯度。DNN 模型使用优化器基于梯度进行更新。   DL 训练是计算密集型的，因此通常使用 GPU。然而，正如微软和阿里巴巴所报道的那样，广泛采用的独家图形处理器分配导致生产中的图形处理器利用率较低。    现存工作的局限性   提高 GPU 利用率的一种自然方法是 GPU 共享。\n 如果一个容器不能利用所有的 GPU 资源，那么可以通过多个容器共享一个 GPU 来提高 GPU 的利用率。 然而，共享 GPU 上的容器会争夺 GPU 的计算和内存资源，这种干扰会降低作业的速度。    GPU 共享可以在应用层或操作系统层完成。\n 应用层解决方案[6,12,13]  主要缺点是它们对用户不透明，也就是说，它们需要对 DL 框架进行重大修改。 用户只能使用给定框架的一组受支持的版本，如果某个特定 DL 框架的新版本出现，则必须等待集成。 这种方法失去了允许用户使用任何工具在容器中开发和运行应用程序的优势。   OS 层解决方案  NVIDIA MPS [9]是一个用于 GPU 共享的 OS 层解决方案。  它需要应用程序知识来正确设置每个进程的资源限制，以确保性能隔离。更重要的是，MPS 需要进程的总 GPU 内存来适应 GPU 内存容量，并依赖于应用程序来处理 GPU 内存和主机内存之间的内存交换。 MPS 的另一个限制是它不提供故障隔离。MPS 将多个进程的 CUDA 上下文合并到一个 CUDA 上下文中以共享 GPU。当一个进程失败时，它会使 MPS 服务器和其他进程处于未定义的状态，并可能导致进程挂起、损坏或失败。   NVIDIA 多实例 GPU (MIG)[14,15]是另一种 OS 层解决方案。  MIG 需要 GPU 硬件支持，目前只能在三种高端 GPU 上使用  NVIDIA A100 NVIDIA A30 NVIDIA H100。   MIG 不能根据应用程序的需要任意地对 GPU 进行分区; 它只支持针对给定配置集的 GPU 分区。  NVIDIA A100 GPU 可以划分为 GPU 实例，为不同的 DL 训练任务提供单独的计算和内存资源，MIG 只为每个 GPU 实例提供7种固定配置 每个 GPU 实例不能使用超过4/7的 GPU 计算资源或1/2的 GPU 内存资源。   此外，如果 GPU 上有正在运行的作业，即使容器的 GPU 使用发生了变化，它也不能动态更改 GPU 实例所拥有的 GPU 资源。MIG 的重构只能在 GPU 空闲时进行。 MIG 不支持内存超订。        TGS 总览  目标  透明性：应该对应用程序透明，以便用户可以使用任何软件开发和训练集装箱 DNN 模型。 高利用率：应实现高 GPU 利用率的计算机和内存资源。 性能隔离：应该为 DL 作业提供性能隔离，生产作业不应该受到机会作业的显著影响。 错误隔离：一个容器中的应用程序的故障不应该导致其他容器中的应用程序崩溃。 \rimage-20230508104744867\r   架构  \rimage-20230507223214328\r TGS 是一种 OS 层解决方案:  透明：  它位于容器和 GPU 之间。容器和应用程序不知道 TGS。用户可以使用任何自定义框架来开发和训练 DNN 模型。 一个 GPU 作为一个普通的 GPU 暴露在容器中。容器中的进程将 GPU 核函数(即在 GPU 上执行的函数)发送给 GPU，就像它们对专用 GPU 所做的那样。   共享  TGS 使用一个轻量级的间接层在几个容器的工作负载之间共享 GPU。间接层从容器中截取 GPU 核函数并调节这些 GPU 核函数以控制每个容器的资源使用。       核心idea  TGS 利用自适应速率控制机制和透明的统一内存机制解决了在操作系统层提供透明 GPU 共享的两个挑战。  第一个挑战是在没有应用程序知识的容器之间自适应地共享 GPU 计算资源。  为了应对这一挑战，TGS 的速率监控器监控每个容器的性能，并提供 CUDA 块(GPU 上的一个基本调度和执行单元)的数量作为控制回路的实时信号。 TGS 的速率控制根据信号自适应地控制每个集装箱向 GPU 发送 GPU 核函数的速率。 控制回路自动收敛到机会作业利用尽可能多的剩余资源来实现高 GPU 利用率而不会严重影响生产作业的性能。   第二个挑战是启用透明的 GPU 内存超订。  AntMan [6]修改了 DL 框架，以便在 GPU 内存超额预订时交换 GPU 内存。 OS 层解决方案 MPS 不支持 GPU 内存超订，并且依赖于应用程序来处理内存交换。这些方法是不透明的。 为了解决这个问题，TGS 开发了 CUDA 统一存储器[1  它将 GPU 存储器和主机存储器统一到一个单独的存储器中的内存空间。 TGS 拦截和重定向 GPU 内存分配调用从容器到 CUDA 统一内存空间。当 GPU 内存超额预订时，TGS 可以自动将一些机会作业的数据驱逐到主机内存中，并将相应的虚地址映射到主机内存中的新数据位置。 整个过程对应用程序是透明的。为了确保性能隔离，TGS 使用内存放置首选项来优先分配 GPU 内存用于生产作业，而不是机会作业。       TGS 的设计还有两个好处。  首先，架构是轻量级的。TGS 开销小，符合容器原理。 其次，TGS 提供了与常规容器相同的故障隔离属性。  TGS 中的容器使用单独的 GPU 上下文，而 MPS 将容器的 CUDA 上下文合并为一个。 因此，一个容器中的应用程序错误不会影响或终止其他容器。        TGS 设计 共享GPU计算资源   TGS的目标与挑战\n 确保 生产作业的表现不会受到机会作业的严重影响。 机会主义工作使用的资源不超过生产性工作留下的资源。 要做到这一点，我们需要解决两个问题。  首先，我们需要估计生产作业留下了多少资源。 其次，我们需要控制机会主义的工作，使用剩余的资源。      稻草人解决方案: 优先级调度。\n 过程  它从容器中截取 GPU 核函数，并根据作业的优先级将其放入生产队列和机会队列中。 只有当生产队列为空时，机会队列中的核函数才被调度到 GPU。 该解决方案通过检查生产队列是否为空来估计是否有剩余资源，通过对生产队列中的核函数进行优先排序来控制机会作业的资源使用。 这是一种性能隔离和高利用率的规范解决方案，已经在计算机系统中得到广泛应用。   但是，这种解决方案不适合 GPU 共享。  GPU 作业的空生产队列并不意味着生产作业不使用 GPU。  GPU 核函数是一个经过优化的 GPU 函数，可以运行一段时间。 过去调度的 GPU 核函数可能仍然在 GPU 上运行，而生产队列是空的。   空队列也不能告诉 GPU 上剩下多少资源。  因此，如果机会队列中的核函数被发送到 GPU，并且生产作业使用了大部分的 GPU 资源，那么来自两个作业的 GPU 核函数将会相互竞争，这将会给生产作业带来巨大的开销。   跟踪运行在 GPU 上的 GPU 核函数也是不可行的，因为 GPU 的状态是不完全可见的。   在 GPU 设备驱动程序中实现一个优先级调度程序是有可能的  这样调度程序就可以完全看到资源的使用情况，并且可以执行细粒度控制。 这个解决方案并不通用。  它与低级别的 GPU 细节紧密相连，并且需要基于其架构和执行模型与每种类型的 GPU 进行深度集成。 有些 GPU 是黑盒，不向操作系统公开这种控制。        我们的解决方案: 自适应速率控制。\n  TGS 使用自适应速率控制方法。\n 主要思想：根据核函数到达速率精确控制机会主义队列中核函数的出队速率，使得机会主义作业可以在不影响生产作业的情况下消耗剩余的计算资源 这是一种通用的 OSlayer 方法: 它与低级别的 GPU 细节分离，并且不需要访问 GPU 内部控制。    \rimage-20230508124005393\r\n  这种方法需要一个反馈信号来告诉控制回路是否可以增加机会队列的排队速率以使用更多的资源或应该减少以避免降低生产作业。\n  1）理想情况下，我们希望使用应用程序性能，\n 即 DL 训练工作负载的训练吞吐量作为反馈信号，因为这是我们最终关心的指标。 然而，我们不能直接获得训练的吞吐量，因为这需要应用知识，我们的目标是设计一个操作系统层的解决方案，是透明的应用程序。    2）信号的一种选择是 GPU 利用率，\n 也就是说，如果 GPU 利用率低于100% ，则提高利用率。 而这个选择看起来很自然，但有两个缺点。  首先，GPU 利用率的定义是特定于硬件的，并且常常是模糊的。  今天的图形处理器在单个芯片上包含不同类型的计算单元  例如，NVIDIA 图形处理器上不同数据类型的张量核心和 CUDA 核心。   GPU 驱动程序报告的 GPU 利用率(如果支持的话)通常缺乏一个精确的定义。  即使是这样(例如，使用流处理器的百分比) ，对于具有多种计算单元类型的 GPU 来说，单个利用率值实际上意味着什么还不清楚。     其次，GPU 利用率仅与应用程序性能松散耦合。  即使报告的 GPU 利用率低于100% ，这并不意味着我们可以在不减慢生产作业的情况下增加机会队列的排队速度。 例如  一个生产作业和一个机会作业可能会竞争已经被生产作业单独使用的同一类型的计算单元，尽管还有其他类型的计算单元处于闲置状态 两个作业也可能会竞争 GPU 利用率所捕获的资源之外的其他资源。          3）在 TGS 中，我们使用生产作业的核函数到达率(即 TGS 从容器接收核函数的速率)作为反馈信号\n 计算图：  一个 DL 训练作业基于 DNN 模型为其训练过程构造一个计算图。 它使用计算机图形生成和发送核函数到 GPU 执行训练。 计算图捕获核函数之间的依赖关系。   核函数到达率直接对应于训练吞吐量。  如果训练速度变慢，则核函数完成速度变慢，依赖性得到满足的速度变慢，核函数到达率下降。   因此，TGS 采用速率监控模块来监控生产作业的核函数到达率，并以此作为反馈信号来控制机会作业的核函数出队率。  注意，生产作业和机会作业之间的任何争用都可以通过这个核函数到达率来捕获，包括  GPU 缓存争用 CPU 争用 网络争用   它们中的一些超出了 GPU 硬件设计可以控制的范围，TGS 使用速率控制作为旋钮来控制所有这些。 由于核到达率的方差很小，TGS 使用移动平均来平滑核到达率的估计。 对于来自生产作业的核函数  TGS 只执行一个简单的计数操作来估计核函数到达率。 它不对核函数排队，而是直接将它们传递给 GPU，以最小化对生产作业性能的影响。            速率自适应算法。\n 速率自适应算法控制机会队列的核出队速率，使生产作业的核到达率不受影响，使机会作业的核出队速率达到最大。  在形式上，让 αin 和 αout 分别表示生产作业的核到达和离开 TGS 的速率 而 βin 和 βout 表示机会作业的核到达和离开的速率   TGS 只监视，但不限制生产作业的速度。所以 αin = αout。  设 GPU 不共享时生产作业的核函数到达率为 R。速率控制算法是将 β 输出最大化，使得 αin = R。 在公式中，βout 是由算法控制的变量，αin 依赖于 βout。 设 f 是捕捉 αin 和 βout 之间关系的函数，即 αin = f (βout)。 然后算法必须解决以下最佳化问题。 \rimage-20230508110522147\r   F (βout)的确切形状是未知的，但我们知道它的粗略形状的性质的问题。  \rimage-20230508125705811\r 如图3(a)所示，f (βout)是平的，当 βout 小时等于 R，当 βout 大时单调递减。  直觉认为，当 β 输出较小时，图形处理单元没有得到充分利用，执行机会作业的核心不会影响生产作业的表现，从而导致生产作业的表现为平线; 当 β 引爆流行减小时，机会作业开始与生产作业竞争图形处理单元的资源，从而导致生产作业的表现下降。注意单调递减部分不一定是线性的 图3(a)说明了当 βout 增加时 αin 递减的一般趋势。这个算法的目标是找到 f (βout)开始减小的引爆流行 β * 。   图3(b)是 GPU 已经被生产作业充分利用的特殊情况  因此即使为机会作业执行少量核函数也会降低生产作业的性能。 在这种情况下，直线没有平面部分。   图3(c)是机会作业需求非常小的特殊情况，  因此即使排队速度不受限制，生产作业的性能也不受影响。 在这种情况下，直线没有单调递减部分。     为了逼近最优的 β 输出，我们使用典型的加法增加乘法减少(AIMD)方法来控制比率 β 输出，如算法1所示。  \rimage-20230508110648106\r 具体来说，TGS 首先测量 GPU 上生产作业的 R 速率，  然后再向 GPU 添加一个用于共享的机会作业(第1-3行)。 加入机会作业后  如果 α 大于或等于 R (第24-行) ，则 TGS 加大 βout 如果 αin 低于 R (第29-30行), 则乘法减少 βout。   AIMD 确保 β 输出可以近似收敛为临界点β*。  为了加速收敛，采用了慢启动阶段(第17-22行)。  实验结果表明，该算法收敛速度快。   当生产作业更改其资源使用模式时，TGS 检测到 R 的方差超过了阈值。  在这种情况下，速率控制模块暂停机会作业并测量新的 R (第26-28行)。   当 R 变得稳定时，速率控制模块使用 AIMD 来调整临界点。  为了保证自适应码率控制算法在大多数情况下的收敛性，我们给出了以下定理。             定理1假设 DL 作业在剖析阶段和收敛阶段是稳定的，自适应速率控制算法在 O (B log B)函数调用中收敛，其中 B 是 GPU 中作业的吞吐量限制。\n该定理的证明见附录 A。证明是基于深度学习训练工作量的稳定性。对于熟悉计算机网络拥塞控制的读者来说，我们的问题类似于多个流争夺共享链接的带宽资源时的带宽分配问题。在带宽分配中，每个流都使用一个拥塞控制算法来控制自己的速率，在系统收敛之后，每个流都会获得相当份额的链路带宽。我们的问题与带宽分配的微妙区别在于，我们没有限制生产作业的速率，只是控制机会作业的速率，以确保生产作业的性能不会受到资源共享的很大影响。\n 共享GPU内存资源   稻草人解决方案: 直通式分配。\n 草人的解决方案是直接将 GPU 内存分配调用从容器传递给 GPU。  好处：只要容器有足够的需求，GPU 内存就能得到充分利用。 限制：  这个解决方案的主要局限性在于它对生产作业有很大的开销。  在这个解决方案中，当生产作业不使用所有的 GPU 内存时，机会作业可以获得剩余的内存。 稍后，如果生产作业想要分配更多的 GPU 内存，它们将无法因为剩余的内存已经分配给机会作业。 如果没有足够的 GPU 内存，生产作业可能会以较低的速度运行，甚至失败，这违反了故障隔离。   此解决方案的另一个限制是它没有考虑到 DL 框架的特征。  当启动一个作业时，一些 DL 框架(例如 TensorFlow)会占用所有可用的 GPU 内存，即使训练作业不需要那么多内存。  这些 DL 框架通常有一个缓存所有已分配内存的内存池，并根据需要将内存分配给训练作业 当某些内存没有被使用时，它们不会释放并将分配的内存返回给 GPU。   这是这些 DL 框架中的一种优化，以避免在作业期间频繁调用 GPU 内存来分配和释放的开销。 这种优化给共享 GPU 内存带来了挑战。  像 AntMan [6]这样的应用层解决方案可以直接修改 DL 框架来获取训练作业的内存使用情况，并禁用不必要的内存缓存来将未使用的 GPU 内存返回给 GPU。 但是，为了设计透明的 OS 层解决方案，不允许修改 DL 框架或应用程序。            我们的解决方案: 统一的 GPU 和主机内存。\n 现代 GPU 提供了一种称为统一存储器的功能，它将 GPU 存储器和主机存储器统一在一个地址空间中。  统一内存传统上被应用程序用来简化 GPU 内存管理。 TGS 统一应用 CUDA为了实现 GPU 内存共享的透明性和性能隔离，采用 CUDA 统一内存分配作为 GPU 内存分配的间接方式。  具体来说，TGS 将 CUDA 统一内存作为伪 GPU 内存公开给容器。 当一个容器发出 GPU 内存分配调用时，无论该调用是针对普通 GPU 内存还是 CUDA 统一内存，TGS 都会拦截该调用并在 CUDA 统一内存空间中分配该调用所请求的内存。 当生产作业没有用完 GPU 内存时，机会作业可以获得剩余的 GPU 内存。     伪 GPU 内存是指所分配的内存对容器和应用程序来说似乎是普通的 GPU 内存  而实际上它可以来自 GPU 内存，也可以来自主机内存，具体取决于可用性。  注意，我们不更改虚拟内存系统。   伪内存仍然是虚拟内存，应用程序使用虚拟内存地址访问分配的伪内存。  GPU/主机虚拟内存地址由 GPU/主机内存管理单元转换为 GPU/主机物理内存地址。     TGS 中的透明统一存储器与原来的 CUDA 统一存储器有两个不同之处，  一是性能隔离  为了提供性能隔离，  TGS 使用 CUDA 统一存储器中的位置偏好来优先分配 GPU 存储器到生产作业。  当 GPU 内存不满时，来自任何作业的内存分配请求将获得 GPU 内存。 当 GPU 内存满时，TGS 尝试将生产作业块放置在 GPU 内存中，并在必要时将机会作业块驱逐到主机。   这对容器是透明的  因为容器仍然使用相同的虚拟内存地址来访问它们分配的内存空间。虚拟内存地址被转换为不同位置的物理内存地址。 这种机制也不会引入额外的内存不足(OOM)故障，因为从 DL 训练作业的角度来看，GPU 的内存容量与原始 GPU 的内存容量相同。       二是 GPU 存储器的透明超订   TGS 中的透明统一内存还解决了在现有 DL 框架中过度占用 GPU 内存的问题，而不需要修改 DL 框架。  当 DL 框架要求所有可用的 GPU 内存时，TGS 从 CUDA 统一内存空间中分配请求的内存量。  实际使用的内存将触发 GPU 页面错误，并在首次使用时交换到 GPU 内存，然后将驻留在 GPU 内存中。   因此，只有训练作业积极使用的部分在 GPU 内存中; 其余部分在主机内存中。 这允许机会作业有效地共享 GPU 内存。      TGS 实现   我们已经用 C + + 和 Python 实现了 TGS 的3000行代码的系统原型，并将其与 Docker 和 Kubernetes 集成。\n 协调进程负责资源管理，并利用 TGS 的间接层实现容器之间的 GPU 共享。 具体来说，采用 TGS 提供的自适应速率控制和透明统一存储器实现 GPU 共享。    自适应速率控制。\n TGS 从容器中拦截与 CUDA 核函数启动相关的 CUDA 驱动程序 API 调用，用于速率监控和速率控制。 由于 CUDA 核函数启动可能由容器中的多个线程引发，TGS 使用一个全局计数器来记录在给定时间段内启动的 CUDA 块的数量。 CUDA 块是一组必须在同一 SM (流式多处理器)中执行的线程，不同的 CUDA 块可以独立并行运行。 由于 CUDA 驱动程序 API 调用中指定了核函数包含的 CUDA 块的数量，因此悬而未决的 CUDA 块的数量可以作为一个实时信号来估计生产作业的性能。 对于生产容器，一个独立的线程充当速率监视器，它定期读取 TGS 的这个计数器，并将值发送到同一 GPU 上机会容器的速率控制组件。 对于机会容器，当 CUDA 驱动程序开始工作时，将创建一个速率控制线程。 速率控制线程根据接收到的统计信息调整机会容器的速率限制。 为了使机会容器的核函数启动率保持在一个合适的值，所有 CUDA 核函数启动 API 调用都会首先重定向到速率控制组件。 速率控制组件访问由速率监视器生成的统计信息，以检查速率限制是否得到满足，并且如果机会容器的速率超过速率限制，则推迟核函数启动。    统一内存管理。\n 为了实现透明的内存共享，TGS 拦截与 GPU 内存分配相关的 CUDA 驱动程序 API 调用，比如 cuMemAlloc，并使用 cuMemAllocManaged 将这些调用替换为统一的内存分配调用。 我们使用 cuMemAdvise 对生产容器的 GPU 内存分配进行优先级排序。  具体来说，我们使用 cuMemAdvise 将内存分配的首选位置设置为当前 GPU，以避免对生产容器进行清除。   当生产容器完成时，机会容器中的间接层将使用 CUDA 驱动程序 API cuMemPrefetchAsync 透明地预取位于主机内存中的内存。    实验 自适应比率控制 统一内存管理 Job流混合负载 系统开销 收敛性 支持不同的DL框架 与AntMan的比较  在这个实验中，我们将 TGS 与 AntMan [6]进行了比较  AntMan [6]是一个用于 GPU 共享的最先进的应用层解决方案。 AntMan 与 DL 框架紧密耦合，并使用应用层度量(迭代时间)来控制机会主义的工作。 AntMan 的开源 GitHub 存储库功能不全。它不包括将资源动态分配给作业的逻辑。我们联系了 AntMan 的作者，并按照他们的说明添加必要的代码以运行 AntMan。 图11a 和11b 分别显示了低竞争(批量4的 ShuffleNet 和批量4的 MobileNet)和高竞争场景(批量8的 ResNet-50和批量4的 ShuffleNet)下的比较。  尽管 AntMan 使用应用层知识并控制应用层的作业，TGS 仍然实现了与 AntMan 类似的性能。  TGS 模式下生产作业的吞吐量比 AntMan 模式下高104.1% ~ 104.3% 采用 TGS 模式的机会作业的吞吐量比 AntMan 模式下高103% ~ 122%   与 AntMan 相比，TGS 提供了 GPU 共享的同样好处，并且对 DL 框架是透明的。      \rimage-20230508110928901\r\n大模型训练的GPU共享 讨论 ","date":"2023-05-06T15:38:04+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202305081343395.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0tgs%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】TGS论文阅读笔记"},{"content":"Aryl: An Elastic Cluster Scheduler for Deep Learning 摘要  训练和推理中的问题:  当流量负载较低时，推理集群的 GPU 利用率较低 由于缺乏资源，训练作业往往需要较长的排队时间   我们引入了 Aryl，一个新的集群调度器来解决这些问题。  Aryl 引入了容量贷款，将空闲推理 GPU 服务器贷款给训练工作。它进一步利用弹性扩展来扩展培训作业的 GPU 分配，以更好地利用借出的资源。 容量借贷和弹性扩展为集群管理带来了新的挑战。  当需要返回借出的服务器时，我们需要最小化作业抢占的数量 当更多的 GPU 可用时，我们需要将它们分配到弹性作业，并最小化作业完成时间(JCT)   Aryl使用基于原则的启发式方法来解决这些组合问题。  它引入了服务器抢占成本的概念，并在服务器回收期间使用贪婪的方法降低这一成本。 它进一步依赖于为弹性工作的每个额外工人定义的 JCT 缩减值，以多选择背包问题解决调度问题。   在64-GPU 测试平台上的原型实现和超过50,000个生产作业的15天跟踪的大规模模拟表明  Aryl 在平均排队时间和 JCT 方面带来了1.53 x 和1.50 x 的减少 集群调度器提高了高达26.9% 的集群使用率      引言   现在的通常做法\n  分别构建和管理两种类型的 GPU 集群\n 一种用于培训 一种用于推理    这是因为，对于相同的模型\n 推理比训练需要更少的计算和 GPU 内存，并且不太可能利用训练 GPU 的众多核心[11,35,39]。 推理集群通常使用较弱的 GPU，比如 Nvidia T4 训练集群使用 Nvidia V100和 A100    这种分离给双方都造成了问题， 尤其是对训练工作。\n 具体来说，由于白天的流量模式，推断集群的利用率通常很低(\u0026lt; 40%)。 与此同时，培训工作在开始之前经历了漫长的排队过程  从超过50,000个工作的15天的跟踪中可以看出，平均排队次数超过3,000次，95% 的排队次数接近10,000次。 排队时间长是由于高集群利用率和 GPU 资源碎片造成的。        为了解决上述问题，我们提出了容量贷款\n 以允许推理集群在低流量时期贷款空闲的 GPU 服务器来运行培训作业，并在推理工作负载再次增加时收回它们。  容量贷款缓解了推理的利用率问题和训练的排队问题。对 GPU 类型要求不严格的训练工作是可行的。 对于借用服务器，我们需要确保它们在可用时被培训作业迅速利用。   我们从弹性伸缩中获得灵感，用于培训工作以更好地使用借用服务器。  弹性伸缩允许正在运行的作业向外伸缩或向内伸缩，以更好地利用动态变化的资源池。 它还有助于减少排队延迟，因为弹性作业可以首先从少量工作线程开始，并在有更多资源可用时增加其工作线程。      容量加载和弹性伸缩为集群调度创造了新的自由度。随着我们在新的设计领域中的不断探索，我们遇到了一些新的挑战，这些挑战必须在我们获得收益之前加以解决。\n  首先，虽然贷款决策可以完全由推理集群调度程序来确保推理工作负载不受影响，但是回收更加复杂。\n 当推断集群需要回收一些租借的服务器时，训练调度程序必须抢占这些服务器上所有正在运行的作业。考虑到与抢占相关的高开销和长时间运行时间，调度程序必须仔细选择服务器，以最小化总的抢占。    其次，作业调度问题在具有弹性伸缩性的情况下会更加复杂。\n 资源配置必须考虑需求固定的非弹性工作和需求可变的弹性工作的组合。 我们发现经典的调度策略，如最短作业优先在弹性情况下不在有用，并在只有两个作业的情况下找到 JCT的最优解是困难的 给定分配结果，调度程序仍然需要确定worker-server的位置，以最小化碎片，由于容量借用导致服务器是GPU异构的。      我们解决这些挑战的关键直觉\n 优先考虑每个工作所需的最小资源而不是弹性需求 优先考虑专用训练服务器而不是借用推理服务器 这是有意义的，因为弹性工作的最小需求等同于非弹性工作，不分配资源是有害的，但弹性部分可以在以后实现，而不拖延工作。    因此，我们的解决方案遵循上述直觉，呈现出两阶段结构。\n 对于回收  我们首先杀死在租借服务器上运行的弹性工作线程，因为停止它们不会导致任何作业级别的抢占。 当抢占成为不可避免的，我们将问题描述为一个具有依赖项值的背包问题[32] ，并开发一种有效的启发式方法来解决它   对于资源配置  我们首先分配非弹性工作和弹性工作的基本需求，目的是创造尽可能多的工作。  第一阶段可以使用 SJF 来减少排队时间   然后，如果资源允许，我们扩展调度的弹性作业。  第二阶段则是一个多项选择的背包问题 ，以最小化运行时间 这在实践中通常可以使用动态规划来解决：  我们通过将非弹性作业放置到训练服务器 并尽可能将弹性作业放置到租借服务器 作业根据最佳拟合递减策略进行排序，以解决装箱和最小化碎片。          Aryl，这是一个新的集群调度器，可以实现弹性扩展的容量借贷。\n Aryl 有一个协调器，通过执行来自推断调度器的关于何时以及如何贷款或回收的指令，以及通过决定返回哪些借出的服务器以回收，来管理容量贷款。 然后作业调度器周期性地确定分配和放置，并根据资源和作业动态对新的和现有的弹性作业进行扩展。 为了实用起见，Aryl 只考虑大型 DNN 的弹性伸缩    我们将我们的贡献总结如下。\n 我们报告了培训和推理分组单独管理的问题，即推理分组的利用率低，培训分组的排队时间长，这些问题是从生产性的图形处理单元分组来衡量的。 我们提出集群级容量借贷和作业级弹性调度两个新的集群调度控制旋钮，以解决上述问题。 我们研究由此产生的集群调度问题，发展一种关键的直觉，优先考虑每项工作所需的最低资源，以解决弹性问题，并使用一种有原则的方法来描述和解决每个问题。 我们设计并实现了集成了我们的解决方案的新型集群调度器 Aryl。Aryl 与现有资源管理框架协同工作，可以部署。通过试验台试验和大规模仿真验证了其优越性能。    ","date":"2023-04-24T09:48:20+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202304240955302.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0aryl%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Aryl论文阅读笔记"},{"content":"安装k8s-1.18.9版本的脚本   下载之后最好修改\n KUBE_VERSION=\u0026ldquo;1.18.9\u0026rdquo; 以安装自己想要的版本 MASTER1_IP= 自己的master IP NODE1_IP= 集群的节点IP    可以自定义修改， 但是也可以不动\n POD_NETWORK=\u0026ldquo;10.244.0.0/16\u0026rdquo; SERVICE_NETWORK=\u0026ldquo;10.96.0.0/16\u0026rdquo;    网络插件\n 这个脚本默认使用flannel 如果使用calico记得修改calico脚本里的pod network    github\n  项目地址：Tweakzx/k8s-scripts (github.com)\n  wget https://raw.githubusercontent.com/Tweakzx/k8s-scripts/main/k8s-install.sh     # !/bin/bash KUBE_VERSION=\u0026#34;1.18.9\u0026#34; KUBE_VERSION2=$(echo $KUBE_VERSION |awk -F. \u0026#39;{print $2}\u0026#39;) DOCKER_VERSION=\u0026#34;\u0026#34; MASTER1_IP=10.10.0.25 NODE1_IP=10.10.0.26 MASTER1=master NODE1=node1 POD_NETWORK=\u0026#34;10.244.0.0/16\u0026#34; SERVICE_NETWORK=\u0026#34;10.96.0.0/16\u0026#34; . /etc/os-release IMAGES_URL=\u0026#34;registry.aliyuncs.com/google_containers\u0026#34; CRI_DOCKER_VERSION=0.2.6 CRI_DOCKER_URL=\u0026#34;https://ghproxy.com/https://github.com/Mirantis/cri-dockerd/releases/download/v${CRI_DOCKER_VERSION}/cri-dockerd_${CRI_DOCKER_VERSION}.3-0.ubuntu-${UBUNTU_CODENAME}_amd64.deb\u0026#34; LOCAL_IP=`hostname -I |awk \u0026#39;{print $1}\u0026#39;` COLOR_SUCCESS=\u0026#34;echo -e \\\\033[1;32m\u0026#34; COLOR_FAILURE=\u0026#34;echo -e \\\\033[1;31m\u0026#34; END=\u0026#34;\\033[m\u0026#34; color () { RES_COL=60 MOVE_TO_COL=\u0026#34;echo -en \\\\033[${RES_COL}G\u0026#34; RES_COL=50 SETCOLOR_SUCCESS=\u0026#34;echo -en \\\\033[1;32m\u0026#34; SETCOLOR_FAILURE=\u0026#34;echo -en \\\\033[1;31m\u0026#34; SETCOLOR_WARNING=\u0026#34;echo -en \\\\033[1;33m\u0026#34; SETCOLOR_NORMAL=\u0026#34;echo -en \\E[0m\u0026#34; echo -n \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; $MOVE_TO_COL echo -n \u0026#34;[\u0026#34; if [ $2 = \u0026#34;success\u0026#34; -o $2 = \u0026#34;0\u0026#34; ] ;then ${SETCOLOR_SUCCESS} echo -n $\u0026#34;OK\u0026#34; elif [ $2 = \u0026#34;failure\u0026#34; -o $2 = \u0026#34;1\u0026#34; ] ;then ${SETCOLOR_FAILURE} echo -n $\u0026#34;FAILED\u0026#34; else ${SETCOLOR_WARNING} echo -n $\u0026#34;WARNING\u0026#34; fi ${SETCOLOR_NORMAL} echo -n \u0026#34;]\u0026#34; echo } install_prepare () { ${COLOR_SUCCESS}\u0026#34;开始处理环境的基本配置...\u0026#34;${END} cat \u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 127.0.0.1 localhost $MASTER1_IP $MASTER1 $NODE1_IP $NODE1 EOF hostnamectl set-hostname $(awk -v ip=$LOCAL_IP \u0026#39;{if($1==ip)print $2}\u0026#39; /etc/hosts) swapoff -a sed -i \u0026#39;/swap/s/^/#/\u0026#39; /etc/fstab cat \u0026gt;/etc/modules-load.d/k8s.conf \u0026lt;\u0026lt;EOF modprobe overlay modprobe br_netfilter EOF cat \u0026gt;/etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF sysctl --system color \u0026#34;安装前准备完成！\u0026#34; 0 sleep 1 } install_docker () { ${COLOR_SUCCESS}\u0026#34;开始安装docker...\u0026#34;${END} apt update apt -y install docker.io || { color \u0026#34;安装Docker失败！\u0026#34; 1; exit 1; } cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://reg-mirror.qiniu.com\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34; ], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF systemctl restart docker.service docker info \u0026amp;\u0026amp; { color \u0026#34;安装 Docker 成功\u0026#34; 0; sleep 1; } || { color \u0026#34;安装 Docker 失败！\u0026#34; 1; exit; } } install_kubeadm () { ${COLOR_SUCCESS}\u0026#34;kubeadm依赖安装...\u0026#34;${END} apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt; EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrorS.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-cache madison kubeadm | head ${COLOR_FAILURE}\u0026#34;5秒后即将安装： kubeadm-\u0026#34;${KUBE_VERSION}\u0026#34; 版本......\u0026#34;${END} ${COLOR_FAILURE}\u0026#34;如果想安装其它版，请桜 ctrl + c 键退出，修改版本再执行\u0026#34;${END} sleep 6 #安装指定版本 apt install -y kubeadm=${KUBE_VERSION}-00 kubelet=${KUBE_VERSION}-00 kubectl=${KUBE_VERSION}-00 [ $? -eq 0 ] \u0026amp;\u0026amp; { color \u0026#34;安装 kubeadm 成功！\u0026#34; 0; sleep 1;} || { color \u0026#34;安装 kubeadm 失败！\u0026#34; 1; exit 2; } } # Kubernetes -v1.24之前版本无需安装 cri-dockerd  install_cri_dockerd () { ${COLOR_SUCCESS}\u0026#34;开始安装cri-dockerd...\u0026#34;${END} [ $KUBE_VERSION2 -lt 24 ] \u0026amp;\u0026amp; return if [ ! -e cri-dockerd_${CRI_DOCKER_VERSION}.3-0.ubuntu-${UBUNTU_CODENAME}_amd64.deb ] ;then curl -LO $CRI_DOCKER_URL || { color \u0026#34;下载cri-dockerd失败！\u0026#34; 1; exit 2; } fi dpkg -i cri-dockerd_${CRI_DOCKER_VERSION}.3-0.ubuntu-${UBUNTU_CODENAME}_amd64.deb sed -i -e \u0026#39;s#ExecStart=.*#ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=\u0026#39;\u0026#34;$IMAGES_URL\u0026#34;\u0026#39;/pause:3.7#g\u0026#39; /lib/systemd/system/cri-docker.service systemctl daemon-reload systemctl restart cri-docker.service } kubernetes_init () { ${COLOR_SUCCESS}\u0026#34;开始初始化k8s...\u0026#34;${END} kubeadm init \\ --kubernetes-version=v${KUBE_VERSION} \\ --apiserver-advertise-address=$(MASTER1_IP) \\ --image-repository registry.aliyuncs.com/google_containers \\ --service-cidr=${SERVICE_NETWORK} \\ --pod-network-cidr=${POD_NETWORK} \\ --v=5 if [ $? -eq 0 ]; then mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config color \u0026#34;初始化 k8s 成功！\u0026#34; 0 else color \u0026#34;初始化 k8s 失败！\u0026#34; 1 fi } reset_kubernetes () { #To do kubeadm reset rm -rf /etc/kubernetes/* rm -rf $HOME/.kube/ } configure_network () { kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml } remove_docker () { apt-get autoremove docker docker-ce docker-engine docker.io containerd runc dpkg -l |grep ^rc|awk \u0026#39;{print $2}\u0026#39; |sudo xargs dpkg -P rm -rf /etc/systemd/system/docker.service.d rm -rf /var/lib/docker } install_helm () { wget https://mirrors.huaweicloud.com/helm/v3.1.1/helm-v3.1.1-linux-amd64.tar.gz tar -zxvf helm-v3.1.1-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm } remove_kubernetes () { #To do apt remove -y kubeadm kubectl kubelet sudo rm -rf /etc/kubernetes /var/lib/kubernetes /var/lib/etcd /var/lib/kubelet /var/run/kubernetes ~/.kube sudo ip link delete cni0 sudo ip link delete flannel.1 } main () { PS3=\u0026#34;请选择编号（1-8）\u0026#34; options=(\u0026#34;初始化kubernetes集群\u0026#34; \u0026#34;加入kubernetes集群\u0026#34; \u0026#34;退出kubernetes集群\u0026#34; \u0026#34;配置网络\u0026#34; \u0026#34;安装helm\u0026#34; \u0026#34;删除kubernetes\u0026#34; \u0026#34;删除docker\u0026#34; \u0026#34;退出本程序\u0026#34;) select item in \u0026#34;${options[@]}\u0026#34;; do case $item in \u0026#34;初始化kubernetes集群\u0026#34;) install_prepare install_docker install_kubeadm install_cri_dockerd kubernetes_init break ;; \u0026#34;加入kubernetes集群\u0026#34;) install_prepare install_docker install_kubeadm install_cri_dockerd ${COLOR_SUCCESS}\u0026#34;加入kubernetes前的准备工作已完成，请执行kubeadm join命令！\u0026#34;${END} break ;; \u0026#34;退出kubernetes集群\u0026#34;) reset_kubernetes break ;; \u0026#34;配置网络\u0026#34;) configure_network break ;; \u0026#34;安装helm\u0026#34;) install_helm break ;; \u0026#34;删除kubernetes\u0026#34;) reset_kubernetes remove_kubernetes break ;; \u0026#34;删除docker\u0026#34;) remove_docker break ;; \u0026#34;退出本程序\u0026#34;) exit ;; *) echo \u0026#34;invalid option $REPLY\u0026#34;;; esac done } main 217,4 Bot ","date":"2023-04-21T16:06:05+08:00","permalink":"https://tweakzx.github.io/p/kubernetesubuntu%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","title":"【Kubernetes】ubuntu脚本安装k8s集群"},{"content":"Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs 摘要   虽然最近的深度学习工作负载调度器表现出很好的性能，但在实际应用中很难部署它们，是由于\n 缺乏灵活的入侵方式、 过高的集成和维护成本 有限的可伸缩性 不透明的决策过程    Lucid: 基于可解释模型的非侵入式深度学习工作负载调度器 。\n 它由三个创新模块组成。  首先，为了有效地收集作业度量和及时调试作业反馈，引入了一个二维优化剖析器。 其次，Lucid 利用一种惰性包装策略来规避干扰。 第三，Lucid 基于估计的作业优先级值和共享分数来编排资源，以实现有效的调度。   此外，Lucid 通过设计良好的系统优化器促进模型性能维护和系统透明调整。 我们的评估表明，与最先进的抢占式调度器 Tiresias 相比，Lucid 将平均作业完成时间减少了1.3倍。此外，它为实际部署提供了明确的系统解释和优秀的可伸缩性。    Introduction 过往的GPU集群或者GPU集群调度器， 从两个方面看存在G1-G5五个鸿沟， 以致于无法实际部署。\n  大型多租户DL集群\n [Tiresias, QSSF商汤集群, Antman,]    集群调度器\n [ONES, Tiresias, QSSF（商汤）,Optimus, Pollux, Gandiva, Antman,, Horus]    首先，为了获得更好的系统性能，大多数现有的方法依赖于支持抢占的调度范例，如迁移[Gandiva ]、弹性[AFS ]和自适应训练[Pollux ]。然而，由于其不可避免的侵入机制，它们在部署方面遇到以下障碍:\n G1: 不灵活和容易出错。为了实现弹性训练和作业检查点，现有的调度器要求用户导入特定的库并修改代码来实现这些机制[AFS,Gavel,Optimus,Pollux,Gandiva]。这种用户代码入侵方法不仅给用户带来了复杂的模型训练控制逻辑负担，而且还可能产生不确定性错误。此外，由于调度器接管了培训工作流，它们还极大地限制了用户定制代码的灵活性。正如微软[Singularity]所说，“当今大多数 DNN 培训工作量本身是不可检查点或可调整的。”泛化问题也阻碍了入侵调度器的实际应用。 G2: 整合和维护费用高。将研究原型转移到生产级系统中并非易事。通常，将调度程序设计集成到商业或开放源码集群管理系统中需要一个专家团队付出巨大的努力和成本来处理所有可能的问题。此外，为了支持高级调度特性，一些调度程序[AFS,Singularity,Gandiva]需要修改底层 DL 框架(例如 Pytch [Pytorch])或 CUDA 库[94]的源代码。它们需要持续维护，以适应 DL 生态系统的快速版本迭代。过高的集成和维护成本对大多数公司和研究机构来说是不切实际的。 G3: 适应性训练的模型质量退化。为了争取极端的培训效率，一些调度程序[11,Aryl,Pollux]分配的资源，适应性地调整工作批量和学习率。然而，就验证性能而言，这可能会降低最终模型的质量[51,108]。在商业应用中，微小的质量改进可以显著提高客户参与度和公司利润[43]。因此，由于退化问题，开发人员不倾向于采用这种机制。    其次，许多调度器采用基于机器学习(ML)的方法[QSSF,60,74,92,SLURM]或基于优化的方法[30,65,Gavel,107]来寻找最优调度策略。然而，它们在实践中也存在重大缺陷:\n **G4: 可伸缩性有限。**随着工作负载变得更加密集，集群变得更大规模，这些调度器[Poster Abstract,30,66,Gavel,74,92,93]在部署到生产级系统时遇到了可伸缩性瓶颈。例如，Gavel [Gavel]花费数千秒通过线性规划解决2048年的工作分配问题，这需要很长时间才能满足实时需求。基于强化学习(RL)的调度器也面临同样的问题: Metis [93]只能处理几十个作业，而生产集群可以同时运行几千个作业 G5: 决策不透明，难以调整。大多数基于 ML- 的调度器是建立在黑盒模型之上的，比如随机森林(Random Forest，RF)[32,52]、梯度提升决策树(GBDT)[QSSF,SLURM]和 RL [74,92]。开发人员主要关注于改进关键的调度指标(例如，完成时间) ，而忽略了它们的可解释性。这些模型的预测过程对人类来说是难以理解的[33,55,81]。由于这种不透明性，系统操作员不能保证模型预测是可靠的，也没有足够的信心来部署它们。此外，即时调试和系统配置调优也是基于 ML- 和基于优化的调度程序面临的重大挑战。不适当的修改可能导致严重的性能降低[Primo]    我们设计了 Lucid，一个非侵入性和透明的调度器，它可以提供比抢占式和侵入式调度器更好的性能。\n Lucid 的核心设计源于以下三点见解。  首先，以非侵入性的方式解决集群 GPU 利用率不足的问题是可行的。由于 GPU 在生产级 DL 培训集群中通常未得到充分利用[QSSF,48,95] ，现有的 DL 调度程序打包作业以通过侵入式方式提高利用率[10,Gavel,Gandiva,SLURM,Salus]。然而，通过全面分析工作配置，我们发现有可能实现有效的工作打包，而不会造成任何干扰。 其次，可以根据以前的工作历史进行工期预测。由于大多数工作负载遵循循环模式，并且用户倾向于多次提交相似的任务[QSSF,95] ，因此我们可以根据新任务的概要特征和历史提交数据来估计其持续时间。 第三，系统的可解释性是必不可少的，可以提高性能。对系统行为的全面了解可以增强操作员对实际部署的信心，并提供透明的性能调优。   Lucid的三个核心调度模块  我们提出了一个二维优化的非侵入性作业剖析器来收集作业资源的使用特征，包括 GPU 的利用率， GPU内存足迹和GPU内存使用。它实现了及时的调试作业反馈和高效的作业度量收集，其中以非侵入的方式进行分析只需要几分钟。 在作业打包阶段，我们引入了仿射作业对绑定器的惰性和动态打包策略，以规避干扰，最大限度地提高集群范围内的作业速度 工作负载评估模型为下列资源协调器的每个作业分配一个优先级值。此外，Lucid 集成了一个用于模型性能维护的更新引擎和透明调整和系统增强的系统调谐器。   Lucid实现了以下几个特性  **A1: 高效率的非侵入性时间安排。**Lucid 的工作流程是无需抢占的，不需要侵入用户作业或 DL 框架的代码。同时，Lucid 的性能优于几个 SOTA 入侵调度器。 **A2: 部署费用低。**Lucid 可以很容易地集成到现有的商业或开源集群管理系统中(例如 Slurm [101] ，Kubernetes [15])。它也不需要持续维护 DL 框架或 CUDA 库更新。 **A3: 保留模范业绩。**用户完全控制他们的模型和 Lucid 从来没有篡改模型配置，完全保持他们的原始质量 **A4: 大规模集群的可伸缩性。**即使对于大量复杂的工作负载，系统也可以快速地获得最优的调度策略(在几毫秒内)。 **A5: 透明的系统调整。**所有的模块都是可解释的，帮助开发人员进行指导系统配置调整，并带来额外的改进。    背景与Motivation 背景  DL 训练。DL 模型在迭代过程中学习它的参数(即权重)[58,Gandiva]。在每次迭代中，它都会处理一批带标签的数据，通过梯度下降法更新模型权重。整个训练过程通常由许多小批量迭代组成，可以持续数小时到数天，可以通过检查点进行抢占和恢复[Prague,Capuchin]。基于重复模式，操作员可以对几个迭代进行剖析，以获得作业的资源利用特征。与之前基于概要分析的 DL 作业调度程序[30,31,62]依赖于侵入式库来检查作业执行状态不同，Lucid 非侵入式地收集度量。 DL集群调度。建立多租户 DL 集群以促进 DL 模型的开发是科技公司和研究机构的普遍做法。在许多公司[QSSF,48,95]中，集群通常被划分为几个虚拟集群(Virtual Clusters，VC) ，专门用于不同的产品组。用户通过相关配置(例如 GPU 需求、 CPU 需求、作业名称)向集群提交 DL 培训作业。  采用 DL 集群调度器调节资源和作业执行。为了提高资源利用率并使平均 JCT 最小化，大多数现有的 DL 集群调度器[11,31,Optimus,Pollux,Gandiva,Antman,SLURM]都是侵入性的: 它们通过修改 DL 框架或依赖于用户代码自适应来实现一些高级特性。有两个常见的高级功能: (1)作业打包(即，作业同位，图形处理器共享)允许多个任务使用 NVIDIA MPS [5]或 MIG [4]技术共享图形处理器。(2)弹性培训动态调整 GPU 工人的规模，甚至自适应地修改批量和学习率，以加快工作培训进度[11,Pollux]。然而，它们有几个明显的缺点，如1(G1 something G3)所述。\nDL集群特性  **低 GPU 利用率。**最近的研究[Gandiva,Antman,SLURM,Salus]显示了一个普遍的现象，即大多数 GPU 在 DL 集群中未得到充分利用。图1(a)显示从阿里巴巴数据中心收集的每周图形处理器使用量统计数据的累积分布函数。 高度倾斜的工作负荷分布。实际生产 DL 集群[QSSF,48,95]呈现类似的工作负载分布: (1)小规模。在 Microsoft [48]和 SenseTime [QSSF]中，超过95% 的作业是单节点作业(在4/8 GPU 内)。(2)循环。大多数作业(something 90%)是重复的超参数搜索作业[95,104]。(3)调试。大多数作业都是出于调试目的的短期作业，微软近70% 的资源被失败或取消的作业所占用。用户希望及时获得调试作业反馈。然而，现有工作往往忽视了工作负载的多样性，缺乏针对调试工作的具体设计。  非侵入调度的一些机会  工作装箱干扰的特征。为了了解工作包装的干扰效应，我们对不同工作量(表一)进行了广泛的分析，包括计算机视觉、自然语言处理、强化学习和推荐等不同领域的不同配置。 非侵入式干扰感知作业包装。所有现有的支持打包的 DL 调度程序都依赖于侵入式范例。具体来说，他们修改 DL 框架[Gandiva,Antman,Salus]或者需要用户代码适应[10,Gavel,SLURM]来实现内省作业打包。然而，我们发现无干扰地实现工件包装是可行的。根据我们的角色塑造，非侵入性的图形处理器利用率指标应该足以让调度程序做出打包决定(图2(a)) ，打包策略适用于所有单节点作业(图3(b)) ，覆盖超过95% 的工作负载(2.2)。值得注意的是，GPU 利用率定义为一个给定的采样间隔中一个或多个内核在 GPU 上执行的时间百分比，而不是活动单位百分比[6,SLURM]。除此之外，我们还采用了另外两个非侵入性的特性，它们也可以帮助我们做出更精确的决定: GPU 内存利用率(在过去的采样期间内存被读写的百分比)和 GPU 内存(GPU 上的内存占用率)。 工作时间估计。最近来自 SenseTime 和阿里巴巴的数据数据聚类发现，大多数工作负载都有反复出现的模式，用户往往会多次提交类似的任务。这启发我们利用历史日志数据来预测作业持续时间。此外，对作业资源利用率的特征分析还可以帮助我们更精确地将它们与以前的作业匹配，从而有助于更准确地预测和更好的调度策略。  系统设计 总览  原则及目标。对于实用和简单的系统采用，  Lucid 遵循三个设计原则:  非侵入性。整个调度工作流遵循无抢占方式，不需要用户操作和 DL 框架修改(解决 G1 something G3)。 可伸缩性。该系统可以快速获得大量复杂工作负载的调度策略(求解 G4)。 可解释性。所有的模块都是透明的，可以由集群运营商进行清晰的调整(解决 G5)。我们的主要目标是最小化培训工作量的平均 JCT。这对于 DL 用户来说尤其可取。   此外，Lucid 还提高了资源利用率，并提供了及时的调试反馈。我们未来的工作旨在为更多的调度目标服务，比如公平性和服务水平保证。   架构与工作流。  \rimage-20230403132927064\r 图4说明了 Lucid 的架构以及调度工作流。它由用于工作负载调度的三个关键调度器模块(蓝色块)以及用于性能增强和维护的两个系统优化器(紫色块)组成。对于每个模块，都有相应的可解释模型(橙色块)负责预测关键指标，以协助调度。Lucid 的系统工作流以黑色箭头表示。具体来说，在分配给目标集群之前，需要首先分析作业(something)。我们采用非侵入性作业剖析器来过滤大部分的测试和调试作业。同时，该模块还记录了一般训练工作的资源使用情况的统计信息，并把它们分为不同的类别(something)。在分析之后，我们设计一个仿射作业对绑定器来确定是否以及如何打包各种作业。它根据未来集群吞吐量预测(something)动态改变打包策略。资源协调器根据概要分析和用户提供的特性，为每个作业分配一个优先级值，并选择要分配的作业(something)。   模块间依赖。通过所有系统模块的协作，Lucid 实现了预期的调度性能。没有其他模块的帮助，每个单独的模块不能提供所需的性能(4.5)。我们在图4中用红色箭头描述了它们之间的交互: 编配器使用 Profiler 中的特性来更好地估计持续时间。如果没有特征描述，Lucid 无法精确匹配以前的经常性工作。吞吐量预测模型不仅决定了 Binder 内部的打包策略，而且还有助于探查器集群扩展，从而有效地处理突发作业提交案例。在没有吞吐量预测模型的情况下，作业必须承受较高的分析排队延迟。粘合剂需要从编配器估计持续时间，以优化包装决策。因为长期的作业包装有时会恶化线头阻塞问题，延长 JCT，所以在作业包装过程中注意时间是非常重要的。  非侵入Job剖析器 Lucid 采用作业剖析机制优化后续的分配策略。非侵入性作业分析器为每个作业设置短期运行时限制 Tpro f，并收集与作业分析相关的硬件指标，包括 GPU 利用率、 GPU 内存占用率和 GPU 内存利用率。这些可以方便地通过 NVIDIA-SMI [6]或 DCGM [3]以非侵入性的方式进行测量。然后剖析器将这些特征发送到包装分析模型(3.5.1) ，该模型遵循非侵入性原则，主动预测包装的有效性，而不是在同位后测量吞吐量。为了方便随后的工作打包和资源分配，Lucid 不预测工作搭配的数值结果，而是将工作分为三个不同的类别(Tiny、 Medium 或 Jumbo) ，并为每个工作分配一个共享分数(SS)来表示其类别。具体来说，Tiny (SS = 0)作业指的是那些资源利用率极低的作业，它们几乎不会受到同地放缓的影响。相反，Jumbo (SS = 2)作业需要很高的资源利用率，关于它们的同位性的决策应该谨慎。中型(SS = 1)工作的打包对他们的培训速度影响相对较小。\n为了提高剖析效率，我们提出了一种二维优化剖析策略，该策略结合了工作负载剖析的空间考虑以减少排队延迟，以及剖析器集群的时间考虑以最大限度地提高资源效率: 空间感知剖析。由于分析时间短 Tpro f，工作负载的时间尺度应该相似，所以我们可以专注于优化他们的空间尺度调度，这是以前基于分析的 DL 工作负载调度器从未考虑过的[30,31,62]。通过对请求较少资源的作业进行优先排序，可以有效地解决小规模剖析集群中的线头(head-of-line，HOL)阻塞问题。算法1显示了我们的空间感知分析算法的伪代码。由于有限的 GPU 资源是 DL 培训工作的典型瓶颈，我们根据它们的 GPU 需求对工作进行分类(第4行)。然后，我们采用独占和合并分配策略(第8行)来减少资源碎片化[QSSF]。时间感知缩放。为了保证分析的资源可用性，分析集群通常与主计算集群解耦。然而，由于作业提交的时变模式，静态分析配置可能导致严重的队列延迟和资源不平衡。为此，我们提出了基于当前状态动态调整作业规模限制 Npro f、分析时间限制 Tpro f 和分析集群容量 Cpro f 以及未来集群范围作业吞吐量预测的时间感知伸缩算法。例如，当一系列作业在短时间内发生时，分析器将暂时从相对闲置的 VC 中借出一些节点，并减少 Tpro f。当集群吞吐量降低，突发作业队列消除时，将返回资源。\n注意，大多数作业都需要进行分析，除了超过作业规模限制的大型分布式作业。Lucid 在不进行分析的情况下即时收集这些大型作业的指标。此外，我们假设作业初始化或数据移动时间不超过 Tpro f，否则探查器无法获得正确的资源消耗特性。为了支持这些作业，运营商应该相应地延长 Tpro f 设置，或者赋予用户将其作业标记为“长冷启动”作业的权利，以扩展 Tpro f。\n与分析带来额外的队列延迟和资源需求的普遍观点相反，我们的分析机制具有以下优势:\n (a)及时反馈。由于当前部署的集群的运行时不可知的调度范例，许多短期调试作业都会遭受严重的队列延迟(2.2)[QSSF,48,95]。同时 Lucid 的分析器可以很好地解决这个问题，提高工作的公平性。 (b)轻而易举。Lucid不需要依赖于任何侵入性指标(例如，作业进度、每次迭代的时间) ，并且不需要修改任何代码。 (c)提高系统性能。分析器可以过滤掉主集群的大多数失败或调试作业，从而通过减少优化空间来显著促进调度优化。  仿射工作绑定 与以前使用打包的调度程序[10,Gavel,Gandiva,Antman,,SLURM]不同，这些调度程序应用用户代码或 DL 框架侵入方法来识别具有干扰的作业对，Lucid 根据概要特征根据非侵入原则确定打包作业对。为此，Lucid 在仿射作业对绑定器中设计了以下两种策略。\n 懒惰包装。Lucid 只包含不会造成干扰的任务。虽然这种不活跃的方式可能会错过一些优化机会，但它可以有效地避免干扰，并为用户提供包装激励。具体来说，惰性打包为每个 GPU 设置 GPU 共享容量(GSS) ，它将打包作业的共享得分的总和限制在 GSS 以下(默认值 = 2)。此外，Lucid 为作业打包设置了以下规则:  (1)对 GPU 内存使用采取严格限制，以防止内存不足(OOM)问题; (2)由于并行培训的滞后效应，它从不将不同 GPU 资源需求的作业打包; (3)在一组 GPU 上合并多达两个作业，因为打包三个作业通常不会带来额外的好处[Gavel] ; (4)如果检测到不稳定的资源利用模式，它会自省地驱逐打包的作业; (5)由于网络争用，分布式作业不会默认打包。图5描述了表1中列出的所有可能的作业对组合的绑定决策。显然，Lucid 有效地识别了干扰很小的作业对，其中超过98.1% 的可打包作业对是无干扰的(阈值: 标准化速度的0.85) ，并且在这种非干扰策略下发现了87.0% 的打包机会。   动态策略。现有的工程[10,Gavel,Gandiva,Antman,,SLURM]通常在没有集群意识的情况下，对作业打包保持一个固定的策略。然而，大多数集群[QSSF,79]在作业提交率(吞吐量)和集群利用率方面呈现日常模式。当集群相对空闲时，忽视集群吞吐量可能导致不必要的作业打包，延长作业培训的进度。  因此，我们开发吞吐量预测模型(3.5.2)来对集群作业数量和 GPU 请求吞吐量进行时间序列预测。基于其预测和当前集群状态，当当前集群吞吐量相对较低(可定制)且未来不太可能增加时，我们可以动态调整打包策略从默认模式(GSS = 2)到无情模式(GSS = 1) ，甚至可以暂时禁用作业共享以提高作业完成速度。\n资源调度器 为了使平均 JCT 最小化并提高资源利用率，Lucid 使用了资源协调器来管理集群资源和协调工作负载执行。主要的挑战是解决 HOL 阻塞问题，其中长时间运行的作业对 GPU 具有独占访问权，直到它们完成，将短期作业保持在队列中等待[Gandiva]。经验法则是优先考虑短期工作，如最短工作优先(SJF)政策[31] ，而在现实中不可能获得完美的工作持续时间信息。此外，由于 DL 培训工作的高取消率和失败率，以前的侵入性预测范式[Gavel,Optimus,Gandiva]即迭代时间测量可能会产生误导[QSSF,48]。然而，正如在2.3中提到的，大多数工作负载是重复的，我们可以利用先前的数据来训练工作负载估计模型(3.5.3) ，以提供调度的作业持续时间估计。\n资源协调器综合考虑了 DL 作业的时间和空间方面。算法2说明了作业调度和资源分配过程。首先，Workload Estimate Model 预测每个作业的持续时间，然后将预测乘以 GPU 的数量作为作业的优先级值(第4行)。额外考虑作业资源消耗(GPU 需求)可以有效地提高调度性能[31,QSSF]。接下来，根据优先级值对作业队列进行排序。然后检查当前是否允许工作包装(第6行)。(1)如果没有，则以排他方式分配作业(第16行)。我们应用合并安置策略，以最大限度地提高每个工作的培训速度，并减少资源分散。(2)如果是，我们打包适合同位的作业，并消除剩余运行时很少的作业(第7行)。此外，对于没有历史信息的新作业，Lucid 可以根据用户的历史行为生成新作业的估计。如果它是由一个新用户提交的，Lucid 可以使用具有相同 GPU 需求的所有作业的平均持续时间作为持续时间预测[QSSF]。更多地，当新作业终止之后，UpdateEngine 将收集其信息并使用最新数据对模型进行微调。通过这种方式，可以在排队和干扰较少的情况下有效地调度作业。\n可解释的模型 装箱分析模型 受 LinnOS [35]的启发，我们将 SSD 存储延迟预测建模为一个二进制分类问题，我们引入分享得分方案将干扰预测简化为一个三进制分类问题，以获得高可扩展性和可理解性。具体来说，对于每个工作负载组合(表1) ，我们测量独占和相互协同位置吞吐量，以获得标准化的速度。然后根据模型配置对其他配置的影响，给每个配置分配一个共享得分。如果一个作业的平均标准化速度大于可定制的小作业阈值(例如0.95) ，则该作业被视为 Tiny; 如果速度介于小作业阈值和中等作业阈值之间，则被视为 Medium。否则，作业将被标记为 Jumbo。我们采用决策树(DT)模型进行工作类别预测，以发现资源使用与工作共位特征之间的共同关系。DT 可以提供一个透明的决策过程和优秀的预测准确性对这一任务。此外，它需要较少的训练数据，并在动态系统环境下表现稳健[Primo]。我们利用最小的成本-复杂性修剪[14]来修剪学习树，以获得一个紧凑和准确的模型。\n\rimage-20230403092756533\r\n解释: 图6展示了学到的打包分析模型。除了资源使用模式(UG、 MG 和 UM)之外，Lucid 还支持一个可选的指标(A) ，允许用户指定是否在其作业提交命令中应用混合精度培训(例如 torch.cuda.amp)。从这个树中，我们可以清楚地理解 Lucid 是如何对每个作业进行分类的。我们还可以通过观察每个决策路径的深度(箭头线)和右侧图形(特征基尼重要性)来获得对整体模型行为的直观认知。显然，UG 对同居行为的影响最大。其他指标也有助于做出精确的预测\n吞吐分析模型 我们采用了一种新的加性模型算法 GA2M [59,69]来进行聚类吞吐量预测。 $$ 𝑦 = 𝜇+\\sum 𝑓_𝑖 (𝒙^𝑖) + \\sum 𝑓_{𝑖𝑗}(𝒙^𝑖, 𝒙^𝑗), $$ 其中 μ 是截距(训练数据的平均目标值) ，fi (·)表示特征 i 和 j 的相互作用效应。由于每个形状函数是一元或二元的，它们的组合是可加的，因此它为预测过程提供了全面的解释。为了获得准确的未来吞吐量预测，我们通过特征工程提取了与时间相关的数据，如集群 GPU 需求和任务提交的趋势(增加或减少)和季节性(周期性模式)。具体来说，我们编码重复的模式(例如，小时，日期)来探索周期性的变化。此外，我们还计算了不同滚动窗口大小(如1小时)下的平均、中位数和加权软总和吞吐量值。\n\rimage-20230403092745150\r\n解释: 图7(a 和 b)给出了每个特征重要性和所学形状函数的全局解释。它描绘了从土星轨迹学习的模型，其表现优于一系列复杂的黑盒模型(表7)。从图7(a)中，我们发现与1小时前相关的小时和一系列增强特性在模型预测中起着最重要的作用。此外，图7(b)演示了小时特性的学习形状函数，其中每个 bin 表示一天中不同的小时，除了 bin 0被赋予一个默认值。该图显示了一个明显的日常模式，与我们的经验非常吻合，为集群配置调整提供了可靠和准确的建议。\n负载时间预测模型 工期预测采用 GA2M 模型。具体来说，该模型从跟踪中提取所有特征(例如，用户名、作业 ID、 GPU 需求)和实际作业持续时间，并对这些分类特征进行编码。对于极其稀疏和高维的特征，如职位名称，我们利用莱文斯坦距离将它们转换为相对密集的数值，并利用亲和传播将类似的数值桶化。对于诸如工作提交时间之类的时间特性，我们将它们解析为几个时间属性，例如月或小时。\n解释: 图7(c)展示了 SenseTime 中来自Venus集群的一个作业预测的特征解释[QSSF]。预测结果是每个特征得分和截距常数的总和。通过局部解释，开发人员可以清楚地检查每个预测的模型行为。\n系统优化 系统调优 集群调度程序通常包含由系统操作员调整的多个参数，以提高性能或实现不同的调度目标。调优这些参数需要丰富的领域知识和手工操作。不适当的调整可能导致严重的性能下降。不同公司和研究所的 DL 集群有不同的工作负载类型和分布。因此，为了获得最佳的调度性能，必须进行相应的手动系统调整。由于数据驱动策略的特性，Lucid 可以通过以前的作业和基于集群信息的模拟进行明确的调整。此外，为了优化可解释模型的性能，我们采用了池相邻违反者(PAV)[8]算法来对学习提出单调约束[Primo]\n更新引擎 在实际的生产级集群中，环境是动态变化的，会带来工作负载和集群分布的漂移。因此，需要频繁地对模型进行微调或再训练，以解决由于模型陈旧而引起的性能恶化问题。为此，我们设计了更新引擎以适应更改。它收集实时系统状态、作业日志，并使用最新数据定期微调 Lucid 模型。\n","date":"2023-04-01T19:51:18+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202304012045098.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0lucid%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Lucid论文阅读笔记"},{"content":"生成客户端证书 etcd 的证书包括了多种类型的证书，其中包括 peer.crt 和 etcd-client.crt 两种证书。peer.crt 证书是用于 etcd 集群中节点之间的身份验证和加密通信的，而 etcd-client.crt 则是用于客户端与 etcd 之间的身份验证和加密通信的。\n如果你已经生成了 etcd 集群节点之间的证书，那么你可以使用以下命令生成 etcd-client.crt：\n$ openssl genrsa -out etcd-client.key 2048 $ openssl req -new -key etcd-client.key -out etcd-client.csr \\  -subj \u0026#34;/CN=etcd-client/O=example.com\u0026#34; $ openssl x509 -req -in etcd-client.csr -CA ca.crt -CAkey ca.key \\  -CAcreateserial -out etcd-client.crt -days 365 其中，ca.crt 和 ca.key 是用于签名证书的根证书和私钥，需要和 etcd 集群中节点之间的证书一起使用。etcd-client.key 是生成的客户端证书的私钥，etcd-client.csr 是证书签名请求，etcd-client.crt 是签名后的证书。\n在生成 etcd-client.crt 之后，你可以将其和根证书一起用于访问 etcd，例如\nimport etcd3 etcd_host = \u0026#39;https://192.168.0.10:2379,https://192.168.0.11:2379\u0026#39; ca_cert = \u0026#39;/path/to/ca.crt\u0026#39; client_cert = \u0026#39;/path/to/etcd-client.crt\u0026#39; client_key = \u0026#39;/path/to/etcd-client.key\u0026#39; # 创建 etcd3 客户端 client = etcd3.client( host=etcd_host, ca_cert=ca_cert, cert_key=client_key, cert_cert=client_cert ) # 访问 etcd 的 API key = \u0026#39;/registry/services/specs/default/kubernetes\u0026#39; value, metadata = client.get(key) # 打印结果 print(value) 注意，在使用 openssl 生成证书时，应该按照实际情况修改证书的主题和有效期限等信息。此外，生成的证书应该严格保密，避免泄露给未经授权的人员使用\n","date":"2023-03-31T10:04:38+08:00","permalink":"https://tweakzx.github.io/p/kubernetes%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E9%9B%86%E7%BE%A4etcd/","title":"【Kubernetes】如何访问集群etcd"},{"content":"Singularity: Planet-Scale, Preemptive and Elastic Scheduling of AI Workloads Abstract\n Singularity  微软的全球分布式调度服务，高效和可靠地执行深度学习训练和推理工作负载 核心：是一个新颖的、工作负载感知的调度器，它可以透明地抢占和弹性地扩展深度学习工作负载  以提高利用率 而且不会影响它们在全球 AI 加速器(GPU/FPGA)中的正确性或性能   所有作业都是可抢占的、可迁移的，并且在默认可以动态调整大小(弹性) : 一个活动的作业可以被动态和透明地  被抢占和被迁移到不同的节点集、集群、数据中心或者区域，并且可以从执行被抢占的地方精确地恢复 在给定类型的一组不同的加速器上调整大小   机制透明：不要求用户对代码进行任何更改，也不要求使用任何可能限制灵活性的自定义库。 可靠：利用Singularity可以获得效率和可靠性增益，而对稳态性能的影响可以忽略不计。 我们的设计方法是对DNN 网络架构不感知的，并且可以处理各种并行策略(数据/流水线/模型并行)    Introduction Singularity的建立有一个关键目标: 通过在全球规模的加速器的固定容量池中最大化总有用吞吐量来降低人工智能的成本，同时为多个定价层次提供严格的 SLA。图1显示了 Singularity的高级体系结构，包括其分层调度系统，该系统由在全球、区域和工作负载范围内调度微服务组成。\n\rimage-20230317130437261\r\n设计目标   不闲置资源:\n Singularity将所有加速器（舰队）视为一个单一逻辑的共享的集群，并避免任何资源碎片化或静态容量保留。 机会性地使用空闲容量    提供job级别的 SLA: Singularity通过尊重作业级别的 SLA 来提供隔离。\n 例如，Singularity适合于增加推理作业的负载，通过弹性缩小或抢占训练作业来释放容量。\n   失败恢复: 作业从它们被抢占的地方恢复，从而最小化浪费\n  关键机制   两个关键的核心调度原语:\n 抢占和迁移: Singularity可以透明地checkpoint、抢占和迁移所有跨节点甚至跨集群和区域的 DNN 作业。checkpoint是通过使用一个有效的同步屏障来实现对分布式作业的所有Worker的分布式状态的一致切割。 调整大小/弹性: Singularity使所有作业都能够以透明的方式动态和弹性地调整大小，以便使用可变数量的 AI 加速器。    机制特点\n 透明  含义：1）不需要修改用户脚本，2）不依赖于框架/库 好处：为任意的深度学习工作负载提供一致的 SLA ，而不依赖于用户的任何合作来维护 SLA 因此：checkpoint、迁移和弹性在默认情况下对所有作业都是启用的   保存工作  迁移或调整大小的作业在程序执行的同一点恢复，其状态(例如，程序计数器、堆栈等)与被抢占或调整大小时完全相同。      Singularity中透明抢占、迁移和弹性的核心：自动将作业与加速器资源分离（解耦）\n  Singularity中的Worker和加速器设备之间的动态绑定的，并且在工作的生命周期中不断变化。要扩展或缩小作业，我们只需更改Worker映射到的设备数量。\n 这对用户来说是完全透明的，因为不管运行作业的物理设备的数量如何，作业的Worker的总数保持不变。\n   Singularity使用了一种称为副本拼接的新技术：在同一个设备上对多个Worker进行时分，开销可以忽略不计，同时允许每个Worker使用整个设备内存。\n 副本拼接依赖领域知识去利用分布式训练job的Worker之间在程序执行的特定点上的内存内容相似性      解耦的实现：设备代理\n 设备代理在自己的地址空间中运行，并且有一个到物理加速器设备的双射。当作业Worker初始化设备 API 时，它们被拦截并通过共享内存发送到设备代理进程，该进程的生命周期与作业Worker进程的生命周期解耦。 这种分离实现了两个关键的好处:  主机地址空间可以保持Clean：不会有设备特定的映射和其他由 GPU 库(如 CUDA)创建带来的副作用 \u0026ndash;\u0026gt; 从而可以使用CRIU来checkpoint和迁移主机进程 它允许动态的，透明的，时分的多个Worker在同一个设备上工作，多路复用和Worker间调度交由代理设备执行      贡献  checkpoint机制：一种透明和健壮的机制来checkpoint那些不支持checkpoint的通用 DNN 作业，从而使所有作业可以自动checkpoint，抢占和可迁移。 分布式屏障：基于新颖的，语义感知的技术，以完全透明的方式实现了一个DNN job的所有Worker之间的分布式屏障。这是实现 DNN 训练 job (包括程序计数器、堆栈、 CPU 和 GPU 内存等)跨多台机器的分布式状态的一致性削减的关键，这样就可以精准地从被抢占时的点重新启动 副本拼接：该技术允许在同一 GPU 上透明地对 DNN 作业的多个Worker进行时分，开销可以忽略不计(2-3%) ，并且使用超轻量级上下文切换以一种健壮和通用的方式。 我们评估我们的核心机制的功效和端到端的效率，通过使用不同类型的并行性(数据，流水线，或张量并行性)的模型进行详细的实验  性能开销：对于大范围的工作负载，GPU 调用的动态拦截、透明分布式屏障算法和透明时分的稳态性能开销可以忽略不计，均在3% 以内 稳健性：尽管 PyTorch 和 CUDA 的版本发生了迅速变化，但这种方法仍然是健壮的和实用的。 延迟： Singularity中的迁移和弹性延迟是合理的(几十秒) ，没有重复计算， checkpoint大小：checkpoint大小与用户级checkpoint相当。    核心调度机制总览 为了提高利用率和可靠性，Singularity所有作业在默认情况下都可以抢占和调整大小。\n分布在多个节点上的使用各种并行方式的作业都可以进行checkpoint，使用透明时分在一个潜在的不同的区域，不同数量的设备上在稍后恢复\n 在本节中，我们首先描述这些机制的三个关键方面:  透明性：对用户代码没有更改或约束 工作保存：作业从先前被抢占的同一个程序执行点恢复 解耦执行    用户透明   现有的checkpoint和弹性方法\n  依赖于用户直接编写代码来实现这些机制\n  让用户负担了保存/恢复 python 程序状态的复杂性\n 例如，循环变量、控制流、学习速率调度器、数据加载器状态、指令指针等\n   需要确保程序在正确的点恢复\n  当作业向上或向下扩展时，需要改变超参数\n    或者使用处理checkpoint和弹性的特定库(Pytorch Elasstic/Deepspeed)\n 用户失去了灵活性，因为这些库控制了训练循环以保持checkpoint易于处理。这限制了用户的可定制性; 因此这种库的采用率很低。今天大多数 DNN 的训练工作量是不可checkpoint或可调整的。      Singularity的核心机制（checkpoint/迁移/弹性）不需要用户的任何合作，默认情况下是自动的。\n 它使得调度程序能够依赖这些机制作为所有作业的first-class constructs，以提供严格的 SLA 它完全隐藏了checkpoint的复杂性和弹性，使得用户可以专注于编写具有完全灵活性的代码    工作保存  Singularity使用的checkpoint是由作业的各个Worker的一致地址空间快照组成。  当这些快照捕获完整的程序状态(如指令指针、堆栈、堆等)时，作业从被抢占的位置准确地恢复，没有丢失任何工作。 相比之下，现存的checkpoint和弹性机制迫使程序从以前的模型checkpoint重新启动，从而重新执行初始化工作和自上一个checkpoint以来执行的工作    解耦执行  提高作业的可靠性和船队的效率/利用率的目标是高度协同的  Singularity调度程序通过解耦作业与底层资源之间的映射来处理这些目标。Singularity调度器透明地虚拟世界大小和rank分配。 这种解耦是至关重要的：对于透明的checkpoint和抢占作业以及随后在不同的节点、集群、数据中心恢复它们来说，这些节点、集群、数据中心与之前的checkpoint状态具有相同或不同的 GPU 数量。 在作业的稳定状态执行期间，Singularity调度器还透明地将作业工作进程的训练逻辑与其与 GPU 的交互解耦。    用户利好  透明和节省工作量的checkpoint、迁移和灵活性从根本上增强了调度器的能力:  提高了容错性。任何作业(不管用户是否已经编写了checkpoint逻辑)都可以在发生硬件故障(GPU/节点/网络)时从系统自动获取的最新checkpoint精确地恢复，而不是从头开始。这显著提高了有用的舰队范围吞吐量。 机会性使用容量。透明的抢占和迁移允许作业在任何地方使用空闲资源，而不管集群/区域边界如何。此外，它使得具有较低 SLA 的作业可以机会性地使用空闲容量，并且当具有较高 SLA 的作业到达时，可以迅速抢占(而不会丢失作业)。透明弹性使就业机会能够扩大使用闲置产能，并在产能变得稀缺时收缩。 针对本地的后台碎片整理。用于容错(例如机架)、设备与设备互连、数据本地等的局部或拓扑域在作业进入和离开系统时会变得支离破碎，这使得用局部约束调度大型作业变得困难。小作业的迁移使调度程序能够对局部域进行碎片整理，以放置更大的作业。 在线升级。可以在不杀死作业的情况下进行全舰队范围的实时升级，因为在这些机器上运行的作业可以廉价且透明地迁移到不同的集群。    训练效率吞吐的SLA   传统的 SLA（延迟，5个9的可用性)，适用于推理工作量，他们不适合 DNN 训练。\n  Singularity引入了 GPU fraction度量：在抢占和灵活性面前量化吞吐量。\n  \rimage-20230317173405341\r\n  表1描述了 Singularity提供的多层 SLA。\n 一个需要 N 个 GPU (基于软配额)的任务可能获得多于 N 个或少于 N 个 GPU，这取决于竞争集群负载。用户只需支付实际使用的费用，而不需要支付配额;\n     符号定义\n $T_{ideal}$ 是某一个job在 N 个专用 GPU 上非抢占式运行所需的现实世界完成时间 $T_{real}$ 是在Singularity 中的实际完成时间    GPU time fraction of a job = $T_{ideal}$ / $T_{real}$。\n $T_{real}$ \u0026gt; = $T_{ideal}$，在允许资源超额订阅的情况下，因为作业在执行过程中可能被 Singularity抢占或缩小。 在Singularity中运行会相对慢一些，如果工作在一个专用容量设置中需要 H 小时  高级 SLA 工作：最多需要 H/0.95小时 标准 SLA : 最多需要 H/0.7小时   GPU 分数 SLA 以小时粒度强制执行。Singularity中的调度策略旨在最大化舰队范围的吞吐量，同时最小化违反这些 SLA 的情况。    特定领域拦截的方法   设备代理的定义与组成\n 任何与加速器的交互都必须经过特定的库(例如，用于 NVIDIA GPU 的 CUDA、用于 AMD GPU 的 ROCm) 。 Singularity通过 LD_PRELOAD 机制动态拦截它。大多数此功能驻留在设备代理中。 设备代理可以被视为加速器设备的硬件抽象层服务  组成：  服务器组件：每个设备有一个 客户端组件：嵌入在与设备交互的每个进程中   主机调用的所有加速器特定的 API 都会被拦截并发送到设备代理服务器，该服务器在一个独立的地址 $space^1$中运行。 在单独的地址空间中运行设备 API 有两个好处:  它保持主机地址空间没有设备映射和其他类似的依赖关系，避免影响checkpoint实用程序，如CRIU 它允许设备代理在弹性时分期间有效地跨多个工作进程共享        设备代理的通信与体系结构\n  主机进程和设备-代理之间的通信处于分派到设备的关键路径上，因此我们使用无锁共享内存通道，使得每次调用没有上下文切换开销，从而降低了延迟。\n  \rimage-20230317225820981\r\n  设备-代理中有两种类型的拦截器:\n  分派拦截器($D_{Int}$ )\n $D_{Int}$ 是语义无关的，它只处理将 API 跨地址空间发送到设备代理服务器，处理参数/响应的序列化/反序列化    语义感知拦截器($SA_{Int}$)。\n $SA_{Int}$ 在客户端或服务器端(分别称为**客户端 $SA_{Int}$** 和**服务器 $SA_{Int}$** )合并自定义逻辑，以实现诸如屏障、时间分片、内存管理等功能    请注意，$D_{Int}$ 和 $SA_{Int}$ 并不相互排斥;\n 例如，同一个 API 可能同时具有客户端 $SA_{Int}$ 、跨地址空间 $D_{Int}$ 和服务器 $SA_{Int}$ 。\n       限制$D_{Int}$的拦截面积   目标：为了在生产系统中实用和可维护\n 拦截层必须是完整的(作业调用的所有设备调用必须被拦截并发送到设备-代理服务器) 并且可伸缩(低工程成本)    背景：鉴于直接发布内核的新库(如 Apex、 Thust、 Deepspeed、 OpenAI Triton)的发展速度很快，拦截所有这些与设备交互的 API 的幼稚方法是不切实际的。\n  Singularity通过在低层拦截来限制 $D_{Int}$ 的表面积\n 低层：用于启动内核的驱动程序 API (例如，用于 NVIDIA GPU 的 cudaLaunchKernel) 好处：  这确保了良好的覆盖范围， 同时具有可伸缩性，因为其他定义客制化内核的库，最终都要通过launch API。   Singularity有一个自动的代码生成器，可以生成所有 $D_{Int}$ 的存根; 它只需要一个特定加速器库(CUDA)的头文件列表，以及一些注释来指示状态变化调用    $SA_{Int}$ 的硬件抽象   虽然 $D_{Int}$ 的是自动生成的，但是大部分的自定义功能(例如，分布式屏障，用于时分的上下文切换)都驻留在 $SA_{Int}$ 中。\n  $SA_{Int}$ 中的大多数逻辑都是与设备无关的，并且使用一个映射到设备特定 API 的硬件抽象层(例如 nccl_allreduce)。\n 加速器的硬件抽象层(HAL)封装了加速器通用的关键功能。虽然当前的实现是特定于 NVIDIA GPU 的，但是要处理一种新的设备类型，只需要实现该设备的硬件抽象层，将该设备的特定 API 与 HAL 中的等效 API 映射在一起。\n   有三种与设备无关的功能需要使用$SA_{Int}$: 内存分配、通信和设备同步。\n 内存分配。 内存分配API (例如 cudaMalloc、 cudaFree)需要一个 $SA_{Int}$ ，因为设备代理接管了内存分配。这使得设备代理对 GPU 内存中实际使用的区域具有完全的可见性，这有助于减少checkpoint大小。它还允许设备代理使用自定义内存分配机制，以帮助在同一 GPU 上对多个Worker进行透明的时分，从而获得弹性。 通信。多数加速器都有集体通信库(例如，NVIDIA 图形处理器的 NCCL，AMD 图形处理器的 RCCL)。在这些 API 上使用 $SA_{Int}$ 可以实现分布式屏障的算法，以便在分布式作业的多个Worker之间进行同步，从而获得一致的checkpoint。Singularity通过搭载相同的通信 API来提供了一个通用的障碍实现。这些 API 上的 $SA_{Int}$ 还有助于在弹性时分期间管理集合调用。 设备同步。设备同步 API 需要 $SA_{Int}$来处理透明弹性。Singularity中的时分是语义感知的，因为必须正确处理跨时间分片级别的通信。正确处理同步 API (例如 CUDA 中的 cudaStreamWaitEvent)对于时分的正确性和活性至关重要。    宿主机特定功能的$SA_{Int}$  Singularity还使用 $SA_{Int}$ 来选择 CPU 库  特别是 libc 的 I/O 库(例如，open、 read、 write 等)会被拦截来跟踪/记录作业对本地文件系统的更新，这样变异的文件就可以随着进程checkpoint一起迁移。 与设备库 API 的 $SA_{Int}$ 不同，主机 $SA_{Int}$ 没有相应的 $D_{Int}$ ，而是在主机地址空间中运行。    透明迁移的设计   在Singularity中，运行 DNN 作业的抢占、恢复和调整涉及到一致的checkpoint和恢复四大状态\n CPU 中的程序状态(例如堆栈、堆、指令指针等) GPU 中的模型训练状态(例如模型参数、优化器状态等) 处理 CPU 和 GPU 之间交互的控制状态(活动流、同步事件) 处理不同类型并行性(数据/管道/张量并行等)的 GPU 间和节点间通信状态    对于调度迁移，以及从计划外的故障中恢复，Singularity的透明checkpoint逻辑在两种模式下执行\n 当调度程序决定需要抢占作业时，基于扩展命令的随需应变 基于用户指定的间隔(时代级或基于时间)。    实现通用 DNN 作业的透明checkpoint是具有挑战性的，原因有\n 首先，在检查一个给定作业的时候，Singularity必须确保跨越多个主机和 GPU 的分布式状态的一致性; 一个分布式作业的所有Worker必须在集体通信方面处于一个安全和一致的状态(例如，allreduce)。 其次，CPU 和 GPU 之间的运行状态(例如，活动句柄，存储在主机内存中的设备地址)必须始终如一地恢复，尽管状态管理是由专有的闭源库(如 CUDA)完成的。 第三，checkpoint的空间开销必须保持在较低的水平，以适应有数百个Worker的大型分布式作业。    Checkpoint程序状态（CPU）  有多种系统提供地址空间迁移，其中 CRIU 是使用最广泛的。  然而，CRIU 的一个关键限制是它不处理使用 GPU 的进程的设备映射。 要使用 CRIU，主机地址空间必须与特定于设备的库隔离。 幸运的是，设备代理为我们提供了这种隔离。设备代理服务器大部分是无状态的，因此没有checkpoint; 它只是在目的地重新启动。    Checkpoint设备状态   保存方式：模型状态(例如，参数)由设备代理进程通过device to host的 memcpy 进行checkpoint。\n  特点：由于 Singularity中的内存分配 $SA_{Int}$，设备代理知道 GPU 内存的哪些区域实际上正在使用，因此显著减少了checkpoint大小。\n  挑战：在目的地恢复时，设备内存可能被映射到新设备-代理服务器地址空间中的不同地址，从而使主机进程中的指针失效。\n  内存地址映射：\n 为了避免这种情况，设备代理在启动时占用了整个 GPU 内存(被设备库跟踪的状态有些松弛) 有一个 服务器$SA_{Int}$ ，服务于设备分配器(cudaMalloc)所执行的**mmap**， 保证始终映射到相同的 CPU 地址。    设备句柄映射：保证CPU中设备句柄的一致性\n  与内存指针类似，主机地址空间还保留指向设备状态的其他句柄。\n 例如，cudaStreamCreate 返回一个不透明的句柄，主机可以在后续的 GPU 调用中将其用作引用。但是，由于设备代理服务器在迁移后重新启动，句柄将无效。\n   句柄虚拟化：为了在迁移过程中保持这些句柄的保真度，我们对这些句柄进行虚拟化。设备代理不返回设备返回的实际句柄，而是返回一个虚拟句柄，并将此映射作为客户端状态的一部分进行记忆。\n  效果：恢复和重播后，物理句柄可能会更改，但虚拟句柄保持稳定。所有有状态的 API 调用(例如，创建上下文、流、事件等)都有注释，这些调用的 $D_{Int}$ 会自动记录它们，以便在恢复时重播。\n    通信状态   静默：DNN Worker之间的大多数通信都是通过集体通信库(例如 NCCL)进行的，我们无法处理飞行中的通信。因此，在checkpoint时，我们静默所有job，以确保没有飞行中的集体通信调用。\n  死锁产生：\n 要完成一个集体调用(例如 allreduce) ，需要所有参与的Worker都完成该调用 如果一个Worker在第 n 次 allreduce 调用已经返回后进行了checkpoint，而另一个Worker可能已经发出了第 n+1次调用，这个调用永远不会完成，从而导致死锁。 因此，在checkpoint之前，所有Worker必须完成同一个集体调用。Singularity使用了一种新的分布式屏障算法，以完全透明的方式实现这一特性。    分布式Barrier\n meta-allreduces：为了避免引入新的故障路径，Singularity中的障碍算法依赖于作业用于集体通信的同一个通信库，通过引入附加的meta-allreduces来交换障碍协议状态。需要确保附加的meta-allreduces相对于常规的allreduces的排序在所有节点间是一致的，以满足避免死锁的程序顺序要求。 算法：为了保证程序顺序一致性，在每一个数据allreduce操作执行之前， 我们的算法会发射一个异步串联meta-allreduce。   Worker包含两个阶段\n 第1阶段是稳定状态 第2阶段是接收到障碍请求。    串联 meta-allreduce 是有效载荷上的一个 SUM allreduce，由两个整数组成:\n $Need_ {bar}$: 如果一个 worker 已经接收到了一个屏障命令，那么它会发送一个“1”，否则发送“0”。如果 SUM (need) \u0026gt; 0，则Worker知道有人已经启动了屏障协议，并切换到第2阶段。 $Ack_{block}$: 如果Worker已经切换到第二阶段，它将发送一个“1”，也就是说，它承认它已经直接或间接地看到了一个障碍请求，否则发送“0”。如果 SUM (ack) = = world _ size (即rank的总数) ，则Worker知道每个人都已经认可，并且可以安全地获得障碍。    一旦Worker进入阶段2，它就进入了同步模式: 该Worker执行的每个集体调用都是同步的; 这确保了屏障协议的及时终止。\n   开销：开销很小障碍算法保证在最多两个小批内完成，并保证在屏障时没有飞行中的集体调用。它在阶段1的开销非常小，因为meta-allreduce只需要2字节，而且是异步的 局限性：适用于数据并行作业    关于tensor并行和pipline并行\n 张量并行和管道并行作业还有额外的复杂性，这些作业可以在不同的节点组之间执行多个 allreduce，此外还有对等调用，如 send/recv (用于流水线)。 解决方案：  我们可以扩展上述算法，来推断出流水线和数据通信的相对顺序 为了尽可能简单和减少checkpoint的大小，我们通过使用领域知识，确定一个小批的结束，在这一时间点上没有飞行中的通信，无论是tensor并行和pipline并行。 我们使用与上面相同的串联 meta-allreduce 协议，但是在小批处理结束时只使用一次来实现障碍   tradeoff：将障碍延迟到mini-batch结束(对于大型模型来说只需要几秒钟) ，带来了时间延迟，但是与在mini-batch执行中进行屏障相比，减小了checkpoint大小    文件系统状态  背景：Worker有时安装本地软件包，更新本地文件。迁移到新节点后需要保留这些内容。 挑战：执行容器维度的文件系统状态diff操作的代价太高。 解决方案：  使用Libc 文件系统 API 上的host $SA_{Int}$ 每当以可写模式打开本地文件时，我们将文件名附加到日志中，并在checkpoint期间复制这些文件。 通过使用内容checksum，在远程存储的数据副本实现跨Worker去重。    Checkpoint/Restore工作流  工作流  在成功获得一个屏障之后，每个Worker使用CRIU执行checkpoint。 CRIU镜像和GPU状态镜像，随后被移动到远程存储。 CPU：在新的目的地上，使用CRIU的restore，程序从它被checkpoint的状态重新开始 GPU：  设备代理客户端执行的第一个操作是重新生成一个新的设备代理服务器，然后重播状态，使 GPU 恢复到checkpoint之前的状态。 设备代理服务器还将 GPU 张量复制回checkpoint之前的相同地址的 GPU RAM。     设备-代理最终执行一个新的会合，worker可以发现彼此的新位置，并重新建立通信环。 checkpoint执行时机：除了由调度程序启动的随需checkpoint之外，Singularity中的每个作业都以用户指定的频率(例如，每30分钟)接受checkpoint，以处理计划外的故障。  压缩Checkpoints Singularity采用多种技术来减小checkpoint的大小。\n GPU镜像大小：  Singularity执行每个缓冲区的内容校验求和，以便在Worker之间进行去重。 只有在没有其他Worker上传相同的缓冲区时才上传缓冲区 通过这种方式，Singularity中 GPU镜像的大小与用户级checkpoint的大小相似。   CRIU镜像的CPU 地址空间是在空间和时间两方面进行去重的  空间：在主训练过程和数据加载过程之间存在高度的内容重叠; 我们拦截 CRIU 发出的写调用来执行基于内容散列的页面分解 时间：在不同时间点采取的同一进程的checkpoint之间存在高度的重叠(因为地址空间变化很小) ; 时间维度上的去重使得之后的增量checkpoint比第一个 CRIU checkpoint小得多。    透明弹性的设计   Singularity的弹性实现是不改变世界大小的， 改变的是worker和device之间的映射。\n  \r\n  透明弹性建立在Singularity中的透明迁移支持的基础之上。\n  技术挑战：\n 时分复用：一个训练工作的多个Worker在同一个 GPU 上进行时分共享时，Worker之间的细粒度通信必须像在不同的 GPU 上一样 空分复用：对于大模型来说，每个Worker几乎可以利用 GPU 上的整个RAM; 在同一个 GPU 上运行多个Worker，需要将 GPU 状态在设备和内存之间来回交换， 代价昂贵。 调度死锁：为了支持使用数据并行、流水线并行和张量并行相结合的作业，需要在 GPU 上仔细安排Worker，以便数据并行副本和相同的模型并行碎片在同一GPU上进行时分，并防止通信调度中出现死锁。    语义感知的时分   原因：\n 一个独立的Worker占有独立的内存是不可取的， 因为大模型的每个Worker往往需要整个GPU RAM， 这样的运行方式会耗尽内存 所以，时分需要语义感知    Singularity中的设备代理使得这样的时分成为可能。\n 因为设备代理与主机进程解耦，所以我们在多个主机进程(即多个rank)之间共享相同的设备代理。 当所有与 GPU 的交互都通过设备-代理进行时，它智能地安排多个rank，在给定的时间只允许一个rank在 GPU 上执行，然后选择特定的点来切换到另一个rank。 从概念上讲，在上下文切换的时候，设备代理将原始rank使用的 GPU 内存换出，然后换入新rank的GPU 内存，从而使每个Worker能够使用几乎整个 GPU 内存。    为了保持低开销，我们必须在绝对必要的情况下切换上下文。\n 在后向传播之后，数据-并行rank参与集体通信以交换梯度，这需要所有rank参与(并贡献其各自的梯度) ，必须进行上下文切换。 请注意，在单个小批处理中，为了重叠使计算与通信开销，框架可能会发出多个异步 allreduce 调用， 然后在某个点进行同步。在这个同步点，设备代理切换到共享 GPU 的下一个rank，独占地运行，直到达到同步点，然后上下文切换到下一个rank，以此类推。    集体通信通过专有库进行。\n 通信器：NCCL 有一个通信器的概念，它被初始化为一个特定的参与rank环，随后的操作只是引用通信器。 解耦： 为了保持 NCCL 通信器与用户级时分的交互可控，我们将作业的逻辑数据并行世界大小与 NCCL 看到的世界大小解耦; 在我们的方法中，NCCL 只看到每个 GPU 一个rank。 梯度累计：在时分过程中，设备-代理透明地在抓取缓冲区中进行本地累积，并且只有共享 GPU 的最后一个rank使用本地累计梯度执行实际的nccl_allreduce 大小调整：因此，在一个调整大小的操作之后，NCCL 看到的世界大小发生了变化，恢复后由新的会和处理    内存共享的副本拼接 副本拼接使上下文切换更加便宜 \u0026ndash;\u0026gt;5.2.1\n基于checksum的动态数据去重   一项训练工作所消耗的 GPU 内存分为四类:\n 参数(P)。模型的每一层的权重/参数; 正向和反向传递在这些张量上运行。 优化器状态(O)。由优化器跟踪的状态，以计算每次迭代应用于参数的增量。跟踪历史状态(例如，第一和第二阶段的梯度) **梯度(G)。**每个副本都有与其迷你批处理相对应的渐变副本。在向后传递之后，对所有副本的梯度取平均值，然后使用这个平均值一致地更新权重 激活(A)。每一层的正向通道的中间输出; 在反向通道中用于计算相对于输入的梯度以进行反向传播。    副本拼接利用的洞见\n 在数据并行副本中，P和O在小批处理结束时由所有副本一致地更新，所有rank具有相同的平均梯度 在mini-batch结束后， 因为后向传播已经结束， A会被框架释放掉    由于设备代理控制内存分配器，它对框架分配的每个缓冲区都具有可见性。\n 在上下文切换期间，设备代理为每个活动缓冲区计算内容校验和。 在换出过程中，它首先查看主机是否已经包含一个具有相同内容校验和的缓冲区，如果是，它避免换出，并简单地将 GPU 缓冲区标记为未使用 在换入过程中， 它检查设备是否已经有一个带校验和的缓冲区; 如果有，它就避免了来自主机的换入。请注意，虽然内容匹配，在新的rank该缓冲区可能被映射到一个不同的设备地址 。在这种情况下，设备代理执行设备到设备的移动，将缓冲区移到所需的地址    一个例子：\n 通过上述优化，如果4个rank共享一个 GPU，在上下文切换期间P和O缓冲区的交换只需要在第一个rank完成; 其他人会发现校验和已经存在于主机内存中，并忽略掉交换。 然而，对于每个rank来说换入仍然必须进行： 当一个rank开始其时间片时，其本地状态包含来自前一个mini-batch的P和O，而前一个rank的P和O副本被更新到当前的mini-batch。 如果我们有空间存储两个额外的 P 和 O 版本在 GPU，我们就可以避免这个换入，这带来了两个挑战:  大模型一般没有额外的空间容纳额外两组P和O \u0026ndash;\u0026gt; 5.2.3 我们仍然需要执行设备到设备的 P 和 O 副本在上下文切换，因为每个rank可能分配了在不同的地址分配了相同的缓冲区，D2D 复制开销仍然很大。\u0026ndash;\u0026gt; 5.2.2      为持久分配使用领域知识  使用深度学习训练的领域知识来使地址保持一致  在数据并行副本中，根据定义，稳定缓冲区（如 P 和 O 在小批处理中保留的分配序列）的大小、顺序必须在所有副本中相同，因为它们具有相同的参数集。 然而，可能存在跨副本大小可变的其他分配(例如，大小取决于输入数据大小的激活，这种激活可能在不同的小批处理中有所不同)。由于这种可变大小的分配，内存分配器的状态在不同的副本之间发生差异，甚至导致稳定的缓冲区分配得到错误对齐的地址。   为了处理这个问题，Singularity中的设备代理使用一个双向内存分配器。  稳定缓冲区(如 P 和 O)在地址空间的高端分配，而其他缓冲区在低端分配。这确保了瞬态分配(例如激活)中的不稳定性不会影响高区域中的内存分配器元数据，从而确保稳定的缓冲区(例如 P 和 O)跨副本获得相同的地址。 为了识别稳定的缓冲区，比如 P 和 O，我们在分配器中添加了一个预先识别的堆栈跟踪列表(Python 和 C + +) ，这个列表与参数和优化器状态分配有关。 在分配时，最多有两个版本的 P 和 O 是活跃的-当前的小批量和以前的小批量，并且第三个副本需要作为草稿空间，以便当前的rank不会覆盖以前的小批量的原始版本的 P 和 O。    压缩选定的操作   使用了特定领域的洞察避免处理 P 和 O 的多个副本。\n 根据定义，所有数据并行副本将在完成小批处理之后使用一致的P和O缓冲区。 我们还知道，P和O缓冲区只有在所有副本的梯度allreduce的之后才会更新。 因此，如果我们能够识别更新参数和优化器状态的操作，我们就可以只在共享设备的其中一个rank中执行这些操作，并简单地在其他rank中“压缩”这些操作，因为  它们无论如何都会导致相同的最终状态， 缓冲区在队列中有相同的对应地址，所以随后的小批量计算将看到正确的数据。   为了压缩一个操作，设备代理只是省略了为这些操作向 GPU 发出 CudaLaunchKernel。通过这种压缩，我们避免了在前一个mini-batch的P和O的换入，因为它们不再由其他rank更新。    为了确保健壮性，我们遵循保守验证的方法\n 对照组：我们总是运行第一个mini-batch，同时禁用了压缩(因此产生了交换进/交换出成本) ; 这保证了正确的执行。 实验组：在验证mini-batch中，断言假设，验证使用缓冲区内容校验和来推断事后操作效果的方法：在验证mini-batch中，我们验证模型是否符合以下不变量:  在压缩窗口期间的所有缓冲区更改必须在共享 GPU 的所有rank之间是相同的。 在压缩窗口期间执行的设备到主机的副本必须在共享 GPU 的所有rank之间完全复制相同的数据。   如果上述验证失败，我们将模型视为压缩的不安全模型，并退回到基于交换的机制(如果必要，将作业“回滚”到验证成功的最后一个checkpoint)。 我们监视由于时分造成的开销，如果它超过一个阈值，我们将禁用该模型的时分。这将是一个罕见的场景，但仍然需要优雅地处理以获得健壮性。    处理模型并行工作   张量并联、流水线并联等模型并联作业的处理带来了新的挑战。\n 张量并行job中的每个矩阵乘法执行allreduce。如果我们为这样的 allreduce 进行上下文切换，那么副本拼接将无法工作，因为激活张量A仍然是活跃的。 pipeline并行job对每个mini-batch执行 GPU/节点之间的端到端地发送与接收激活A和梯度G; 在mini-batch期间进行时分会由于活跃的A和G导致过多的交换。    为了应对这些挑战，Singularity使用了两种关键技术:\n 拼接感知的放置。通过拼接感知的放置，我们确保只有同一个的模型并行分区的数据并行副本在同一个 GPU 上进行时分。  请注意，这要求 Singularity了解rank分配逻辑。对于使用具有不同rank分配策略的自定义启动程序的作业，Singularity为该作业提供了一个 API，用于传递所有rank的rank到拓扑的映射(例如，Rank 4是 DP0、 MP0、 PP1等)   推断集体调用的意图。设备代理推断集体通信的意图，并仅在数据并行维度上集体调用时触发时分。其他的集体调用只是简单地通过而没有上下文切换。推断方法：  Singularity利用集体通信的初始化路径(例如，ncclCommInitRank)来实现这一点。它强制在每个ncclCommInitRank 之后进行上下文切换，并且设备代理计数每个通信者。 在完成一轮上下文切换之后，如果通信器的本地计数大于1，则设备代理将推断该通信器处于数据并行维度。因为拼接感知的放置。 在集体调用期间，它只需要在通信器上查找一个map，就可以了解它是否是数据并行的。      处理ZeRO-冗余优化器  ZeRO切分数据并行状态，这样数据并行的Worker之间就没有冗余。这样的分区违反了压缩验证的不变量(5.2.3)。 为了解决这个问题，Singularity为 ZeRO 引入了部分分片的概念，它将分片因子(在 GPU 中拟合模型所需的最小值)与数据并行度(并行度)分离开来。  如果两者是相等的，那么根据定义，这个模型不能缩小到更少的 GPU，因为它不适合。 如果数据并行性因子更高，比如说，4倍于分片因子) ，那么我们可以支持多达4路的时分/缩小。在这个场景中，部分分片因子只是变成了模型的另一个维度——并行性，并且只有相同的 ZERO 分片的副本是时间分片的。   在 DeepSpeed 中引入部分分片非常简单  实现  实现方面的挑战  序列化不透明参数。在我们的基于拦截的设备代理中，CudaLaunchKernel 的 $D_{Int}$ 是具有挑战性的，因为它的签名是不透明的，使得序列化变得困难(签名是由 NVIDIA 的1 nvcc 在内部生成的，拦截器不可见)。为了处理这个问题，我们有一个自定义的服务器 $SA_{Int}$ ，它使用 cuObjecDump，CUDA 工具包中的一个二进制实用程序，解析生成的内核库并提取参数信息。为了避免高成本，我们缓存此信息，并仅在缓存丢失时运行 cuObjecdump。对于 JIT 内核，我们拦截 nvrtcCompileProgram 并通过解析生成的 PTX 提取参数签名。 隐藏调度延迟。设备代理中的跨地址空间调用发生在诸如 cudaLaunchKernel 和 cudaGetLastError 等操作的关键路径上，这会影响性能。我们对最频繁的调用使用特定于域的优化。对于 cudaGetLastError，我们在每次启动内核时都会在服务器上发布它，并将它与响应一起附加，这样当 PyTorch 发布它时，设备代理客户端就可以从缓存中返回它。对于 cudaLaunchKernel，我们执行延迟错误通知，调用在客户端返回，而不等待来自设备代理服务器的响应; 在向服务器发出下一个调用之前，对响应进行延迟读取，因此允许客户端 PyTorch 处理和服务器引起的延迟之间重叠; 因为当遇到这种(罕见的)错误时，PyTorch (和其他框架)崩溃，这不会影响作业语义。 隐藏上下文切换开销。在时分过程中从一个rank切换到另一个rank涉及到计算所有活动设备张量的校验和，并将它们与另一个rank的副本进行比较，如果需要，执行缓冲区移动(几毫秒的 CPU 活动)。此外，开关逻辑依赖于校验和计算的输出，校验和计算依次等待所有以前的 GPU 操作完成，隐含地强制设备同步。为了避免在关键路径中产生这种代价，Singularity执行下一级的急切分派。设备代理开始并行地服务于下一级(CPU 逻辑、设备操作分派)的有用工作和交换延迟。通过仔细使用诸如 cudaStreamWaitEvent 之类的异步排序原语，我们确保只有在切换完成之后，才能在 GPU 上执行对新排序的操作。    评估 相关工作 总结 ","date":"2023-03-17T11:37:06+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202303171218307.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0singularity%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Singularity论文笔记"},{"content":"Github520 crontab -e 0 0 * * * sed -i \u0026#34;/# GitHub520 Host Start/Q\u0026#34; /etc/hosts \u0026amp;\u0026amp; curl https://raw.hellogithub.com/hosts \u0026gt;\u0026gt; /etc/hosts 安装Go wget -c https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz -O - | sudo tar -xz -C /usr/local vim /etc/profile export PATH=$PATH:/usr/local/go/bin export GOPROXY=https://goproxy.cn,direct export GO111MODULE=on source /etc/profile go version 安装MiniConda(python 3.8) wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.1.0-1-Linux-x86_64.sh \u0026amp;\u0026amp; sh ./Miniconda3-py38_23.1.0-1-Linux-x86_64.sh vim ~/.condarc channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ ssl_verify: true source ~/.condarc 查看服务器上的图片 python -m http.server 8888 ","date":"2023-02-22T10:43:18+08:00","permalink":"https://tweakzx.github.io/p/%E6%8C%87%E4%BB%A4%E9%AD%94%E6%B3%95%E6%8C%87%E4%BB%A4/","title":"【指令】魔法指令"},{"content":"Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads Abstract  背景  加速器，如 GPU、 TPU、 FPGA 和定制 ASIC 表现出跨模型架构的异构性能行为。 现有的针对加速器集群的调度器(用于在许多用户之间仲裁这些昂贵的培训资源)已经展示了如何针对各种多任务、多用户目标(如公平性和完成时间)进行优化。不幸的是，现有的调度程序基本上不考虑性能异构性。   异构感知调度器 Gavel  它系统地推广了大量现有的调度策略。Gavel 将这些策略表示为优化问题，然后使用我们称为有效吞吐量的抽象将这些问题系统地转换为能够识别异构性的版本。 然后，Gavel 使用一种基于循环的调度机制，以确保在给定目标调度策略的情况下，作业能够得到理想的分配。 Gavel 的异质性感知策略允许异质集群维持更高的输入负载，并且与异质性不可知策略相比，最终目标如完成时间和平均工作完成时间分别提高了1.4倍和3.5倍。    Introduction   背景\n 我们的研究小组在其私有集群中拥有 NVIDIA Titan V、Titan X 和 P100 GPU。 这些多租户设置中的资源通常由调度程序仲裁。 Themis [40]、Tiresias [28]、AlloX [37] 和 Gandiva [58] 等 GPU 集群调度程序因此需要决定  如何为多个用户分配 同时实施复杂的集群范围调度政策，优化目标，例如公平或完工。      不幸的是，由于以下三个原因，很难在这种情况下选择最有效的加速器类型：\n 性能异质性。由于各种架构差异，常用模型显示出跨加速器类型的异构性能行为。  一些调度策略也可以受益于在多个资源类型之间拆分作业：例如，最小化受延迟 SLO 影响的作业成本（例如，在 10 小时内完成作业）可能涉及使用更便宜的加速器开始训练，然后切换到满足 SLO 的更快、更昂贵的设备。 因此，即使是简单的单一作业设置，加速器类型的选择也很重要，并且取决于作业和策略。这在多工作设置中变得更加复杂，因为授予所有工作他们的可能无法同时使用首选加速器。 现有的调度程序，如 Gandiva、Tiresias 和 Themis 不考虑这种异构性能行为。   \rimage-20230220122906946\r  图 1a 显示，与 K80 GPU 相比，ResNet-50 模型从 NVIDIA V100 GPU 获得近 10 倍的加速，而 A3C 深度强化学习模型仅获得 2 倍的加速。 然而，如图 1b 所示，当我们考虑每美元训练的样本数量时，V100 不再是所有模型的最佳选择——对于许多模型，较旧的 P100 GPU 在每美元基础上具有竞争力或更便宜。   调度策略之间普遍性。  集群操作员可能希望根据其业务目标实施不同的调度策略，例如优化  完成一组批处理作业的时间 (makespan)、 临时作业的公平性，   或更复杂的分层策略，  将资源分配给高层实体（例如，部门）使用一种策略， 然后实体内的个别工作使用另一种 [34]。   在数据分析集群中，许多作业调度程序已经支持分层分配策略 [Hadoop、Spark、YARN、Delay Scheduling]。 最近提出的两个考虑异构资源的 GPU 调度程序 AlloX [37] 和 Gandiva-fair [18]，针对单个调度目标进行优化，并将其调度机制与该目标紧密耦合（例如，最大-最小公平性）。因此，它们无法轻松支持实践中经常使用的更复杂的策略。   托管和安置优化。  为了提高集群利用率，现有的 GPU 调度程序经常部署优化，例如  Gandiva [58] 中的空间共享，其中多个作业可以同时使用相同的加速器， 以及 Themis 和 Tiresias [28, 40] 中的布局敏感性，这涉及谨慎将任务放置在分布式作业中以确保良好的扩展性能。   在针对全局调度目标进行优化时，应明确考虑这些优化的性能优势，因为这些优化在以异构感知方式部署时更加有效。 我们表明，与 Gandiva 的ad-hoc方法相比，对空间共享进行显式建模可以将目标提高 2.2 倍。      在本文中，我们介绍了 Gavel，这是一种新的集群调度程序，专为本地和云部署中的 DNN 训练而设计，它有效地结合了硬件加速器和工作负载中的异构性，以推广广泛的现有调度策略。例如，Gavel 可以提供公平共享/最少实现服务 [28]、FIFO、最小完工时间、受 SLO 约束的最低成本、完成时间公平性 [40]、最短工作优先和分层策略 。\n Gavel 的主要观察是，许多广泛使用的调度策略（包括分层策略）可以表示为优化问题，其目标是作业实现的吞吐量的函数。  例如，最少达到的服务相当于最大化作业之间的最小缩放吞吐量，完工时间相当于最小化最大持续时间（计算为迭代次数与达到的吞吐量之比）等等。 鉴于调度策略的优化问题，Gavel 引入了一种通用方法来转换问题，使其具有异质性、协同定位和放置感知能力。 特别是，Gavel 将问题更改为  搜索每个作业的异构分配，花在各种资源配置上的时间比例（例如，60% 的时间在 V100 GPU 上单独运行，40% 的时间在 A100 GPU 上共享空间）与另一项工作）， 并将目标函数中的吞吐量项更改为有效吞吐量，即作业在其分配的资源组合中的平均吞吐量。 需要添加额外的约束以确保返回的分配有效。   我们表明，即使对于具有数百个 GPU 和作业的集群，Gavel 的转换优化问题也能高效执行，并且可以支持广泛的策略。许多这些问题都可以使用一个或多个线性程序的序列来解决。   Gavel 对每个作业的异构感知分配需要映射到实际的调度决策（在指定的持续时间内将作业放置在集群中的特定资源上）。  为实现这一点，Gavel 使用基于轮次的抢占式调度机制来确保作业接收的资源与计算的目标分配相似。 Gavel 的调度机制需要能够调度同时请求多个加速器的分布式训练作业，以及由于空间共享而在给定加速器上同时运行的作业组合。   Gavel 透明地做出这些调度决策：  它在调度程序和应用程序之间指定了一个 API，允许在现有深度学习框架（如 PyTorch [48] 和 TensorFlow [13] 中编写的作业以最少的代码更改在资源之间移动， 并使用类似的机制到 Quasar [21] 来估计并置工作的绩效测量，当先验不可用时，这些测量需要作为 Gavel 政策的输入。   通过明确考虑性能异质性，Gavel 提高了各种策略目标（例如，平均作业完成时间或 makespan）：  在较小的物理集群上，它将平均 JCT 提高了 1.5 倍， 在较大的模拟集群上，它增加了一个集群可以支持的最大输入负载 ，同时将平均作业完成时间提高 3.5 倍，完工时间提高 2.5 倍，成本降低 1.4 倍等目标。      总而言之，我们的主要贡献是：\n 一种将现有集群调度策略转换为考虑异构和托管的等效策略的系统方法； 这些等效的优化问题对于当前的 DNN 集群是实用的 基于轮次的调度机制，以确保集群实现这些策略返回的分配。 对我们框架中的许多现有政策进行概括，以改进相应的目标。    Background DNN训练 System Overview  给定一组作业，Gavel 在常驻作业中仲裁集群资源（以不同类型的加速器的形式），同时针对所需的集群目标进行优化。  这是通过两步过程完成的：  首先，异构感知策略计算不同作业（和组合）应在不同加速器类型上运行的时间分数，以优化预期目标。这些策略需要输入每种加速器类型上每个作业的性能行为（根据吞吐量）  这些行为可以由用户提供 也可以由 Gavel 的吞吐量估算器即时测量   分配在仅在分配重新计算事件之间是重要的  Gavel 可以在重置事件发生时（作业到达或完成，集群中的 worker 失败）或以周期性时间间隔重新计算其策略 给定策略的输出分配，Gavel 的调度机制授予作业在不同资源上的时间，并根据需要在工人之间移动作业，以确保每个作业在不同资源上花费的时间的真实比例与策略返回的最优分配非常相似。 Gavel 的工作流程如图 2 所示 \rimage-20230220133715374\r        异构感知的调度策略 Gavel 将调度策略表示为各种感兴趣目标的优化问题，例如公平性或完工时间，并将分配表示为矩阵，指定作业在分配重新计算之间应花费在每种加速器类型上的挂钟时间的分数。矩阵 X 可以表示单个加速器类型（同类设置）、多种加速器类型（异构设置）以及其他优化的分配。考虑 X 示例：\n\rimage-20230220091702416\r\n根据在三个作业和三个加速器类型上指定的分配，作业 0 应该花费 60% 的时间分配在 V100 GPU 上有效，其余 40% 的时间在 P100 GPU 上。这在图 3 中直观地显示了。\n\rimage-20230220132027092\r\nGavel 在给定表示为优化问题的策略的情况下为矩阵 X 找到最优值。为了构建给定策略的优化问题，Gavel 需要一个吞吐量矩阵 T，其中包含每个作业在不同加速器上的吞吐量（每秒训练迭代次数）。如果作业 m 不在加速器类型 j 上运行（例如，由于内存限制），则可以将 Tm j 设置为 −∞。\n给定 T 和 X，我们将模型 m 的有效吞吐量定义为跨加速器和作业的时间加权平均吞吐量。为简洁起见，我们将此数量表示为 throughputT (m, X) 或简称为 throughput(m, X)（去掉 T）。对于没有空间共享的分配 X，\n\rimage-20230220132127428\r\n不同的集群调度策略可以表示为 X 最大化或最小化适当目标函数的优化问题。需要指定约束以确保 X 是有效分配。最大化总有效吞吐量的假设策略看起来像，\n\rimage-20230220132148339\r\n受以下限制：\n\rimage-20230220132216259\r\n这些约束确保每个工作-工人分配都是非负的且介于 0 和 1 之间（等式 1），工作的总分配不超过 1（等式 2），并且分配不会超额分配工人（等式 3） ).\n空间共享。 Gavel 的分配矩阵也可以包含空间共享 (SS)。虽然之前的工作使用贪婪算法进行空间共享，但我们发现，根据它们消耗的资源，不同对的 DNN 应用程序在实践中在并置在一起时具有截然不同的性能（图 4）。使用空间共享时，X 需要包含每个可行的作业组合的行，并且 T 需要具有作业组合的吞吐量，例如：\n\rimage-20230220132319613\r\nSS 感知分配 X 规定了每个作业组合应该花费在每种加速器类型上的时间分数。\n我们将 T 的条目限制为最多 2 个工作的组合；我们根据经验发现，较大的组合很少会增加净吞吐量。此外，尽管 T 的大小随着工作数量的增加呈二次方增长，即使工作组合的大小为 2，我们发现在实践中我们只需要考虑实际表现良好的组合。我们在 §7.4 中评估了这些 SS 感知策略的缩放行为。\n我们将 T 的条目限制为最多 2 个工作的组合；我们根据经验发现，较大的组合很少会增加净吞吐量。此外，尽管 T 的大小随着工作数量的增加呈二次方增长，即使工作组合的大小为 2，我们发现在实践中我们只需要考虑实际表现良好的组合。我们在 §7.4 中评估了这些 SS 感知策略的缩放行为。\n\rimage-20230220132437684\r\n约束也需要稍微修改，以确保 X 是这个新制度中的有效分配：\n\rimage-20230220132540750\r\nCm 是包含作业 m 的所有作业组合的集合。\n位置敏感性。同样，Gavel 的分配矩阵也可以扩展以包含位置敏感性。观察到的分布式作业的吞吐量取决于任务的位置以及模型和加速器类型（速度较慢的工作人员不太可能受到通信限制，\n\rimage-20230220132614795\r\n这意味着任务合并的效率较低）。我们可以通过考虑分布式作业在以下方面的性能来使我们的策略对位置敏感：1) 统一设置，其中尽可能多的加速器在同一台服务器上（例如，如果使用 8-GPU 服务器，则每台服务器 8 个 GPU）， 2) 一个未合并的设置，其中加速器位于独立的服务器上。这些是布局空间中的极值点，是性能的上限和下限。我们可以在我们的策略中对此进行建模，方法是让两种不同的工作人员类型（合并的和未合并的）在 T 中具有相应的吞吐量值，在 X 中分配值。\n基于轮次的调度机制 在计算出最优分配后，Gavel 的下一步是将作业（或作业组合，在 SS 的情况下）分配给加速器类型，同时尽可能匹配最优分配。也就是说，要实现上面的分配$X^{example}$，调度机制需要保证在集群中只有作业0、1、2这三个可运行的作业的时间段内，作业应该按照自己计算出的最优时间获得资源分数。\n为此，调度程序会为每个作业和加速器类型组合计算一个优先级分数，当作业收到的时间分数小于最佳分配时，该分数会很高。调度是轮流进行的；在每一轮中，调度程序以优先级递减的顺序运行作业，同时确保给定作业不会在给定轮次中安排在多个工作程序（或加速器）上。这在图 5 中显示。优先级在轮次完成时更新。我们根据经验发现，大约 6 分钟的回合持续时间允许 Gavel 有效地接近理想分配（§7.5）。\n吞吐估算器 为了估计并发作业的吞吐量（例如，在空间共享的情况下），Gavel 使用了一个吞吐量估计器，类似于 Quasar [21] 等先前工作中发现的那些。 Gavel 的吞吐量估算器将新作业映射到一组预先配置的参考作业。然后可以将最接近的参考作业的吞吐量用作新作业组合的初始性能估计。对于单个作业，不需要吞吐量估算器，因为当作业在不同的资源类型上运行时，可以动态估算吞吐量。\n局限性与非目标 虽然 Gavel 公开了一个支持各种策略和目标的灵活 API，但我们并未在这项工作中提出新的调度策略或性能优化。相反，Gavel 的主要目标是确定如何以异构感知的方式最好地在许多不同的用户和作业之间共享资源，同时支持许多现有的集群范围的目标。 Gavel 通过一个策略框架来实现这些目标，该框架可以轻松地使策略具有异质性、共置和位置感知（§4）、可重用的调度机制（§5）以及允许用户部署他们的窄调度程序 API代码更改最少的应用程序 (§6)。\nGavel调度策略 Gavel调度机制 ","date":"2023-02-19T19:57:52+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202302201343545.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0gavel%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Gavel论文阅读笔记"},{"content":"Hugo+Typora+PicGO+Github搭建个人博客    我之前使用过typecho来搭建个人博客，由于是搭建在服务器上，我需要使用宝塔linux来完成各种配置，花花绿绿的各种环境让我对搭建博客多少有些累觉不爱\n  不过随着我阿里云服务器的到期，我决定趁机使用github托管我的个人网站，但是我不想麻烦区配置各种后端环境， 所以选择使用hugo。\n  与其说我精挑细选了一种博客框架，倒不如说我的目的只是想要快速搭一个博客出来，技术的选型其实并没有花太多心思。\n   你现在看到的博客使用到的工具链是：Hugo+Typora+PicGO+Github，它们分别的功能是\n  hugo: 编译静态网站\n  typora：编写博客内容\n  picgo：配合github搭建图床， 与typora联动上传博客中的图片\n  github：一方面网站需要使用到github提供的pages功能， 另一方面我也使用了github作为图床\n  下面是搭建这个博客的一些步骤， 记录下来一方面方便他人参考，另一方面如果我要重新搭建博客也方便我自己参考。\n搭建前的一些环境配置\n 系统： windows 包管理器：scoop （非必须） 版本控制：git markdown编辑器：Typora  github创建项目  打开github新建一个库 建议项目名与github用户名保持一致，比如我的是 tweakzx，那么输入的 Repository name 就是 tweakzx.github.io  hugo创建站点 hugo 安装 参看官方的安装教程Installation | Hugo (gohugo.io)\n我使用的系统是Windows的，我推荐以下两种安装方式\nChocolatey (Windows)\n如果使用 Windows 并且使用 Chocolatey 作为包管理器，可以使用如下一行代码来安装 Hugo ：\nchoco install hugo -confirm\r如果想要使用支持 Sass/SCSS 的扩展版Hugo的话，可以使用如下命令：\nchoco install hugo-extended -confirm\rScoop (Windows)\n如果使用 Windows 并且使用 Scoop 作为包管理器，可以使用如下一行代码来安装 Hugo ：\nscoop install hugo\r如果想要使用支持 Sass/SCSS 的扩展版Hugo的话，可以使用如下命令：\nscoop install hugo-extended\rhugo 生成站点 创建一个用于存放网站文件的文件夹， 进入到这个文件夹，使用命令创建网站， 自定义一个站点名称\nhugo new site \u0026lt;BLOG NAME\u0026gt; 创建成功会看到官方的指导流程\nCongratulations! Your new Hugo site is created in D:\\software\\Scoop\\apps\\hugo-extended\\test. Just a few more steps and you\u0026#39;re ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; command. 2. Perhaps you want to add some content. You can add single files with \u0026#34;hugo new \u0026lt;SECTIONNAME\u0026gt;\\\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 3. Start the built-in live server via \u0026#34;hugo server\u0026#34;. Visit https://gohugo.io/ for quickstart guide and full documentation. 然后你会在该位置发现一个站点名称命名的文件夹，进入这个文件夹\nhugo 主题 我选择的是Stack主题，当然你可以去官网或者github选择你喜欢的主题Complete List | Hugo Themes (gohugo.io)\ncd \u0026lt;BLOG NAME\u0026gt; git clone https://github.com/CaiJimmy/hugo-theme-stack.git ./themes/hugo-theme-stack ## 复制stack的exampleSite下的配置, 并删除原本的config.toml cp -rf themes/hugo-theme-stack/exampleSite/config.yaml ./ rm ./config.toml 编辑config.yaml， 注意修改baseurl， 其他配置自定义修改吧\nbaseurl:https://tweakzx.github.io/languageCode:zh-cntheme:hugo-theme-stackpaginate:5title:Tweakzx...上传Github的方式 其实这里分为两种编译方式\n  在线编译：上传整个站点文件，在线编译生成html静态文件，\n  本地编译：本地编译，直接上传静态文件， 也就是public 文件夹\n  在线编译的方式需要在Github Action里找到Hugo 编译的 Action模板直接用就可以了\n 我选择了使用本地编译然后上传静态文件的方式， 主要原因是当时不太懂，瞎搞的， 于是顺其自然了， 要是能重来， 我要选在线编译。\n 本地编译上传静态文件也是有好处的\n 配置简单， 无需复杂操作 本地编译成功的东西部署之后可以立刻看到， 不用等github服务器慢吞吞的编译， 如果本地成功编译预览没有问题，部署后看到的肯定也没有问题， 如果自己做了一些自定义的设置， 可能在GIthub端可能就不一样，甚至无法正常编译 而且自己的配置信息可以不用被别人看到， 如果有一些敏感信息也比较安全   注意，一定要上传到仓库的default分支才可以，default分支一般是main分支， 我创建的比较早所以是master。\n Github Action的配置 进入Github 仓库， 点击Settings-\u0026gt;Code and automations-\u0026gt;Pages\n可以看到有个Static HTML的workflow， 点击配置\n\rimage-20230207155747151\r\n完成之后就可以在上方看到一个网址， 点击访问就可以看到网站部署成功。\n\rimage-20230207155914140\r\n撰写博客的流程 创建博文 我个人的博客创建时会用【】框住主题， 然后起名，这样方便我分类并查找。\nhugo new ./content/post/【主题】博文题目.md 编辑博文 使用Typora编辑刚刚创建的md文档， 在顶端设置好配置信息， 例如作者， 头图等等。\ntitle:\u0026#34;【博客】Hugo+Typora+PicGO+Github搭建个人博客\u0026#34;author:\u0026#34;Tweakzx\u0026#34;date:2022-08-23T10:57:40+08:00description:categories:tags:image:draft:true本地预览 本地编辑完成后， 或者完成过程中， 可以预览效果， 将draft:true改为false\n然后在网站根目录运行\nhugo server 单击http://localhost:1313/, 即可本地预览。\n上传Github 为了实现一键更新博客， 需要写一个bat脚本， 但是如果你想要使用手动更新的话， 这一步可以不用做\n新建并编辑upload.bat 文件， 双击上传\ncd \u0026lt;网站根目录路径\u0026gt; hugo cd public git add . git commit -m \u0026#34;updatde-blog-post\u0026#34; git push origin master:master 个人技巧 我会在./content文件下另外创建两个文件夹， 一个todo， 一个doing。todo文件只有题目， 是打算写的文章； doing是写了一半的文章\n当我开始写文章时\n 如果只是打算写但是现在不想写， 我会创建到todo文件夹 如果马上就要开始写，我会创建到doing文件夹 开始写todo的文章时我会把该文章移入doing文件夹 如果大体上已经写完，可以发布，日后只需要一些简单的编辑的话，我会放入post文件夹  picgo图床的搭建 我选择使用github作为图床， 方便简单， 就是不好访问。\n可能之后会转向七牛云， 不过七牛云需要一个固定的域名， 如果没有的话就比较麻烦了。\n创建github仓库  创建一个github仓库， 必须是public 在个人设置里Settings-\u0026gt;Developer settings生成一个token，repo要打勾， 不过具体的权限与时间自行斟酌吧。 复制这个token  安装并配置Picgo  下载安装PicGo is Here | PicGo 配置picgo  仓库名：刚刚创建的仓库名 分支名：注意是main还是master Token：刚刚生成的token 存储路径， 如果有路径 配置CDN： 使用 https://cdn.jsdelivr.net/gh/ 用户名/仓库名@main 即可    配置giscus评论区   在giscus官网，输入自定义配置，生成相关配置ID giscus\n 生成repoID 生成categoryID 选择合适的主题    修改config.yaml即可\n  giscus:\rrepo: \u0026quot;Tweakzx/Tweakzx.github.io\u0026quot;\rrepoID: \u0026quot;\u0026quot;\rcategory: \u0026quot;Announcements\u0026quot;\rcategoryID: \u0026quot;\u0026quot;\rmapping: \u0026quot;title\u0026quot;\rlightTheme: \u0026quot;light_tritanopia\u0026quot;\remitMetadata: 0\r","date":"2023-02-07T10:57:40+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202302071614641.png","permalink":"https://tweakzx.github.io/p/%E5%8D%9A%E5%AE%A2hugo-typora-picgo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","title":"【博客】Hugo+Typora+PicGO+Github搭建个人博客"},{"content":"利用坚果云实现Obsidian的多端同步 win端下载插件Remotely Save  启用obsidian的第三方插件 浏览社区插件市场并安装Remotely Save 安装完成后如图所示  \rimage-20230204151516303\r\n注册坚果云  进入账户信息-\u0026gt;安全选项-\u0026gt;第三方应用管理 点击添加应用，配置名称生成密码，名称自己设置，可以是Obsidian等等 记得复制生成的密码  win端同步设置  打开Remotely Save的设置页面 选择远程服务 Webdav 填写  坚果云的服务器地址：https://dav.jianguoyun.com/dav/ 自己的账户名 刚刚生成的密码   检查可否连接 重启插件生效 单击侧边栏的↺同步文件  \rimage-20230204152135645\r\nipad端与Android端同步设置   创建与win端的同步仓库同名的仓库\n  进入设置安装Remotely Save插件\n  打开电脑端设置-\u0026gt;第三方插件-\u0026gt;Remotely Save-\u0026gt;导入导出部分设置\n  在导出侧边点击生成QR code\n  打开ipad的相机 / Android手机的扫一扫功能\n  扫描这个QRcode导入配置信息即可\n  自定义设置以及其他  建议打开ipad和安装端的启动后同步  步骤：基本设置-\u0026gt;启动后运行一次-\u0026gt;启动后第1s运行一次 我个人电脑端为主要进行读写操作，其他端为读操作   可以打开同步配置文件夹  步骤：进阶设置-\u0026gt;同步配置文件夹-\u0026gt;打开 打开后会同步.obsidian文件夹，Obsidian的设置也会在各个平台同步，不过我觉得没有必要   安卓端扫码比较费劲， 但是多试几次也可以成功  ","date":"2023-02-04T15:07:34+08:00","image":"https://obsidian.md/images/screenshot-1.0-hero-combo.png","permalink":"https://tweakzx.github.io/p/obsidianwin-ipad-andriod%E5%88%A9%E7%94%A8%E5%9D%9A%E6%9E%9C%E4%BA%91%E4%B8%89%E7%AB%AF%E5%90%8C%E6%AD%A5/","title":"【Obsidian】win+ipad+andriod利用坚果云三端同步"},{"content":"如何在k8s集群中挂载基于NFS的StorageClass 配置NFS服务器 查看系统版本 # cat /etc/redhat-release  CentOS Linux release 7.7.1908 (Core) CentOS 7.4 以后，支持 NFS v4.2 不需要 rpcbind 了，但是如果客户端只支持 NFC v3 则需要 rpcbind 这个服务\n安装服务端 sudo yum install nfs-utils  只安装 nfs-utils 即可，rpcbind 属于它的依赖\n 配置服务端 设置 NFS 服务开机启动\nsudo systemctl enable rpcbind sudo systemctl enable nfs 启动 NFS 服务\nsudo systemctl start rpcbind sudo systemctl start nfs 防火墙需要打开 rpc-bind 和 nfs 的服务, 但是我的k8s集群已经关闭了防火墙， 所以这一步我掠过了。\nsudo firewall-cmd --zone=public --permanent --add-service={rpc-bind,mountd,nfs} success sudo firewall-cmd --reload success 配置共享目录 服务启动之后，我们在服务端配置一个共享目录\nsudo mkdir /data sudo chmod 755 /data 根据这个目录，相应配置导出目录\nsudo vi /etc/exports 添加如下配置\n/data/ 192.168.0.0/24(rw,sync,no_root_squash,no_all_squash)  /data: 共享目录位置。 192.168.0.0/24: 客户端 IP 范围，* 代表所有，即没有限制。 rw: 权限设置，可读可写。 sync: 同步共享目录。 no_root_squash: 可以使用 root 授权。 no_all_squash: 可以使用普通用户授权。  :wq 保存设置之后，重启 NFS 服务。\nsudo systemctl restart nfs 测试验证 在本机验证， 查看共享目录\nshowmount -e localhost Export list for localhost: /data 192.168.0.0/24 在其他节点上验证, 安装启动NFS服务， 查看共享目录\nshowmount -e 192.168.0.110\rExport list for localhost:\r/data 192.168.0.0/24\r在K8s中配置并挂载StorageClass 配置Deployment 创建一个文件nfs-client.yaml， 内容如下，将环境变量 NFS_SERVER 和 NFS_PATH 替换，当然也包括下面的 nfs 配置。\nkind:DeploymentapiVersion:apps/v1metadata:name:nfs-client-provisionerspec:replicas:1strategy:type:Recreateselector:matchLabels:app:nfs-client-provisionertemplate:metadata:labels:app:nfs-client-provisionerspec:serviceAccountName:nfs-client-provisionercontainers:- name:nfs-client-provisionerimage:quay.io/external_storage/nfs-client-provisioner:latestvolumeMounts:- name:nfs-client-rootmountPath:/persistentvolumesenv:- name:PROVISIONER_NAMEvalue:fuseim.pri/ifs- name:NFS_SERVERvalue:10.151.30.57# 需要替换成自己的ip地址- name:NFS_PATHvalue:/data/k8s # 需要替换成自己的路径volumes:- name:nfs-client-rootnfs:server:10.151.30.57# 需要替换成自己的ip地址path:/data/k8s # 需要替换成自己的路径配置SA 我们可以看到上面使用了一个名为 nfs-client-provisioner 的serviceAccount，所以我们也需要创建一个 sa，然后绑定上对应的权限。\n创建文件nfs-client-sa.yaml， 内容如下\napiVersion:v1kind:ServiceAccountmetadata:name:nfs-client-provisioner---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:nfs-client-provisioner-runnerrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;persistentvolumes\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;delete\u0026#34;]- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;persistentvolumeclaims\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;update\u0026#34;]- apiGroups:[\u0026#34;storage.k8s.io\u0026#34;]resources:[\u0026#34;storageclasses\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;events\u0026#34;]verbs:[\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;patch\u0026#34;]- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;endpoints\u0026#34;]verbs:[\u0026#34;create\u0026#34;,\u0026#34;delete\u0026#34;,\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;]---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:run-nfs-client-provisionersubjects:- kind:ServiceAccountname:nfs-client-provisionernamespace:defaultroleRef:kind:ClusterRolename:nfs-client-provisioner-runnerapiGroup:rbac.authorization.k8s.io我们新建了一个名为 nfs-client-provisioner 的ServiceAccount，然后绑定了一个名为 nfs-client-provisioner-runner 的ClusterRole，而该ClusterRole声明了一些权限，其中就包括对persistentvolumes的增、删、改、查等权限，所以我们可以利用该ServiceAccount来自动创建 PV。\n配置StogrageClass对象 创建文件nfs-client-class.yaml， 内容如下\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:course-nfs-storageprovisioner:fuseim.pri/ifs我们声明了一个名为 course-nfs-storage 的StorageClass对象，注意下面的provisioner对应的值一定要和上面的Deployment下面的 PROVISIONER_NAME 这个环境变量的值一样。\n创建资源 kubectl create -f nfs-client.yaml kubectl create -f nfs-client-sa.yaml kubectl create -f nfs-client-class.yaml 创建完成之后我们就可以查看资源状态\n# kubectl get pods NAME READY STATUS RESTARTS AGE nfs-client-provisioner-7ddfdb5759-jthzf 1/1 Running 0 153m # kubectl get storageclass NAME PROVISIONER AGE course-nfs-storage fuseim.pri/ifs 154m 参考链接  CentOS 7 下 yum 安装和配置 NFS - Zhanming\u0026rsquo;s blog (qizhanming.com) StorageClass · 从 Docker 到 Kubernetes 进阶手册 (qikqiak.com)  ","date":"2023-01-10T11:24:28+08:00","permalink":"https://tweakzx.github.io/p/kubernetes%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%8C%82%E8%BD%BD%E5%9F%BA%E4%BA%8Enfs%E7%9A%84storageclass/","title":"【Kubernetes】在k8s集群中挂载基于NFS的StorageClass"},{"content":"在k8s集群上部署测试Adaptdl 环境配置  Centos Linux realease 7.9.2009 三台（GPU节点），CentOS Linux release 7.7.1908 一台（Master节点） 内核：3.10.0-1062.18.1.el7.x86_64 Docker-ce：18.09.7 Kubernetes：1.16.15 Nvidia Driver： 460.32.03两台，510.47.03一台 Helm: 3.1.1 安装配置Nvidia-docker2， 集群可以使用nvidia.com/gpu 配置NFS，在集群中配置StorageClass  配置docker私有仓库 Adaptdl提供的安装方法Installing the AdaptDL Scheduler — AdaptDL documentation里面提到， 使用--set docker-registry.enabled=true 可以安装一个Docker Registry， 但是这是不安全的， 建议使用别的docker registry来替代， 在安装的时候将--set docker-registry.enabled=true 省略掉。\n我自己尝试使用adaptdl自带的registry结果在submit job的时候无法拉去到镜像， 试了很多别的方法， 发现没有效果， 只能自己搭建私有仓库来尝试解决， 万幸确实解决了。\n在[91]节点上配置registry 拉取镜像\ndocker pull registr 搭建私有仓库\ndocker run -d --name registry -p 5000:5000 -v /opt/registry:/var/lib/registry registry:latest\r查看是否建成\n# ls /opt/registry docker # docker ps |grep registry 4375a31c83e3 registry:latest \u0026#34;/entrypoint.sh /etc…\u0026#34; 44 minutes ago Up 44 minutes 0.0.0.0:5000-\u0026gt;5000/tcp registry # netstat -antup | grep 5000 tcp6 0 0 :::5000 :::* LISTEN 10552/docker-proxy 查看镜像列表， 你可以发现此刻仓库镜像为空\n# curl http://10.10.108.91:5000/v2/_catalog  {\u0026#34;repositories\u0026#34;:[]} 在其他节点[10, 14, 15]上使用私有仓库 vim /etc/docker/daemon.json 修改daemon.json， 写入以下内容\n\u0026quot;insecure-registries\u0026quot;: [ \u0026quot;10.10.108.91:5000\u0026quot; ],\r之后重启docker\nsystemctl daemon-reload systemctl restart docker 从其他节点上上传镜像到仓库 docker pull busybox\rdocker tag busybox:latest 10.10.108.91:5000/busybox:latest\rdocker push 10.10.108.91:5000/busybox:latest\r先下载busybox， 改名之后再上传到私有仓库， 之后查看镜像列表， 发现私有仓库里多了busybox\n# curl http://10.10.108.91:5000/v2/_catalog  {\u0026#34;repositories\u0026#34;:[\u0026#34;adaptdl-submit\u0026#34;,\u0026#34;busybox\u0026#34;]} 私有仓库配置成功\n安装Helm 建议下载helm3， 具体版本支持策略可以看Helm | Helm版本支持策略， 安装方法参考Helm | 安装Helm\nwget https://get.helm.sh/helm-v3.1.1-linux-amd64.tar.gz tar -zxvf helm-v3.1.1-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm # 查看是否安装成功 helm version Helm安装Adaptdl Adaptdl官方提供的安装策略 helm install adaptdl adaptdl-sched --repo https://github.com/petuum/adaptdl/raw/helm-repo --namespace adaptdl --create-namespace --set docker-registry.enabled=true\r我执行这个命令， 出现了很多问题， 例如\n 获取https://github.com/petuum/adaptdl/raw/helm-repo 超时 无法识别\u0026ndash;create-namespace 参数 validator-webhook.yaml报错Invalid value: []string{\u0026ldquo;v1\u0026rdquo;}: must include at least one of v1beta1 安装成功后， 仓库拉取镜像超时  我的安装过程 本地下载adpatdl-sched\nwget https://raw.githubusercontent.com/petuum/adaptdl/helm-repo/adaptdl-sched-0.2.11.tgz\r解包修改一点内容\ntar -zxvf adaptdl-sched-0.2.11.tgz vim adaptdl-sched/templates/validator-webhook.yaml 将admissionReviewVersions由v1改为v1beta1\nwebhooks: - name: {{ .Release.Name }}-validator.{{ .Release.Namespace }}.svc.cluster.local clientConfig: caBundle: {{ $cert.Cert | b64enc }} service: name: {{ .Release.Name }}-validator namespace: {{ .Release.Namespace }} path: \u0026#34;/validate\u0026#34; rules: - operations: [\u0026#34;CREATE\u0026#34;, \u0026#34;UPDATE\u0026#34;] apiGroups: [\u0026#34;adaptdl.petuum.com\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;adaptdljobs\u0026#34;] admissionReviewVersions: - v1beta1 sideEffects: None 重新打包\nhelm package adaptdl-sched 创建namespace\nkubectl create namespace adaptdl\r安装adaptdl\nhelm install adaptdl adaptdl-sched-0.2.11.tgz --namespace adaptdl 使用私有仓库\nexport ADAPTDL_SUBMIT_REPO=10.10.108.91:5000/adaptdl-submit export ADAPTDL_SUBMIT_REPO_CREDS=mysecret 查看效果\n# kubectl get pods -A  NAMESPACE NAME READY STATUS RESTARTS AGE adaptdl adaptdl-adaptdl-sched-67c844b5b6-zpqk8 3/3 Running 0 56m adaptdl adaptdl-validator-5f48976d4d-kr9cf 1/1 Running 0 56m 测试 Hello-world 这一部分参考Submitting a Simple Job — AdaptDL documentation\nmkdir hello_world\rcd hello_world\r创建文件 一共要创建三个文件\n hello_world.py Dockerfile adaptdljob.yaml  hello_world/hello_world.py\nimport adaptdl.env import os import time print(\u0026#34;Hello, world!\u0026#34;) with open(os.path.join(adaptdl.env.share_path(), \u0026#34;foo.txt\u0026#34;), \u0026#34;w\u0026#34;) as f: f.write(\u0026#34;Hello, world!\u0026#34;) time.sleep(100) hello_world/Dockerfile\nFROMpython:3.7-slimRUN python3 -m pip install adaptdlCOPY hello_world.py /root/hello_world.pyENV PYTHONUNBUFFERED=true hello_world/adaptdljob.yaml\napiVersion:adaptdl.petuum.com/v1kind:AdaptDLJobmetadata:generateName:hello-world-spec:template:spec:containers:- name:maincommand:- python3- /root/hello_world.py提交任务 adaptdl submit hello_world/ --checkpoint-storage-class=course-nfs-storage 查看效果\n# kubectl get pods -A  default hello-world-vbfp9-3f6dd75e-cf18-447a-a49a-3e1f39234b7a-0-0 1/1 Running 0 6s # adaptdl ls Name Status Start(UTC) Runtime Rplc Rtrt hello-world-vbfp9 Running Jan-10 07:29 0 min 1 0 # kubectl logs hello-world-6r2h6-57779eff-089b-47d9-ab44-fb23d7433518-0-0  Hello, world! BERT 这部分的代码在adaptdl/examples/BERT中\ndocker build的时候， 只能读取与Dockerfile同目录以下的内容， 目录外的内容无法使用， 所以你可能要对Dockerfile做一下修改， 然后将adaptdl的源码文件复制一份到BERT/目录下， 然后\nadaptdl submit examples/BERT/ --checkpoint-storage-class=course-nfs-storage\r查看运行状态，以及输出\n# kubectl logs mlm-task-bnqxw-390a44d3-ca34-4cb9-9ea7-091b1d4ec4db-0-0 INFO:root:Downloading file wikitext-2-v1.zip to .data/wikitext-2-v1.zip. wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:05\u0026lt;00:00, 811kB/s] INFO:root:File .data/wikitext-2-v1.zip downloaded. INFO:root:Opening zip file .data/wikitext-2-v1.zip. INFO:root:Creating train data INFO:root:Creating test data INFO:root:Creating valid data 1lines [00:00, 3.76lines/s] INFO:root:File .data/wikitext-2-v1.zip already exists. INFO:root:Opening zip file .data/wikitext-2-v1.zip. INFO:root:.data/wikitext-2/ already extracted. INFO:root:.data/wikitext-2/wiki.test.tokens already extracted. INFO:root:.data/wikitext-2/wiki.valid.tokens already extracted. INFO:root:.data/wikitext-2/wiki.train.tokens already extracted. INFO:root:Creating train data INFO:root:Creating test data INFO:root:Creating valid data 问题  Error: No Docker registry could be found!\n export ADAPTDL_SUBMIT_REPO=10.10.108.91:5000/adaptdl-submit export ADAPTDL_SUBMIT_REPO_CREDS=mysecret 解决：记得在Master节点上设置仓库的环境变量\n Unsupported storageclass from available storageclasses []\n https://github.com/petuum/adaptdl/issues/112#issue-1115791420\n解决：配置StorageClass， 在提交job的时候指定--checkpoint-storage-class=...\n参考链接  Installing the AdaptDL Scheduler — AdaptDL documentation Submitting a Simple Job — AdaptDL documentation docker私有镜像仓库的配置和使用 - 知乎 (zhihu.com) AdaptDL hello_world can not run · Issue #112 · petuum/adaptdl (github.com) Helm | Helm版本支持策略 Helm | 安装Helm Submitting a Simple Job — AdaptDL documentation  ","date":"2023-01-09T21:15:48+08:00","permalink":"https://tweakzx.github.io/p/kubernetes%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E6%B5%8B%E8%AF%95adaptdl/","title":"【Kubernetes】在k8s集群安装测试adaptdl"},{"content":"在Kubernetes集群中使用GPU资源 在前文中， 我们搭建了k8s集群， 但是k8s原生不支持GPU资源， 需要使用各大GPU厂商开发的插件才能使用。\n这一部分大家可以参考NVIDIA/k8s-device-plugin: NVIDIA device plugin for Kubernetes (github.com)\n本文就记录我在k8s集群中配置插件的过程。\n安装前的环境配置  Centos Linux realease 7.9.2009 内核：3.10.0-1062.18.1.el7.x86_64 Docker-ce：18.09.7 Kubernetes：1.16.15 Nvidia Driver： 460.32.03两台，510.47.03一台  安装nvidia-docker2  如果你使用的是新版本的docker (\u0026gt;=19.03)，则推荐使用nvidia-container-toolkit包来代替nvidia-docker2包：\n 由于我的docker版本是18.09.7， 所以我需要安装nvidia-docker2\n##设置仓库 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \\ sudo tee /etc/yum.repos.d/nvidia-docker.repo ##更新仓库中的key DIST=$(sed -n \u0026#39;s/releasever=//p\u0026#39; /etc/yum.conf) DIST=${DIST:-$(. /etc/os-release; echo $VERSION_ID)} sudo yum makecache ##安装nvidia-docker2 sudo yum install nvidia-docker2 这时候， 查看/etc/docker/daemon.json，你会发现内容已经改了， 内容如下\n{ \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 但是， 我还有一些其他的设置， 例如cgroupdriver， 镜像等等也需要设置， 所以我的修改了daemon.json，内容如下\n{ \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://n73pm3wf.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ], \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 修改完之后，记得重启docker\n##重启docker systemctl restart docker 特别注意一个字段\u0026quot;default-runtime\u0026quot;: \u0026quot;nvidia\u0026quot;, 这个default-runtime需要加上才可以在k8s中找到设备。\n加了之后重启docker， 使用\ndocker run --security-opt=no-new-privileges --cap-drop=ALL --network=none -it -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins nvidia/k8s-device-plugin:1.11 如果成功的话会出现\n2023/01/09 12:56:36 Loading NVML\r2023/01/09 12:56:36 Fetching devices.\r2023/01/09 12:56:37 Starting FS watcher.\r2023/01/09 12:56:37 Starting OS watcher.\r2023/01/09 12:56:37 Starting to serve on /var/lib/kubelet/device-plugins/nvidia.sock\r2023/01/09 12:56:37 Registered device plugin with Kubelet\r如果不加的话，使用如下命令也可以验证docker是否可以使用GPU\ndocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi 等待几秒，下载完成后出现会以下结果\n# docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\rMon Jan 9 12:33:59 2023 +-----------------------------------------------------------------------------+\r| NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 |\r|-------------------------------+----------------------+----------------------+\r| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r| | | MIG M. |\r|===============================+======================+======================|\r| 0 Tesla V100-PCIE... Off | 00000000:02:00.0 Off | 0 |\r| N/A 28C P0 24W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r| 1 Tesla V100-PCIE... Off | 00000000:03:00.0 Off | 0 |\r| N/A 29C P0 25W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r| 2 Tesla V100-PCIE... Off | 00000000:82:00.0 Off | 0 |\r| N/A 30C P0 24W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r| 3 Tesla V100-PCIE... Off | 00000000:83:00.0 Off | 0 |\r| N/A 28C P0 23W / 250W | 0MiB / 16384MiB | 0% Default |\r| | | N/A |\r+-------------------------------+----------------------+----------------------+\r+-----------------------------------------------------------------------------+\r| Processes: |\r| GPU GI CI PID Type Process name GPU Memory |\r| ID ID Usage |\r|=============================================================================|\r| No running processes found |\r+-----------------------------------------------------------------------------+\r在k8s上启动nvidia-device-plugin 在Master节点上运行\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml 然后使用以下命令查看\nkubectl get pods -A -owide 可以发现， 相关的pod已经起来了，对应每个修改了docker运行时的GPU节点都有一个pod\nkube-system nvidia-device-plugin-daemonset-2bhzz 1/1 Running 0 59m\rkube-system nvidia-device-plugin-daemonset-l5djd 1/1 Running 0 59m\rkube-system nvidia-device-plugin-daemonset-lt756 1/1 Running 0 60m\r使用\nkubectl describe nodes\r可以发现可分配资源里出现了nvidia.com/gpu的字段\nCapacity:\rcpu: 56\rephemeral-storage: 2148327Mi\rhugepages-1Gi: 0\rhugepages-2Mi: 0\rmemory: 65678476Ki\rnvidia.com/gpu: 4\rpods: 110\rAllocatable:\rcpu: 56\rephemeral-storage: 2027415715761\rhugepages-1Gi: 0\rhugepages-2Mi: 0\rmemory: 65576076Ki\rnvidia.com/gpu: 4\rpods: 110\rSystem Info:\r测试 我们使用官方文档里的测试方法\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f -\rapiVersion: v1\rkind: Pod\rmetadata:\rname: gpu-pod\rspec:\rrestartPolicy: Never\rcontainers:\r- name: cuda-container\rimage: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\rresources:\rlimits:\rnvidia.com/gpu: 1 # requesting 1 GPU\rtolerations:\r- key: nvidia.com/gpu\roperator: Exists\reffect: NoSchedule\rEOF\r可以得到\n# kubectl logs gpu-pod [Vector addition of 50000 elements]\rCopy input data from the host memory to the CUDA device\rCUDA kernel launch with 196 blocks of 256 threads\rCopy output data from the CUDA device to the host memory\rTest PASSED\rDone\r遇到的问题  Failed to initialize NVML: could not load NVML library.If this is a GPU node, did you set the docker default runtime to nvidia?\n 你可能没有设置`\u0026ldquo;default-runtime\u0026rdquo;: \u0026ldquo;nvidia\u0026quot;字段， 设置之后重启docker， 将pod全部删除重启，一定要注意操作顺序， 安装docker， 配置/etc/docker/daemon.json， 重启docker， 启动k8s-device-plugin, 进行测试。\nkubectl delete -n kube-system \u0026lt;POD NAME\u0026gt;\r如果需要给不同型号GPU打Label # Label your nodes with the accelerator type they have.\rkubectl label nodes \u0026lt;node-with-k80\u0026gt; accelerator=nvidia-tesla-k80\rkubectl label nodes \u0026lt;node-with-p100\u0026gt; accelerator=nvidia-tesla-p100\r调用的时候， 指定nodeSelector条件\napiVersion: v1\rkind: Pod\rmetadata:\rname: cuda-vector-add\rspec:\rrestartPolicy: OnFailure\rcontainers:\r- name: cuda-vector-add\r# https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile\rimage: \u0026quot;k8s.gcr.io/cuda-vector-add:v0.1\u0026quot;\rresources:\rlimits:\rnvidia.com/gpu: 1\rnodeSelector:\raccelerator: nvidia-tesla-p100 # or nvidia-tesla-k80 etc.\r","date":"2023-01-09T20:06:52+08:00","permalink":"https://tweakzx.github.io/p/kubernetes%E5%A6%82%E4%BD%95%E8%AE%A9kubernetes%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8gpu/","title":"【Kubernetes】如何让kubernetes集群使用GPU"},{"content":"基于深度学习的机器学习集群的任务放置 这是一篇发表在INFOCOM'2019上的论文， 特点是使用了强化学习进行任务放置。\nAbstract   背景\n 虽然作业之间的服务器共享提高了资源利用率，但位于ML 作业之间的干扰可能会导致性能显着下降。 现有的集群调度程序（例如，Mesos）在其作业布置中是忽视干扰的，导致资源效率不佳。 干扰感知工作安置已在文献中进行了研究，但使用详细的工作负载分析和干扰建模进行了处理，这不是通用的解决方案。    Harmony\n 这是一种深度学习驱动的 ML 集群调度程序，它以最小化干扰和最大化性能（即训练完成时间）的方式放置训练作业。 Harmony 是基于一个精心设计的深度强化学习(DRL)框架，并辅以奖励建模。 DRL 采用了最先进的技术来稳定训练和提高收敛性，包括行为者-批评算法、任务感知行为空间探索和经验重放。 鉴于普遍缺乏对应于不同放置决策的奖励样本，我们建立了一个辅助奖励预测模型，该模型使用历史样本进行训练，用于为看不见的放置产生奖励。    实验\n 在 6 台 GPU 服务器的 Kubernetes 集群中使用真实 ML 工作负载进行的实验 Harmony 在平均作业完成时间方面优于代表性调度程序 25%。    Introduction   任务干扰来源\n 许多现有的集群调度器倾向于超额分配，比如 CPU 和内存等资源，以最大化资源利用率(假设并非所有作业在任何时候都完全使用所需的资源)。 作业还共享底层资源，如 CPU 缓存、磁盘 I/O、网络 I/O 和总线(例如 QPI、 PCIe)。  例如，当服务器上的图形处理器卡被分配到不同的机器学习作业时，当作业在它们分配的 CPU 和图形处理器之间洗牌数据时，PCIe 总线被共享; 当两个分配的图形处理器没有连接到同一个非均匀访存模型体系结构中的 CPU 时，QPI 总线被共享。      不同类型任务之间的资源争用\n 一些机器学习任务是 CPU 密集型的，例如 CTC [7] ; 一些是磁盘 I/O 密集型的，例如 AlexNet [8] ，因为需要读取图像进行预处理; 还有一些由于模型大小(参数数量)和小批量(导致工人之间更频繁的参数交换) ，例如 VGG-16[9] ，网络带宽消耗水平很高。    将干扰程度较低的工作放在一起以优化性能是一个自然的想法。\n  现有调度器很大程度上是忽视干扰，这主要是由于难以获得许多作业的潜在干扰级别。\n  考虑干扰的工作\n 许多工作展示了干扰感知调度的潜力和有效性  考虑 MapReduce 作业中的网络争用 HPC 作业的缓存访问强度。   这些研究基于某些观察/假设建立了目标性能的显式干扰模型，并依靠手工设计的启发式方法将干扰纳入调度。 他们通常需要在数十个干扰源下进行详细的应用程序分析，并相应地仔细优化性能模型中的系数或启发式中的阈值。    痛点：通用性是这些白盒方法的一个问题：当工作负载类型或硬件配置发生变化时，启发式方法可能无法正常工作。\n    Harmony：一种黑盒方法，考虑干扰的同时，无需详细的性能建模\n 发现了ML工作负载之间共享资源时严重的性能下降， 提出了使用DRL来调度工作负载，自适应变化 采用了很多训练技术以确保DRL收敛到合适的放置策略， 包括actor-critic algorithm, job-aware action space exploration， experience replay。 建立了一个辅助的奖励预测模型， 用于为看不见的放置产生奖励。 在k8s上实现了原型， 实验评估发现， harmony比普遍策略性能高出25%    系统概述  提交一个ML任务，用户需要提交以下信息：    运行worker和PS的资源需求    使用的worker和PS的数量    训练数据集的epoch数     每个作业的worker和参数服务器的布置在整个训练过程中都不会改变  \rimage-20230104120024756\r\n 工作流程：  提交job，使用DRL模型， 得到放置决策 收集放置任务后的Trace， 进行有监督学习，预测奖励，得到奖励预测模型 使用奖励预测模型生成训练样本， 结合一系列State， 通过决策训练网络得到放置决策， 形成反馈 定期更新DRL模型    离线训练 使用纯粹的在线训练效果很差， 所以需要提前进行离线训练。离线训练共分为两步\n 奖励模型训练  通过历史工作轨迹，Harmony 使用监督学习训练奖励预测神经网络。 输入包括作业集信息和放置状态；标签是每项工作的奖励（训练速度）。 该模型通过相应的安置决策为任何工作集提供快速奖励评估。   DRL模型训练  DRL NN 将各种作业集和集群资源可用性作为输入，并为该集中的新作业生成放置决策。 通过奖励预测模型，我们可以有效地扩展了可用trace set并为 DRL 训练生成足够的样本。    在线推理和模型更新  在每个调度间隔中，Harmony 通过对 DRL NN 的推理来决定新作业批次的放置，并观察与放置决策相对应的实际奖励。 我们使用在线收集的样本定期重新训练 DRL NN 和奖励 NN，以随着时间的推移不断改进决策。  基于深度强化学习的任务放置 DRL框架  状态空间 s = (x, r, w, p, v, d)  x：是任务n模型的独热编码。具有相同架构和批量大小的DNN被认为是同一种模型。 r：是一个2(1+K)维的向量，编码worker和PS的资源需求。其中K是组成一个worker或PS的资源类型的数量。第一个值代表任务n需要的worker的数量，下K个值表示每个worker的K种资源的需求，剩余的1+K个值表示PS的数量及其资源需求。对于All-Reduce架构来说，将PS的资源需求置为0即可。 w, p：一个整数，表示分配给任务n的worker和PS的数量。 v：一个M*K的矩阵，表示每个服务器上每种资源的可用数量，M是物理服务器的数量 d：一个M*2N大小的向量，编码任务n的worker和PS在服务器上的放置。   动作空间：依据s和policy $\\pi(s, a)$ 选择相应的动作a  使用一个神经网络进行policy的训练 空间大小为2MN', N\u0026rsquo;是新到的任务数 (n, 0, m)表示服务器m给任务n分配一个worker (n, 1, m,)表示分配一个PS   奖励  目标是最小化平均任务完成时间 但是任务完成时间需要完成任务才知道 所以使用的是速度的求和， cn表示间隔内的已训练epoch， en是设定的总的epoch \rimage-20230104124524937\r   NN Model  \rimage-20230104120548893\r 每个作业或每个服务器的状态分别连接到一个全连接层，然后在输出层之前连接到几个全连接层。 这样，NN 可以从每个作业或每个服务器中提取特征，然后再合并为一个整体。 为了尊重服务器资源容量，在 NN 的输出层，我们通过在策略分布中将其概率设置为 0 来屏蔽无效操作，然后我们重新调整所有动作的概率，使总和仍然等于 1。    DRL模型训练  我们使用强化学习来训练策略NN， 每一个样本都是一个四元组(s, a, r, s')。（状态， 动作， 奖励， 更新后的状态）。 目标：最大化累计折扣奖励  \rimage-20230104125552347\r   训练技巧    Actor-critic：基本思想是引入依赖于状态的基线函数，以改进 SGD 中用于更新策略 NN 的梯度。    Exploration：确保充分探索行动空间， 以获得良好的Policy    Experience replay: 使用连续样本训练 RL 模型很难收敛，采用经验回放来减轻样本序列中的相关性。      奖励预测模型 我们设计了一个奖励模型，可以预测给定作业和集群状态的奖励，我们可以基于该模型为 DRL 训练生成大量样本。\n\rimage-20230104130738864\r\n NN 架构  NN 的输入状态是 DRL NN 输入的子集：(x, w, p, d)。 输出的是预测的任务训练速度 并发作业中 worker 和 PS 的资源需求不包括在内，因为它们通常可以从作业的模型类型中推断出来。输出是一个向量，包括输入作业的预测训练速度输入状态连接到输出层之前的一系列隐藏的全连接层。 在实践中，我们发现与更复杂的神经层相比，全连接层在我们的场景中工作得很好。   NN 训练  我们通过使用历史痕迹中的可用样本进行监督学习来训练 NN。 我们通过计算预测和标签的相对误差，将神经网络产生的每个作业 n 的预测训练速度 cn 与标签 c′n，即轨迹中每个作业 n 的训练速度进行比较。 然后我们使用 SGD 更新 NN 中的参数以最小化整体相对误差。 我们使用历史轨迹中的样本迭代地训练神经网络，使得神经网络产生的预测收敛于可接受的相对误差（例如 10%）。    ","date":"2023-01-04T10:16:46+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202301041018146.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0harmony%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Harmony论文阅读笔记"},{"content":"Tiresias 论文阅读笔记 Abstract  深度学习 (DL) 训练作业给现有的集群管理器带来了一些独特的挑战，例如  不可预测的训练时间 全有或全无的执行模型 GPU 共享的不灵活性   我们对生产中的大型 GPU 集群的分析表明，现有的大数据调度程序会导致  较长的排队延迟 较低的整体性能   我们介绍了 Tiresias  这是一个为分布式 DL 训练作业量身定制的 GPU 集群管理器，它可以有效地安排和放置 DL 作业以减少它们的作业完成时间 (JCT)。 鉴于 DL 作业的执行时间通常是不可预测的，我们提出了两种调度算法——  离散化二维Gittins索引：基于部分信息 离散化二维 LAS： 与信息无关，旨在最小化平均 JCT   此外，我们描述了何时可以放宽合并放置约束，并提出了一种放置算法来利用这些观察结果而无需任何用户输入。   在具有 60 个 P100 GPU 的密歇根 ConFlux 集群上进行的实验和大规模跟踪驱动模拟表明，  与生产中使用的基于 Apache YARN 的资源管理器相比，Tiresias 将平均 JCT 提高了 5.5 倍。 更重要的是，Tiresias 的性能与假设完美知识的解决方案的性能相当。    Introduction 由于 DDL 训练的独特限制，我们观察到当前集群管理器设计中的两个主要限制。\n 训练时间不可预知的朴素调度  尽管已知最短作业优先 (SJF) 和最短剩余时间优先 (SRTF) 算法可以最小化平均 JCT ，但它们需要作业的剩余执行时间，而这对于 DL 训练作业来说通常是未知的. Optimus可以依靠其重复执行模式并假设其损失曲线会收敛来预测 DL 训练作业的剩余执行时间。然而，这些提议对具有平滑损失曲线和运行完成的工作做出了过于简化的假设；在生产系统中，两者都不总是正确的。 正因为如此，生产中最先进的资源管理器相当天真。例如，微软内部的解决方案是从 Apache YARN 最初为大数据作业构建的 Capacity Scheduler 扩展而来的。它只执行基本的编排，即作业到达时的非抢占式调度。因此，当集群超额订阅时，用户经常会遇到长时间的排队延迟——即使是小作业也可能长达数小时。   放置任务过程中过度激进的合并  现有的集群管理器还试图将 DDL 作业整合到具有足够 GPU 的最少数量的服务器上。例如，一个有 16 个 GPU 的作业在每台服务器 4 个 GPU 的集群中至少需要四台服务器，如果找不到四台完全空闲的服务器，作业可能会被阻塞。 基本假设是应尽可能避免使用网络通信，因为它可能成为瓶颈并浪费 GPU 周期 。然而，我们发现这个假设只是部分有效。    在本文中，我们提出了 Tiresias，一种共享 GPU 集群管理器，旨在解决上述有关 DDL 作业调度和放置的挑战。为确保Tiresias实用且易于部署，我们依靠对生产作业轨迹的分析、对训练各种 DL 模型的详细测量以及两个简单而有效的想法。此外，我们有意让 Tiresias 对用户保持透明，即所有现有作业都可以在没有任何额外的用户指定配置的情况下运行。\n 第一个想法：一个新的调度框架（2DAS），旨在在 DL 作业的执行时间不可预测时最小化 JCT  调度算法  Discretized 2D Gittins index Discretized 2D-LAS   上述两个方案都是给任务一个优先级，前者使用Gittins索引，后者直接应用收到的任务(随着时间会改变，任务会按照优先级来进行调度) 使用上述策略有两个挑战  计算任务优先级的时候。需要同时考虑空间和时间两个维度。空间：GPU个数， 时间：任务运行时间。 相对优先级可能会随着工作接受服务二发生变化， 可能会导致工作被抢占， GPU抢占DDL工作的代价昂贵， 所以为了避免抢占， 作业的优先级需要在固定的时间间隔发生变化。   有先验的情况下使用Gittins索引， 没有先验的情况下使用LAS   第二个想法：尽可能使用模型结构来放松合并放置约束  我们观察到只有某些类型的 DL 模型对其是否合并敏感，并且它们的敏感性是由于其模型中张量大小分布的偏差。我们使用这种洞察将工作分为两类：  对整合敏感（高偏差）的工作 其他工作   我们在 Tiresias 中实现了一个 RDMA 网络分析库，它可以通过网络级活动确定 DDL 作业的模型结构。  通过利用分析库和 DDL 训练的迭代特性，Tiresias 可以透明且智能地放置作业。 Tiresias 首先在试用环境中运行作业几次迭代，然后根据从先前测量中总结的标准确定最佳放置策略。      本文的贡献如下：\n Tiresias是第一个信息不可知的GPU集群资源管理器。同时也是第一个使用了两个维度的扩展和优先级来调度DDL工作。 使用了一个简单的， 外部可观察的， model-specific的评价标准来判断什么时候放松GPU合并的约束 设计简单，易部署， 性能显著提高  背景和动机 DDL分布式深度学习 这里只关注数据并行\n\rimage-20221227122558146\r\n 周期性迭代 参数服务器架构 Trial and error 的探索： 超参数调优  挑战  不可预知的作业时间  当前预测 DL 作业训练时间的解决方案 都假设 DL 作业  (1) 具有平滑的损失曲线 (2) 达到其训练目标并完成   然而  对于许多在试错探索过程中较差的模型，它们的损失曲线并不像探索结束时最终选择的最佳模型的曲线那样平滑。如图2  \rimage-20221227123048934\r   DL的终止条件是不确定的   因此，实际的资源管理器设计不应依赖于准确性/损失曲线来预测最终的作业完成时间。   过度激进的任务合并  在模型聚集阶段尝试减少网络的通信在分布式训练中是一种通用的优化，因为网络可能是性能瓶颈并且浪费GPU周期。 然而，许多现存的GPU管理在放置分布式深度学习任务时盲目地遵从一个合并约束，特别地，他们将作业的所有组件（参数服务器和Worker）分配给相同或最小数量的服务器 一个分布式深度学习作业如果不能合并通常会等待，即使集群中有足够的空闲资源，虽然这个约束是为了高性能，但是会导致更长的队列延迟和低的资源利用。   抢占式的时间开销  由于时间开销大，目前的生产集群并没有抢占作业。 \rimage-20221227124028866\r \rimage-20221227124044585\r    潜在的收益 破除两个谬误， 可以获得巨大的提升\n 谬误一：如果没有确切的工作持续时间，就无法很好地安排工作。  尽管 DDL 作业持续时间通常是不可预测的，但可以从历史日志中了解它们的总体分布。 广泛用于解决经典多臂老虎机问题的 Gittins 指数策略，只要给定工作持续时间分布，就可以降低平均 JCT。 即使没有这些信息，LAS 算法也可以根据获得的服务有效地安排作业。   谬误二：DDL 作业应该始终合并。  虽然合并放置作业确实可以最大限度地减少其通信时间，但我们发现某些 DDL 作业对放置不敏感。 我们确定核心因素是模型结构    Tiresias 的设计 总体架构  调度目标  用户为中心：最小化平均JCT 操作者为中心：提高GPU的利用率 平衡以上二者：任务不能无限饥饿   一些假设  任务在线达到 任务持续时间未知 任务的任务特性未知 All or Nothing的资源分配   任务的生命周期  Tiresias 旨在优化上述目标，而无需在特定 DL 框架下对作业的资源需求、持续时间或其内部特征做出任何假设。 \rimage-20221227125201039\r 图 6 展示了 Tiresias 的架构以及作业生命周期中发生的一系列操作。  1）作业一提交，其 GPU 要求就已知，并且它被附加到 WAITQUEUE 。 2）调度程序定期调度来自 WAITQUEUE 的作业，并在作业到达、作业完成和资源可用性变化等事件时抢占集群中正在运行的作业到 WAITQUEUE。 3）首次启动作业或恢复先前抢占的作业时，调度程序依赖于放置模块来分配其 GPU。 4）如果一个作业是第一次启动，放置模块首先对其进行剖析——剖析器识别作业特定的特征，例如张量分布中的偏斜，以确定是否合并该作业 。      调度 我们观察到抢占式调度对于实现调度目标来说是必要的。\n为什么是二维调度 通过回顾基于时间或大小的启发式方法，我们认为在 GPU 资源有限的集群上调度 DDL 作业时仅考虑一个方面（空间或时间）是不够的。\n 在 SRTF 调度程序中，剩余时间短的大型作业会占用许多 GPU，从而导致许多新提交的小型作业出现不可忽略的排队延迟。 如果调度程序是GPU 的数量最小优先SF的，那么大型作业可能会被一连串的小型作业阻塞，即使它们接近完成  \rimage-20221227130428691\r\n2DAS 二维的获得性基于服务的调度器 2DAS是对传统LAS的扩展，同时考虑了时间和空间，它会赋予任务一个优先级，这个优先级和时间以及空间有关。\n而这个优先级函数有不同的情况，当有没有先验知识的时候，即没有任务持续时间的分布的时候，使用LAS算法，如果有先验分布，则使用Gittins索引\n\rimage-20221227132002748\r\n优先级的离散化 使用连续的优先级会导致一系列的抢占和一系列的重启，对于分布式深度学习任务来说，抢占和重启的代价很高，过多的抢占会导致2DAS不可用\n为了解决这个问题，基于传统的**多级别反馈队列(MLFQ)**算法实现优先级离散化的框架。即，将原有的一个队列变成K个队列。\n 整体结构确保具有相似 (WJtJ) 值的作业保留在同一队列中。具有高度不同 (WJtJ) 值的作业保持在不同的优先级。 使用 LAS 时，同一队列中的作业按其开始时间的 FIFO 顺序进行调度（即，首次调度它们的时间），没有任何 HOL 阻塞的风险。 Gittins 指数中的服务量 Δ 也是离散化的。对于 Qi 中的作业，Δi 等于 Qhi i，这是 Qi 的上限。当一个作业用完它的所有服务量时，它将被降级到较低优先级的队列。对于 Gittins 索引，同一队列中的作业根据其 Gittins 索引值进行调度。在最后一个队列 QK 中，ΔK 被设置为 ∞。在这种极端情况下，Gittins 索引的表现与 LAS 类似，最后一个队列中的作业按照 FIFO 顺序进行调度。  \rimage-20221227131451335\r\n放置 给定一个任务，需要参数服务器以及Worker，如果有足够的资源，Tiresias需要知道是否在尽可能少的机器中合并一些任务的GPU或者去分发它们，前者在微软的的生产集群中实现，故一个任务即使资源足够也可能被放置在等待队列。本文使用ILP，即整数线性规划来优化这个分配问题。\n合并对于任务来说重要吗? 深度学习模型中对于合并敏感的一般都有较大的张量， 原因是模型聚合中的消息大小与模型的结构密切相关。 例如，TensorFlow中的模型由许多张量组成。 每个张量都被包装为单个通信消息。因此，DDL中的消息大小分布取决于模型的张量大小分布。 张量大小通常分布不均匀; 有时存在巨大的张量，其中包含这些模型中的大部分参数。 因此，聚合较大的张量会更严重地受到网络争用的影响，而较小张量的传输往往会相互交错。\n利用这个直觉，设计了Tiresias的分析器，用于分析每个模型的偏差程度，再使用放置的算法 \rimage-20221227132216616\r\n总结  与 Apache YARN 的容量调度程序 (YARNCS) 和 Gandiva 相比，Tiresias 旨在最小化平均 JCT。 与 Optimus 不同，Tiresias 可以在没有或有部分先验知识的情况下有效地安排工作（表 2）。 此外，Tiresias 可以根据 Tiresias 分析器自动捕获的模型结构巧妙地放置 DDL 作业。  实现 中心Master 除了启动新作业和完成现有作业外，master 的一个主要功能是当它们的（GPU）资源被调度程序分配给其他作业时抢占正在运行的作业。\n由于 DL 作业的迭代性质，我们不需要将所有数据保存在 GPU 和主内存中以进行作业抢占。\n目前，我们使用几乎所有 DL 框架提供的检查点功能来为抢占作业保存最新模型。\n当触发抢占时，作业首先被暂停；然后它的首席工作者将它的模型检查点到一个集群范围的共享文件系统。\n当调度程序再次恢复暂停的作业时，将在重新启动之前加载其最近的检查点。中央主机还使用放置算法和分析器确定作业的放置。\n分布式RDMA监控 由于 RDMA 在 GPU 集群中广泛用于 DDL 作业，我们将分析器实现为可加载库，用于拦截 RDMA ibverbs API。\n因此，它可以记录每个服务器上的所有 RDMA 活动，例如建立连接、发送和接收数据。所有相关工作人员和参数服务器的 RDMA 级信息随后在中央分析器中聚合。\n基于聚合信息（例如，消息大小和总流量），Tiresias 可以解析给定 DDL 作业的详细模型信息，包括其偏差。\n虽然是为 RDMA 网络实现的，但分析器可以通过拦截套接字 API 轻松扩展以支持 TCP/IP 网络。\n","date":"2022-12-27T09:46:16+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212270951184.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0tiresias%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Tiresias论文阅读笔记"},{"content":"Tensorflow kernal launch 的过程 分析session执行的过程， 并分析Antman对执行过程的修改\n函数调用链 Run()\u0026ndash;\u0026gt;RunInternel()\u0026ndash;\u0026gt;RunAsync()\u0026ndash;\u0026gt;ScheduleReady()\u0026ndash;\u0026gt;Process()\n修改了direct_session.cc , 在session执行前后运行中间件框架\n修改了executor.cc ， 新增一个异步调用队列， 并将需要插入时间槽的异步Op加入队列，在OpManager线程中等待执行。\n\rimg\r\nSession的执行 Session的代码逻辑在TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/direct_session.cc的Run()函数中，\nDirectSession::Run Status DirectSession::Run(const RunOptions\u0026amp; run_options, const NamedTensorList\u0026amp; inputs, const std::vector\u0026lt;string\u0026gt;\u0026amp; output_names, const std::vector\u0026lt;string\u0026gt;\u0026amp; target_nodes, std::vector\u0026lt;Tensor\u0026gt;* outputs, RunMetadata* run_metadata) { //判断计算图是否构建  TF_RETURN_IF_ERROR(CheckNotClosed()); TF_RETURN_IF_ERROR(CheckGraphCreated(\u0026#34;Run()\u0026#34;)); //计数器  direct_session_runs-\u0026gt;GetCell()-\u0026gt;IncrementBy(1); // Extract the inputs names for this run of the session.  //提取输入的张量名字和张量大小  std::vector\u0026lt;string\u0026gt; input_tensor_names; input_tensor_names.reserve(inputs.size()); size_t input_size = 0; for (const auto\u0026amp; it : inputs) { input_tensor_names.push_back(it.first); input_size += it.second.AllocatedBytes(); } metrics::RecordGraphInputTensors(input_size); // Check if we already have an executor for these arguments.  // 检查是否已经创建执行器， 没有的话， 创建  // 一般情况下 每个设备都有一个执行器， 负责这个设备上计算子图的执行  ExecutorsAndKeys* executors_and_keys; RunStateArgs run_state_args(run_options.debug_options()); run_state_args.collective_graph_key = run_options.experimental().collective_graph_key(); TF_RETURN_IF_ERROR(GetOrCreateExecutors(input_tensor_names, output_names, target_nodes, \u0026amp;executors_and_keys, \u0026amp;run_state_args)); { mutex_lock l(collective_graph_key_lock_); collective_graph_key_ = executors_and_keys-\u0026gt;collective_graph_key; } // Configure a call frame for the step, which we use to feed and  // fetch values to and from the executors.  //设置函数调用帧的参数， Tensorflow使用feed和fetch字典来和执行器进行数据交互  //feed是输入， fetch是输出  //构建FunctionCallFrame call_frame, Session与执行器之间相互交互  //处理执行器的输入与输出  FunctionCallFrame call_frame(executors_and_keys-\u0026gt;input_types, executors_and_keys-\u0026gt;output_types); gtl::InlinedVector\u0026lt;Tensor, 4\u0026gt; feed_args(inputs.size()); for (const auto\u0026amp; it : inputs) { if (it.second.dtype() == DT_RESOURCE) { Tensor tensor_from_handle; TF_RETURN_IF_ERROR( ResourceHandleToInputTensor(it.second, \u0026amp;tensor_from_handle)); feed_args[executors_and_keys-\u0026gt;input_name_to_index[it.first]] = tensor_from_handle; } else { feed_args[executors_and_keys-\u0026gt;input_name_to_index[it.first]] = it.second; } } // 设置输入参数  const Status s = call_frame.SetArgs(feed_args); if (errors::IsInternal(s)) { return errors::InvalidArgument(s.error_message()); } else if (!s.ok()) { return s; } const int64 step_id = step_id_counter_.fetch_add(1); if (LogMemory::IsEnabled()) { LogMemory::RecordStep(step_id, run_state_args.handle); } //准备好执行环境之后， 开始调用RunInternal执行计算  TF_RETURN_IF_ERROR(RunInternal(step_id, run_options, \u0026amp;call_frame, executors_and_keys, run_metadata, thread::ThreadPoolOptions())); ... ... // 获取并处理计算图的执行结果 } DirectSession::RunInternal() // RunInternal会启动多个并行的执行器， // 创建执行器的barrier， 确保执行器都执行完， 执行完后返回Run()函数 Status DirectSession::RunInternal( int64 step_id, const RunOptions\u0026amp; run_options, CallFrameInterface* call_frame, ExecutorsAndKeys* executors_and_keys, RunMetadata* run_metadata, const thread::ThreadPoolOptions\u0026amp; threadpool_options) { const uint64 start_time_usecs = options_.env-\u0026gt;NowMicros(); const int64 executor_step_count = executors_and_keys-\u0026gt;step_count.fetch_add(1); ////////////////////////////////////////////////////////  // Running all pre session run action in grouping //  // 在session计算执行之前添加SessionRunActionRegistry //  // 以运行在session开始之前的中间件 //  ////////////////////////////////////////////////////////  SessionRunActionOptions action_options; action_options.device_mgr = \u0026amp;device_mgr_; action_options.sess_ptr = this; TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::PRE_SESSION_RUN, action_options)); //  //  //标记运行状态  RunState run_state(step_id, \u0026amp;devices_); ... ... // profiler TraceMe  //构建 IntraProcessRendezvous 用于本地Tensor管理  run_state.rendez = new IntraProcessRendezvous(device_mgr_.get()); ... ... // ifndef _ANDROID  // Start parallel Executors.  //开始并行执行器  //构建 ExecutorBarrier 用于协调多个 Executor 并行计算，保持 graph 一致性  const size_t num_executors = executors_and_keys-\u0026gt;items.size(); ExecutorBarrier* barrier = new ExecutorBarrier( num_executors, run_state.rendez, [\u0026amp;run_state](const Status\u0026amp; ret) { { mutex_lock l(run_state.mu_); run_state.status.Update(ret); } run_state.executors_done.Notify(); }); ... ... //构建args  // Register this step with session\u0026#39;s cancellation manager, so that  // `Session::Close()` will cancel the step.  ... ...//处理`Session::Close()`  // Use std::unique_ptr to ensure garbage collection  //创建线程池实际运行执行器  std::unique_ptr\u0026lt;thread::ThreadPool\u0026gt; threadpool_wrapper; thread::ThreadPool* pool = nullptr; ...//设置线程池  //异步启动执行器  for (const auto\u0026amp; item : executors_and_keys-\u0026gt;items) { thread::ThreadPool* device_thread_pool = item.device-\u0026gt;tensorflow_device_thread_pool(); if (!device_thread_pool) { args.runner = default_runner; } else { args.runner = [this, device_thread_pool](Executor::Args::Closure c) { device_thread_pool-\u0026gt;Schedule(std::move(c)); }; } if (handler != nullptr) { args.user_intra_op_threadpool = handler-\u0026gt;AsIntraThreadPoolInterface(); } /////////////// 执行器的启动///////////////////  item.executor-\u0026gt;RunAsync(args, barrier-\u0026gt;Get()); /////////////////////////////////////////////  } //等待执行结果  WaitForNotification(\u0026amp;run_state, \u0026amp;step_cancellation_manager, run_options.timeout_in_ms() \u0026gt; 0 ? run_options.timeout_in_ms() : operation_timeout_in_ms_); ... ... //保存运行结果  if (!run_state.tensor_store.empty()) { TF_RETURN_IF_ERROR(run_state.tensor_store.SaveTensors( {executors_and_keys-\u0026gt;callable_options.fetch().begin(), executors_and_keys-\u0026gt;callable_options.fetch().end()}, \u0026amp;session_state_)); } ... ... ///////////////////////////////////////////////////////////  // Running all post session run action in grouping //  // 在session计算执行结束之后添加SessionRunActionRegistry， //  // 以运行在session结束之后的中间件 //  ///////////////////////////////////////////////////////////  uint64 session_end_time = tensorflow::Env::Default()-\u0026gt;NowMicros(); action_options.sess_duration_us = time_duration_usecs; action_options.graph_id = reinterpret_cast\u0026lt;uint64\u0026gt;(executors_and_keys); TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::POST_SESSION_RUN, action_options)); return Status::OK(); } 执行器逻辑 ExecutorState::RunAsyn() # ExecutorState::RunAsync 的实现 # 概述：初始化ready队列， 开启线程池 void ExecutorState::RunAsync(Executor::DoneCallback done) { const Graph* graph = impl_-\u0026gt;graph_.get(); TaggedNodeSeq ready; // 获取 context map，即运行时上下文  Device* device = impl_-\u0026gt;params_.device; const Status fill_status = device-\u0026gt;FillContextMap(graph, \u0026amp;device_context_map_); if (!fill_status.ok()) { done(fill_status); return; } // 初始化 ready 队列，即存放入度为0的node  for (const Node* n : impl_-\u0026gt;root_nodes_) { DCHECK_EQ(n-\u0026gt;in_edges().size(), 0); ready.push_back(TaggedNode{n, root_frame_, 0, false}); } if (ready.empty()) { done(Status::OK()); } else { num_outstanding_ops_ = ready.size(); root_frame_-\u0026gt;iterations[0]-\u0026gt;outstanding_ops = ready.size(); done_cb_ = std::move(done); // 线程池入口  ScheduleReady(ready, nullptr); } } ExecutorState::ScheduleReady() # ExecutorState::ScheduleReady 的实现 # 概述：将节点分为 expensive \u0026amp; inexpensive 节点，将inexpensive节点放入 inline_ready 中 void ExecutorState::ScheduleReady(const TaggedNodeSeq\u0026amp; ready, TaggedNodeReadyQueue* inline_ready) { if (ready.empty()) return; int64 scheduled_usec = 0; if (stats_collector_) { scheduled_usec = nodestats::NowInUsec(); } if (inline_ready == nullptr) { // 运行所有 ready ops \t// 运行ready队列里的节点 ready是当前线程要处理的队列  for (auto\u0026amp; tagged_node : ready) { runner_([=]() { Process(tagged_node, scheduled_usec); }); } return; } // 将节点分类，运行 expensive node  const GraphView\u0026amp; gview = impl_-\u0026gt;gview_; const TaggedNode* curr_expensive_node = nullptr; for (auto\u0026amp; tagged_node : ready) { const NodeItem\u0026amp; item = *gview.node(tagged_node.node-\u0026gt;id()); if (tagged_node.is_dead || !item.kernel_is_expensive) { //  inline_ready-\u0026gt;push_back(tagged_node); } else { //对于高开销节点启动新的线程去执行  if (curr_expensive_node) { runner_(std::bind(\u0026amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec)); } curr_expensive_node = \u0026amp;tagged_node; } } if (curr_expensive_node) { //高开销节点  if (inline_ready-\u0026gt;empty()) { // inline_ready为空， 将首个高开销节点放入inline_ready  inline_ready-\u0026gt;push_back(*curr_expensive_node); } else { // inline_ready不为空， 将高开销节点放入其他线程中执行  runner_(std::bind(\u0026amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec)); } } ... ... } ExecutorState::Process() # ExecutorState::Process 详解 # 概述：线程池中跑的内容，代码太长不贴了。 # 主要流程： # + 将当前节点添加到 inline_ready 队列中。 # + 循环从 inline_ready 队列获取节点并运行，运行完毕后执行 NodeDone（有可能会添加新节点到inline_ready队列） # + 当inline ready队列为空时，跳出循环。 # 其他重要内容： # + 运行节点通过 device 的 ComputeAsync 或 Compute 方法 # + 处理输出结果使用 ProcessOutputs 函数和 PropagateOutputs 函数 # + 计算结束后通过 NodeDone 来收尾 void ExecutorState::Process(TaggedNode tagged_node, int64 scheduled_nsec) { WithContext wc(context_); ... ... // Parameters passed to OpKernel::Compute.  TensorValueVec inputs; DeviceContextVec input_device_contexts; AllocatorAttributeVec input_alloc_attrs; OpKernelContext::Params params; params.step_id = step_id_; // Override device\u0026#39;s threadpool if user provides an intra_op_threadpool  Device* device = impl_-\u0026gt;params_.device; ... ... bool completed = false; inline_ready.push_back(tagged_node); uint64 sess_op_num = 0; //循环处理inline_ready中的每个节点 直到为空  while (!IsAsyncGPUOpQueueEmpty() || !inline_ready.empty()) { tagged_node = inline_ready.front(); inline_ready.pop_front(); ... ... //准备输入数据， 确保输入是有效的  s = PrepareInputs(item, first_input, \u0026amp;inputs, \u0026amp;input_device_contexts, \u0026amp;input_alloc_attrs, \u0026amp;is_input_dead); ... ... // 绝大多数的Op是同步计算模式， send/recv是异步计算模式  if (item.kernel_is_async) { //异步计算, send/recv是高开销的  launched_asynchronously = true; Device* kernel_device = impl_-\u0026gt;params_.device; // Only enqueue this op if it is an async GPU op.  if (need_to_insert_idle_time_ \u0026amp;\u0026amp; (kernel_device-\u0026gt;name()).find(\u0026#34;GPU\u0026#34;) != string::npos) { //////////////////////////////////////////////////////////////////////  // 把这个GPU Op放入async_gpu_op_queue队列中 如果需要在它启动之前插入时间槽的话  // Enqueue this GPU op therefore we can insert a time slot before launching this op.  // 将原本执行异步计算代码的Op放入自定义的async_gpu_op_queue队列中，  // 交由OpManager执行  //////////////////////////////////////////////////////////////////////  sess_op_num++; ... ... // Enqueue this asyn GPU op.  async_gpu_op_queue_lock_.lock(); async_gpu_op_queue.emplace_back(async_gpu_kernel); num_queued_op.fetch_add(1); async_gpu_op_queue_lock_.unlock(); } else { /////////////////////////////////////////////////////////////////////  // 不需要插入时间槽， 所以不放入async_gpu_op_queue队列  // Do not enqueue this op.  // 调用原本的计算异步的函数  /////////////////////////////////////////////////////////////////////  device-\u0026gt;ComputeAsync(async, \u0026amp;state-\u0026gt;ctx, done); } else { // 同步计算  // Synchronous computes.  OpKernelContext ctx(\u0026amp;params, item.num_outputs); nodestats::SetOpStart(stats); ... ... //进行计算 deivce-\u0026gt;Compute(op_kernel, \u0026amp;ctx)  nodestats::SetOpEnd(stats); //处理输出  s = ProcessOutputs(item, \u0026amp;ctx, \u0026amp;outputs, stats); ... ... //传播输出  if (s.ok()) { PropagateOutputs(tagged_node, \u0026amp;item, \u0026amp;outputs, \u0026amp;ready); } ... ... //传播后处理  //结束  completed = NodeDone(s, item.node, ready, stats, \u0026amp;inline_ready); } } // while !inline_ready.empty()  if (sess_op_num \u0026gt; 0) { // Record the total number of the queued op running in this session.  GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm != nullptr) { rm-\u0026gt;SetExecutorQueuedOpNum(impl_, sess_op_num); } } // This thread of computation is done if completed = true.  if (completed) ScheduleFinish(); } ExecutorState::AsyncGPUOpManager() void ExecutorState::AsyncGPUOpManager() { uint64 sleep_time_us = 0; need_to_insert_idle_time_ = false; GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm == nullptr) { return; } while (!terminate_op_magager_thread_) { //设置队列中的Op是否需要插入时间槽  need_to_insert_idle_time_ = rm-\u0026gt;GetEstimatedIdleTime() \u0026gt; 0 ? true : false; std::function\u0026lt;void(void)\u0026gt; queued_call_func = nullptr; async_gpu_op_queue_lock_.lock(); if (!async_gpu_op_queue.empty()) { queued_call_func = async_gpu_op_queue.front(); } async_gpu_op_queue_lock_.unlock(); if (queued_call_func != nullptr) { queued_call_func(); async_gpu_op_queue_lock_.lock(); if (!async_gpu_op_queue.empty()) { async_gpu_op_queue.erase(async_gpu_op_queue.begin()); num_queued_op.fetch_sub(1); } async_gpu_op_queue_lock_.unlock(); // Estimate idle time  uint64 idle_time = rm-\u0026gt;GetEstimatedIdleTime(); uint64 queued_op_num = rm-\u0026gt;GetExecutorQueuedOpNum(impl_); idle_time = queued_op_num \u0026gt; 0 ? (idle_time / queued_op_num) : 0; usleep(idle_time); uint64 remain_time = rm-\u0026gt;GetEstimatedIdleTime(); remain_time = remain_time \u0026gt; idle_time ? (remain_time - idle_time) : 0; rm-\u0026gt;SetEstimatedIdleTime(remain_time); } usleep(default_check_interval); } return; } Antman对内存分配器的修改 主要新增了自己的vmen内存分配器， 调用host的内存\n在TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/gpu/gpu_process_state.cc#做了修改\nAllocator* GPUProcessState::GetGPUAllocator(const GPUOptions\u0026amp; options, TfGpuId tf_gpu_id, size_t total_bytes) { CHECK(process_state_); #if (defined(GOOGLE_CUDA) \u0026amp;\u0026amp; GOOGLE_CUDA) || \\ (defined(TENSORFLOW_USE_ROCM) \u0026amp;\u0026amp; TENSORFLOW_USE_ROCM)  const string\u0026amp; allocator_type = options.allocator_type(); mutex_lock lock(mu_); GpuIdUtil::CheckValidTfGpuId(tf_gpu_id); if (tf_gpu_id.value() \u0026gt;= static_cast\u0026lt;int64\u0026gt;(gpu_allocators_.size())) { gpu_allocators_.resize(tf_gpu_id.value() + 1); } AllocatorParts\u0026amp; allocator_parts = gpu_allocators_[tf_gpu_id.value()]; if (allocator_parts.allocator == nullptr) { // Validate allocator types.  if (!allocator_type.empty() \u0026amp;\u0026amp; allocator_type != \u0026#34;BFC\u0026#34;) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Invalid allocator type: \u0026#34; \u0026lt;\u0026lt; allocator_type; return nullptr; } PlatformGpuId platform_gpu_id; TF_CHECK_OK(GpuIdManager::TfToPlatformGpuId(tf_gpu_id, \u0026amp;platform_gpu_id)); int bus_id = BusIdForGPU(tf_gpu_id); DCHECK_GE(bus_id, 0); while (bus_id \u0026gt;= gpu_visitors_.size()) { gpu_visitors_.push_back({}); } se::StreamExecutor* stream_exec = GpuIdUtil::ExecutorForPlatformGpuId(platform_gpu_id).ValueOrDie(); GPUMemAllocator* sub_allocator = new GPUMemAllocator( stream_exec, platform_gpu_id, (options.per_process_gpu_memory_fraction() \u0026gt; 1.0 || options.experimental().use_unified_memory()), gpu_visitors_[bus_id], {}); GPUBFCAllocator* gpu_bfc_allocator = new GPUBFCAllocator(sub_allocator, total_bytes, options, strings::StrCat(\u0026#34;GPU_\u0026#34;, tf_gpu_id.value(), \u0026#34;_bfc\u0026#34;)); Allocator* gpu_allocator = gpu_bfc_allocator; // GPUVMemAllocator will allocate host memory as backup after running out of  // gpu device memory to avoid OOM failures  //////////////////////////////////////////////////////////////////////////////////  gpu_allocator = maybe_create_gpu_vmem_allocator(gpu_allocator, bus_id, platform_gpu_id, tf_gpu_id.value(), stream_exec); //////////////////////////////////////////////////////////////////////////////////  SharedCounter* timing_counter = nullptr; if (options.experimental().timestamped_allocator()) { timing_counter = new SharedCounter; gpu_bfc_allocator-\u0026gt;SetTimingCounter(timing_counter); } // If true, checks for memory overwrites by writing  // distinctive patterns on both ends of allocated memory.  if (useCudaMemoryGuardAllocator()) { gpu_allocator = new GPUDebugAllocator(gpu_allocator, platform_gpu_id); gpu_allocator = new GPUNanResetAllocator(gpu_allocator, platform_gpu_id); } else if (useCudaMallocAllocator()) { // If true, passes all allocation requests through to cudaMalloc  // useful for doing memory debugging with tools like cuda-memcheck  // **WARNING** probably will not work in a multi-gpu scenario  gpu_allocator = new GPUcudaMallocAllocator(gpu_allocator, platform_gpu_id); } Allocator* recording_allocator = nullptr; if (process_state_-\u0026gt;ProcessState::FLAGS_brain_gpu_record_mem_types) { ProcessState::MemDesc md; md.loc = ProcessState::MemDesc::GPU; md.dev_index = platform_gpu_id.value(); md.gpu_registered = false; md.nic_registered = true; recording_allocator = new internal::RecordingAllocator( \u0026amp;process_state_-\u0026gt;mem_desc_map_, gpu_allocator, md, \u0026amp;mu_); } allocator_parts = {std::unique_ptr\u0026lt;Allocator\u0026gt;(gpu_allocator), std::unique_ptr\u0026lt;SharedCounter\u0026gt;(timing_counter), gpu_bfc_allocator, sub_allocator, std::unique_ptr\u0026lt;Allocator\u0026gt;(recording_allocator)}; } if (process_state_-\u0026gt;ProcessState::FLAGS_brain_gpu_record_mem_types) { return allocator_parts.recording_allocator.get(); } else { return allocator_parts.allocator.get(); } #else  LOG(FATAL) \u0026lt;\u0026lt; \u0026#34;GPUAllocator unavailable. Not compiled with --config=cuda or \u0026#34; \u0026#34;--config=rocm.\u0026#34;; return nullptr; #endif // GOOGLE_CUDA || TENSORFLOW_USE_ROCM } gpu_process_state 是个单例模式, 只有一个实例存在\n/*static*/ GPUProcessState* GPUProcessState::singleton(GPUProcessState* ps) { static GPUProcessState* instance = ps ? ps : new GPUProcessState; DCHECK((!ps) || (ps == instance)) \u0026lt;\u0026lt; \u0026#34;Multiple calls to GPUProcessState with non-null ps\u0026#34;; return instance; } GPUProcessState::GPUProcessState() : gpu_device_enabled_(false) { process_state_ = ProcessState::singleton(); } ","date":"2022-12-23T18:57:14+08:00","permalink":"https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90tensorflow%E7%9A%84session%E6%89%A7%E8%A1%8C%E5%88%86%E6%9E%90/","title":"【代码分析】Tensorflow的session执行分析"},{"content":"Antman对Tensorflow的代码修改 总体的关系图，主要包括两个实现， 内存方面的GPUResourceManagement以及算力方面的GpuOpManager。\ngraph TD\rAgpu_resource_manage_file]\rB[SessionRunRegistry]\rC[SessionRunAction] D[Executor]\rE[GPUResouceManagement]\rF[GPU Statistic]\rG[GpuOpManager]\rH[GpuUsageAdjustment]\rI(dump gpu statistic)\rJ[GPU Process State]\rK[GPUVMemAllocator]\rL[GPUAdjustableAllocator]\rA --|FileListener| E\rB --|Register| E\rE --|need_to_adjust_memory_| H\rH --|new| L\rH --|get| K\rC --|Derive| E\rC --|Derive| F\rB --|Register| F\rF --|need_to_dump_statistics_| I\rB --|Run| C\rJ --|maybe_create_gpu_vmem_allocator|K\rD --|run thread| G\rE --|GetEstimatedIdleTime| G\r GPUVMemAllocator GPUVMemAllocator 可以分配host的mem作为显存的备用，以免出现OOM错误。\n创建allocator maybe_create_gpu_vmem_allocator(\u0026hellip;)可以根据情况返回合适的allocator\ngraph TD\rA([start]) --|gpu_allocator|B[maybe_create_gpu_vmem_allocator]--C{if !gpu_vmem} C --|true| D([返回gpu_allocator])\rC --|false|Z(准备生成VMemAllocator)\rZ -- E[new GpuHostAllocator]\rE --|sub_allocator| F[new BFCAllocator]\rZ --|gpu_allocator| G[new GPUVMemAllocator]\rF --|host_allocator|G\rG --|gpu_vmem_allocator|H([返回gpu_vmem_allocator])\r Allocator* maybe_create_gpu_vmem_allocator(Allocator* gpu_allocator, int bus_id, PlatformGpuId platform_gpu_id, int tf_gpu_id, se::StreamExecutor* stream_exec) { bool gpu_vmem = false; Status status = ReadBoolFromEnvVar(\u0026#34;TF_GPU_VMEM\u0026#34;, true/*enabled by default*/, \u0026amp;gpu_vmem); if (!status.ok()) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;GetGPUAllocator: \u0026#34; \u0026lt;\u0026lt; status.error_message(); } if (!gpu_vmem) { return gpu_allocator; } SubAllocator* sub_allocator = new GpuHostAllocator( GpuIdUtil::ExecutorForPlatformGpuId(platform_gpu_id).ValueOrDie(), bus_id, {}, {}); int64 cuda_host_mem_limit_in_mb = -1; status = ReadInt64FromEnvVar(\u0026#34;TF_CUDA_HOST_MEM_LIMIT_IN_MB\u0026#34;, 1LL \u0026lt;\u0026lt; 16 /*64GB max by default*/, \u0026amp;cuda_host_mem_limit_in_mb); if (!status.ok()) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;GetGpuHostAllocator: \u0026#34; \u0026lt;\u0026lt; status.error_message(); } int64 cuda_host_mem_limit = cuda_host_mem_limit_in_mb * (1LL \u0026lt;\u0026lt; 20); Allocator* host_allocator = new BFCAllocator(sub_allocator, cuda_host_mem_limit, true /*allow_growth*/, strings::StrCat(\u0026#34;GPUHost_\u0026#34;, tf_gpu_id, \u0026#34;_bfc\u0026#34;)); Allocator* gpu_vmem_allocator = new GPUVMemAllocator(gpu_allocator, host_allocator, tf_gpu_id, stream_exec); return gpu_vmem_allocator; } 分配虚拟内存 先尝试分配GPU内存， 分配成功则返回， 分配失败则分配CPU内存。\nvoid* GPUVMemAllocator::AllocateRaw(size_t alignment, size_t num_bytes) { mutex_lock l(lock_); AllocationAttributes new_attr; // Tell the device_allocator_ not to retry  // since we can alloc host memory as backup  new_attr.no_retry_on_failure = true; void* ret = device_allocator_-\u0026gt;AllocateRaw(alignment, num_bytes, new_attr); if (ret != nullptr) { device_ptrs_.insert(ret); return ret; } ret = host_allocator_-\u0026gt;AllocateRaw(alignment, num_bytes); VLOG(3) \u0026lt;\u0026lt; \u0026#34;host_allocator_ allocates \u0026#34; \u0026lt;\u0026lt; (num_bytes/1024.0/1024) \u0026lt;\u0026lt; \u0026#34; MiB\u0026#34;; return ret; } SessionRunActionRegistry（中间件框架） 添加了一个SessionRunActionRegistry框架， 方便在session开始之前或者结束之后添加执行动作\n修改direct_session.cc 和 master_session.cc /修改了原先的session执行流程 在session执行前后分别执行actions\n// Running all pre session run action in grouping  SessionRunActionOptions action_options; action_options.device_mgr = \u0026amp;device_mgr_; action_options.sess_ptr = this; TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::PRE_SESSION_RUN, action_options)); ... const uint64 time_duration_usecs = options_.env-\u0026gt;NowMicros() - start_time_usecs; metrics::UpdateGraphExecTime(time_duration_usecs); // Running all post session run action in grouping  uint64 session_end_time = tensorflow::Env::Default()-\u0026gt;NowMicros(); action_options.sess_duration_us = time_duration_usecs; action_options.graph_id = reinterpret_cast\u0026lt;uint64\u0026gt;(executors_and_keys); TF_RETURN_IF_ERROR(SessionRunActionRegistry::Global()-\u0026gt;RunGrouping( SessionRunActionRegistry::POST_SESSION_RUN, action_options)); 注册action void SessionRunActionRegistry::Register( Grouping grouping, int phase, SessionRunAction* action) { VLOG(2) \u0026lt;\u0026lt; \u0026#34;Register session run action \u0026#34; \u0026lt;\u0026lt; action-\u0026gt;name(); groups_[grouping][phase].emplace_back(action); } // 一些宏函数: 提供注册中间件的接口 #define REGISTER_SESSION_RUN_ACTION(grouping, phase, action) \\ REGISTER_ACTION_UNIQ_HELPER(__COUNTER__, grouping, phase, action)  #define REGISTER_ACTION_UNIQ_HELPER(ctr, grouping, phase, action) \\ REGISTER_ACTION_UNIQ(ctr, grouping, phase, action)  #define REGISTER_ACTION_UNIQ(ctr, grouping, phase, action) \\ static ::tensorflow::session_run_action_registration:: \\ SessionRunActionRegistration register_session_run_action_##ctr( \\ grouping, phase, new action(), \\ #action)  } // namespace tensorflow  #endif // TENSORFLOW_CORE_COMMON_RUNTIME_SESSION_RUN_ACTION_REGISTRY_H_  //定义了一个RunAction()的接口， Action 必须实现这个接口 class SessionRunAction { public: virtual ~SessionRunAction() {} virtual Status RunAction(const SessionRunActionOptions\u0026amp; options) = 0; void set_name(const string\u0026amp; name) { name_ = name; } std::string name() const { return name_; } private: // The name of the action, which is the same as the inherited  // class name.  string name_; }; GPU memory limit adjustment 实现动态资源分配， 自动调整现存限制， 当发现 Host 内存被使用的时候，会提高显存的限制阈值，这样所有的 Tensor 都可以申请在显卡上。这样只会影响一个 mini batch 的性能，后面的 mini batch 跑前向后向计算的时候，所有的 Tensor 都会被申请在显存上。\n修改了tensorflow 原本的 BFC_Allocator.h 增加了friend class GPUAdjustableAllocator;\n// Declare the GPUAdjustableAllocator to be friend of the BFCAllocator,  // therefore it can adjust the memory limit by modifying the private  // member variables of BFCAllocator.  friend class GPUAdjustableAllocator; 增加了扩缩显存分配的函数 class GPUAdjustableAllocator final { public: // Adjust the memory_limit_ to allow memory grow/shrink at runtime  // Returns adjusted memory_limit_. If the return value is less than  // the new_memory_limit, the adjustment failed.  size_t AdjustMemoryLimit(size_t new_memory_limit, BFCAllocator* bfc_allocator); // Get the memory pool size and in used memory size of the bfc_allocator.  void GetMemPoolStats(BFCAllocator* bfc_allocator, int64_t* deviceMemPoolSize, int64_t* deviceMemStable); private: // Free the memory regions that are not in use  size_t FreeEmptyMemory(size_t target_memory_bytes, BFCAllocator* bfc_allocator) EXCLUSIVE_LOCKS_REQUIRED(lock_); }; size_t GPUAdjustableAllocator::AdjustMemoryLimit(size_t new_memory_limit, BFCAllocator* bfc_allocator) { mutex_lock l(bfc_allocator-\u0026gt;lock_); if (new_memory_limit \u0026gt;= bfc_allocator-\u0026gt;total_region_allocated_bytes_) { // 1) new_memory_limit \u0026gt;= memory_limit_ : grow memory size  // 2) memory_limit_ \u0026gt; new_memory_limit \u0026gt;= total_region_allocated_bytes_:  // shrink, but don\u0026#39;t need to free memory  // In both cases, no action needed by changing the memory limit  bfc_allocator-\u0026gt;memory_limit_ = new_memory_limit; bfc_allocator-\u0026gt;stats_.bytes_limit = new_memory_limit; } else { // total_region_allocated_bytes_ \u0026gt; new_memory_limit:  // shrink, need to free memory  size_t free_res = FreeEmptyMemory( new_memory_limit, bfc_allocator); if (free_res \u0026lt;= new_memory_limit) { bfc_allocator-\u0026gt;memory_limit_ = new_memory_limit; bfc_allocator-\u0026gt;stats_.bytes_limit = new_memory_limit; } else { bfc_allocator-\u0026gt;memory_limit_ = free_res; bfc_allocator-\u0026gt;stats_.bytes_limit = free_res; } } return bfc_allocator-\u0026gt;memory_limit_; } File Listener 监控配置文件是否发生改变， 如果发生改变则触发响应的handler,以及一个回调函数\nvoid FileListener::RegisterFileListener(const std::string\u0026amp; file_path, const std::string\u0026amp; handler_name, callback callback_func) { LOG(INFO) \u0026lt;\u0026lt; \u0026#34;Register a file listener named \u0026#34; \u0026lt;\u0026lt; handler_name \u0026lt;\u0026lt; \u0026#34; on file \u0026#34; \u0026lt;\u0026lt; file_path; FileInfo new_file(file_path); std::vector\u0026lt;CallbackFunc\u0026gt; new_handlers; InfoAndHandlers value = {new_file, new_handlers}; CallbackFunc new_callback(handler_name, callback_func); mutex_lock l(lock_); auto res = listeners_.emplace(file_path, value); res.first-\u0026gt;second.file_handlers_.emplace_back(new_callback); if (file_monitor_thread_ == nullptr) { // Note we should start only one monitor thread  StartMonitorThread(); } } GPU resource management（中间件） 继承了SessionRunAction, 是一个资源管理中间件，定义如下\nclass GPUResourceManagement : public SessionRunAction { public: // Note that we will enable TF_FORCE_GPU_ALLOW_GROWTH and TF_GPU_VMEM  // automatically if the GPUResourceManagement feature is enabled.  GPUResourceManagement(); ~GPUResourceManagement() override; ...\t... //🚨配置文件更新后， 解析新的配置并存放于此  // For recording the parsed new gpu resource limit.  //🚨 GPU 资源限制  std::unordered_map\u0026lt;std::string, GPUResourceLimitInfo\u0026gt; gpu_resource_management_info_; // For recording the parsed new gpu performance limitation  // (if the value is 0, then it means to suspend this job).  // 🚨 GPU 性能限制， 如果值为0， 意味着挂起这个job  std::atomic\u0026lt;int\u0026gt; gpu_perf_control_; // For recording the total time of all inserted time slot.  uint64 total_time_slot_; // For recording the estimated total idle time.  uint64 estimated_total_idle_time_; // For recording the total number of queued GPU op running in  // the specified executor.  // 🚨记录每一个Executor要执行的OP数目  std::unordered_map\u0026lt;const void*, uint64\u0026gt; executor_queued_op_num_; // Determine if we need to adjust the GPU usage limit.  // 🚨表示是否需要更改配置  std::atomic\u0026lt;bool\u0026gt; need_to_adjust_memory_; // For performing the adjustment.  // 修改配置的类的实例  GPUUsageAdjustment* gpu_usage_adjustment_; const std::string FILE_LISTENER_NAME = \u0026#34;GPUResourceManage\u0026#34;; }; GPUResourceManagement() GPUResourceManagement::GPUResourceManagement() : need_to_adjust_memory_(false), gpu_perf_control_(100), gpu_usage_adjustment_(new GPUUsageAdjustment()) { //从环境变量中读取gpu配置文件的路径  ReadStringFromEnvVar(\u0026#34;GPU_CONFIG_FILE\u0026#34;, \u0026#34;\u0026#34;, \u0026amp;gpu_resource_manage_file_path_); if (gpu_resource_manage_file_path_.empty()) { enable_gpu_resource_manage_ = false; } else { enable_gpu_resource_manage_ = true; // Note that we will enable TF_FORCE_GPU_ALLOW_GROWTH and TF_GPU_VMEM  // automatically if the GPUResourceManagement feature is enabled.  setenv(\u0026#34;TF_FORCE_GPU_ALLOW_GROWTH\u0026#34;, \u0026#34;true\u0026#34;, 1); setenv(\u0026#34;TF_GPU_VMEM\u0026#34;, \u0026#34;true\u0026#34;, 1); // Register a handler that will be triggered when the file named  FileListener::GlobalFileListener()-\u0026gt;RegisterFileListener( gpu_resource_manage_file_path_, FILE_LISTENER_NAME, //FILE_LISTENER_NAME: \u0026#34;GPUResourceManage\u0026#34;  [](const std::string\u0026amp; str) { // The callback func which is invoked when file changed.  // 传入一个json文件，包含ManageInfo  // 当文件更改时， 获取相应的在session结束后调用顺序为2的action中名为GPUResourceManagement的action  // action解析新的配置信息  // 等到session 触发RunAction()， 更新限制  SessionRunAction* act = SessionRunActionRegistry::Global()-\u0026gt;GetAction( SessionRunActionRegistry::POST_SESSION_RUN, 2, \u0026#34;GPUResourceManagement\u0026#34;); if (act == nullptr) { std::cout \u0026lt;\u0026lt; \u0026#34;Cannot get the instance of GPUResourceManagement \\n\u0026#34;; } if (act != nullptr) { GPUResourceManagement* rm = dynamic_cast\u0026lt;GPUResourceManagement *\u0026gt;(act); if (rm != nullptr) { rm-\u0026gt;ParseManageInfoFromJson(str); } } }); } } 注册中间件 意味着session 结束前后要执行RunAction（\u0026hellip;）\n#if GOOGLE_CUDA // We register the GPUResourceManagement as a POST_SESSION_RUN action // during the initialization phase of the program. REGISTER_SESSION_RUN_ACTION(SessionRunActionRegistry::POST_SESSION_RUN, 2, GPUResourceManagement); #endif // GOOGLE_CUDA 实现函数RunAction（\u0026hellip;） 如果需要进行显存调整， 则调用GPUUsageAdjustment调整资源\nStatus GPUResourceManagement::RunAction( const SessionRunActionOptions\u0026amp; options) { if (!need_to_adjust_memory_ \u0026amp;\u0026amp; gpu_perf_control_ \u0026gt;= 100) { // TODO(shiru): do we need to unregister the  // GPUResourceManagement if the environment variable  // GPU_CONFIG_FILE is set to null?  return Status::OK(); } if (need_to_adjust_memory_) { mutex_lock l(manage_mu_); // Start to adjust the resource limit as required.  for (const auto\u0026amp; it : gpu_resource_management_info_) { gpu_usage_adjustment_-\u0026gt;AdjustMemLimit(it.first, //GPU总线id  it.second.mem_limit_, options.device_mgr,\t//新的显存限制， 设备管理器  options.device_set);\t//设备集合  } need_to_adjust_memory_ = false; } // 暂停一段时间 或者 挂起这个job  DoSleepOrSuspend(options.sess_duration_us); return Status::OK(); } GPUUsageAdjustment.cc bool GPUUsageAdjustment::AdjustMemLimit(const std::string\u0026amp; gpu_pci_bus_id, size_t new_mem_limit, const std::unique_ptr\u0026lt;const tensorflow::DeviceMgr\u0026gt;* device_mgr, const std::unique_ptr\u0026lt;DeviceSet\u0026gt;* device_set) { mutex_lock l(adj_mu_); //一个记录对应gpu使用情况的map  auto cur_info = cur_usage_info_.find(gpu_pci_bus_id); if (cur_info == cur_usage_info_.end()) { //如果没有相应的使用信息， 则立刻获取使用信息  GPUBFCAllocator* allo = GetGPUAllocator(device_mgr, device_set, gpu_pci_bus_id); if (allo == nullptr) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Failed to get the allocator of gpu_pci_bus_id: \u0026#34; \u0026lt;\u0026lt; gpu_pci_bus_id; return false; } GPUUsageInfo usage_info; usage_info.gpu_allocator_ = allo; usage_info.cur_limit_.mem_limit_ = ULONG_MAX; // Get the VGPU_MEMORY_LIMIT  absl::optional\u0026lt;AllocatorStats\u0026gt; device_stats = allo-\u0026gt;GetStats(); usage_info.cur_limit_.initial_mem_limit_ = device_stats ? *device_stats-\u0026gt;bytes_limit : ULONG_MAX; auto ret = cur_usage_info_.emplace(gpu_pci_bus_id, usage_info); if (ret.second == false) { return false; } cur_info = ret.first; } //如果超出虚拟GPU的使用限制， 则使用上限  if (new_mem_limit \u0026gt; cur_info-\u0026gt;second.cur_limit_.initial_mem_limit_) { // The new mem size limit exceeds VGPU_MEMORY_LIMIT  new_mem_limit = cur_info-\u0026gt;second.cur_limit_.initial_mem_limit_; LOG(WARNING) \u0026lt;\u0026lt; \u0026#34;The new mem size limit exceeds VGPU_MEMORY_LIMIT, \u0026#34; \u0026lt;\u0026lt; \u0026#34;therefore, adjust the new mem size limit to : \u0026#34; \u0026lt;\u0026lt; new_mem_limit; } //如果在限制范围内，并且需要调整， 且调整后的值不为0，  //调用GPUAdjustableAllocator， 更改内存限制， 并且更新使用信息  if (cur_info-\u0026gt;second.cur_limit_.mem_limit_ != new_mem_limit \u0026amp;\u0026amp; new_mem_limit \u0026gt;= 0) { // Adjust the memory limit of this GPU  LOG(INFO) \u0026lt;\u0026lt; \u0026#34;Start to manage the mem size limit to \u0026#34; \u0026lt;\u0026lt; new_mem_limit \u0026lt;\u0026lt; \u0026#34; of device gpu_pci_bus_id: \u0026#34; \u0026lt;\u0026lt; gpu_pci_bus_id; GPUAdjustableAllocator* adj = new GPUAdjustableAllocator(); size_t cur_mem_limit = adj-\u0026gt;AdjustMemoryLimit(new_mem_limit, cur_info-\u0026gt;second.gpu_allocator_); cur_info-\u0026gt;second.cur_limit_.mem_limit_ = cur_mem_limit; if (cur_mem_limit \u0026gt; new_mem_limit) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Failed to manage the mem size limit to \u0026#34; \u0026lt;\u0026lt; new_mem_limit \u0026lt;\u0026lt; \u0026#34; of device gpu_pci_bus_id: \u0026#34; \u0026lt;\u0026lt; gpu_pci_bus_id; // TODO(shiru): need to check is gpu_allocator_ has been changed!  return false; } return true; } return false; } Gpu Statistics （中间件） 在session运行结束后执行，执行顺序为1， 判断是否需要导出GPU统计数据\nStatus GPUStatistics::RunAction(const SessionRunActionOptions\u0026amp; options) { if (!need_to_dump_statistics_) { return Status::OK(); } bool huge_change = RecordSessionRunDuration( options.graph_id, options.sess_duration_us); if (!ShouldCheckGPUStatistics() \u0026amp;\u0026amp; !huge_change) { return Status::OK(); } { // Global lock.  mutex_lock l(check_mu_); bool dur_flag = CheckSessionRunDuration(options.graph_id, options.sess_duration_us); bool stat_flag = CheckGPUVMemAllocatorStatistics(options.device_mgr, options.device_set); if (dur_flag || stat_flag) { dumpGPUStatistics(); } gpu_statistics_last_write_ = time(0); } return Status::OK(); } void GPUStatistics::dumpGPUStatistics() { Json::Value dump_json; Json::Value gpu_info_json; for (const auto\u0026amp; a : allocator_status_lists_) { Json::Value device_json; device_json[\u0026#34;deviceMemUsedMax\u0026#34;] = Json::Int64(a.deviceMemUsedMax); device_json[\u0026#34;deviceMemUsedMin\u0026#34;] = Json::Int64(a.deviceMemUsedMin); device_json[\u0026#34;deviceMemPoolSize\u0026#34;] = Json::Int64(a.deviceMemPoolSize); device_json[\u0026#34;deviceMemStable\u0026#34;] = Json::Int64(a.deviceMemStable); device_json[\u0026#34;hostMemUsedMax\u0026#34;] = Json::Int64(a.hostMemUsedMax); device_json[\u0026#34;hostMemUsedMin\u0026#34;] = Json::Int64(a.hostMemUsedMin); device_json[\u0026#34;hostMemPoolSize\u0026#34;] = Json::Int64(a.hostMemPoolSize); device_json[\u0026#34;swapReason\u0026#34;] = a.swapReason; device_json[\u0026#34;deviceMemUsedNvidia\u0026#34;] = Json::Int64(-1); gpu_info_json[a.gpu_pci_bus_id] = device_json; } dump_json[\u0026#34;gpuUsageInfo\u0026#34;] = gpu_info_json; Json::Value sess_json; uint64 max_duration = 0; for (const auto\u0026amp; s : sess_run_durations_) { uint64 du = s.second.duration_; time_t rec = s.second.recording_time_; sess_json[\u0026#34;graph_\u0026#34; + std::to_string(s.first)] = Json::UInt64(du); if (du \u0026gt; max_duration \u0026amp;\u0026amp; time(0) - rec \u0026lt; max_record_interval) { max_duration = du; } } dump_json[\u0026#34;miniBatchDuration\u0026#34;] = Json::UInt64(max_duration); dump_json[\u0026#34;Durations\u0026#34;] = sess_json; Json::StreamWriterBuilder stream_writer; std::unique_ptr\u0026lt;Json::StreamWriter\u0026gt; writer(stream_writer.newStreamWriter()); std::ofstream statistics_file; statistics_file.open(gpu_statistics_file_); writer-\u0026gt;write(dump_json, \u0026amp;statistics_file); statistics_file.close(); // LOG(INFO) \u0026lt;\u0026lt; \u0026#34;gpu_statistics_file updated.\u0026#34;; } 注册中间件\n#if GOOGLE_CUDA // We register the GPUStatistics as a POST_SESSION_RUN action // during the initialization phase of the program. REGISTER_SESSION_RUN_ACTION(SessionRunActionRegistry::POST_SESSION_RUN, 1, GPUStatistics); #endif // GOOGLE_CUDA GpuOpManager  GpuOpManager continuously profiles the GPU operators execution time and simply distributes idle time slots before launching the GPU operators.\n 在GPUResourceManagement.cc 中实现了set()和get() 对应executor要执行OP数的接口，\n在Executor中以一个线程的形式运行， 在ExecutorState::ExecutorState中新增了一个thread成员变量\n修改Executor.cc\n构造函数里，初始化gpu_op_manger_thread // 获取GPUResourceManagement  GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm != nullptr) { enable_op_management = (rm-\u0026gt;GetEstimatedIdleTime() \u0026gt; 0); } if (enable_op_management \u0026amp;\u0026amp; gpu_op_manager_thread_ == nullptr) { gpu_op_manager_thread_ = new std::thread(\u0026amp;ExecutorState::AsyncGPUOpManager, this); } manager负责插入时间槽， 也就是计算好sleep的时间， 释放资源\n// The manager thread which is in charge of inserting the time slot // before launching each queued async GPU op. void ExecutorState::AsyncGPUOpManager() { uint64 sleep_time_us = 0; need_to_insert_idle_time_ = false; GPUResourceManagement* rm = GetGPUResourceManagement(); if (rm == nullptr) { return; } while (!terminate_op_magager_thread_) { need_to_insert_idle_time_ = rm-\u0026gt;GetEstimatedIdleTime() \u0026gt; 0 ? true : false; std::function\u0026lt;void(void)\u0026gt; queued_call_func = nullptr; // 1）从队列中获取第一个要执行的Op  async_gpu_op_queue_lock_.lock(); if (!async_gpu_op_queue.empty()) { queued_call_func = async_gpu_op_queue.front(); } async_gpu_op_queue_lock_.unlock(); if (queued_call_func != nullptr) { //2）调用这个Op  queued_call_func(); async_gpu_op_queue_lock_.lock(); //3）从队列中删除这个op  if (!async_gpu_op_queue.empty()) { async_gpu_op_queue.erase(async_gpu_op_queue.begin()); // 数目减一  num_queued_op.fetch_sub(1); } async_gpu_op_queue_lock_.unlock(); // Estimate idle time 预测空闲时间  uint64 idle_time = rm-\u0026gt;GetEstimatedIdleTime(); uint64 queued_op_num = rm-\u0026gt;GetExecutorQueuedOpNum(impl_); idle_time = queued_op_num \u0026gt; 0 ? (idle_time / queued_op_num) : 0; // 等待一个op的 idle_time  usleep(idle_time); // 设置剩余时间  uint64 remain_time = rm-\u0026gt;GetEstimatedIdleTime(); remain_time = remain_time \u0026gt; idle_time ? (remain_time - idle_time) : 0; rm-\u0026gt;SetEstimatedIdleTime(remain_time); } usleep(default_check_interval); } return; } Process 处理过程 // Process():  ... Device* kernel_device = impl_-\u0026gt;params_.device; // Only enqueue this op if it is an async GPU op. \t// 1 如果是一个异步Op， 则加入到异步OP队列  if (need_to_insert_idle_time_ \u0026amp;\u0026amp; (kernel_device-\u0026gt;name()).find(\u0026#34;GPU\u0026#34;) != string::npos) { // Enqueue this GPU op therefore we can insert a time slot before launching this op.  // 在启动这个op之前我们可以插入一个时间槽  sess_op_num++; const GraphView\u0026amp; gview_t = impl_-\u0026gt;gview_; const NodeItem\u0026amp; item_t = *gview_t.node(id); AsyncState* state = new AsyncState(params, tagged_node, \u0026amp;item_t, first_input, stats); auto async_gpu_kernel = [this, state, id, stats, op_kernel, device] { AsyncOpKernel* async = state-\u0026gt;item-\u0026gt;kernel-\u0026gt;AsAsync(); DCHECK(async != nullptr); auto done = [this, state]() { Device* device = impl_-\u0026gt;params_.device; NodeExecStatsInterface* stats = state-\u0026gt;stats; // Shorthand  Entry* first_input = state-\u0026gt;first_input; // Shorthand  nodestats::SetOpEnd(stats); EntryVector outputs; Status s = ProcessOutputs(*state-\u0026gt;item, \u0026amp;state-\u0026gt;ctx, \u0026amp;outputs, stats); nodestats::SetMemory(stats, \u0026amp;state-\u0026gt;ctx); if (vlog_) { VLOG(2) \u0026lt;\u0026lt; \u0026#34;Async kernel done: \u0026#34; \u0026lt;\u0026lt; state-\u0026gt;item-\u0026gt;node-\u0026gt;id() \u0026lt;\u0026lt; \u0026#34; step \u0026#34; \u0026lt;\u0026lt; step_id_ \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; SummarizeNode(*state-\u0026gt;item-\u0026gt;node) \u0026lt;\u0026lt; (state-\u0026gt;tagged_node.is_dead ? \u0026#34; is dead\u0026#34; : \u0026#34;\u0026#34;) \u0026lt;\u0026lt; \u0026#34; device: \u0026#34; \u0026lt;\u0026lt; device-\u0026gt;name(); } // Clears inputs.  const int num_inputs = state-\u0026gt;item-\u0026gt;num_inputs; for (int i = 0; i \u0026lt; num_inputs; ++i) { (first_input + i)-\u0026gt;ClearVal(); } FrameState* input_frame = state-\u0026gt;tagged_node.input_frame; const int64 input_iter = state-\u0026gt;tagged_node.input_iter; const int id = state-\u0026gt;tagged_node.node-\u0026gt;id(); MaybeMarkCompleted(input_frame, input_iter, id); TaggedNodeSeq ready; if (s.ok()) { PropagateOutputs(state-\u0026gt;tagged_node, state-\u0026gt;item, \u0026amp;outputs, \u0026amp;ready); } outputs.clear(); if (s.ok() \u0026amp;\u0026amp; impl_-\u0026gt;device_record_tensor_accesses_) { // Get the list of all tensors accessed during the execution  TensorReferenceVector accessed; state-\u0026gt;ctx.retrieve_accessed_tensors(\u0026amp;accessed); nodestats::SetReferencedTensors(stats, accessed); // callee takes ownership of the vector  device-\u0026gt;ConsumeListOfAccessedTensors(state-\u0026gt;ctx.op_device_context(), accessed); } const bool completed = NodeDone(s, state-\u0026gt;item-\u0026gt;node, ready, stats, nullptr); delete state; if (completed) ScheduleFinish(); }; nodestats::SetOpStart(stats); { profiler::TraceMe activity( [\u0026amp;] { return strings::StrCat( op_kernel-\u0026gt;name(), \u0026#34;:\u0026#34;, op_kernel-\u0026gt;type_string(), \u0026#34;#id=\u0026#34;, step_container_ ? step_container_-\u0026gt;step_id() : 0, \u0026#34;,device=\u0026#34;, device-\u0026gt;name(), \u0026#34;,async=true#\u0026#34;); }, profiler::GetTFTraceMeLevel(op_kernel-\u0026gt;IsExpensive())); device-\u0026gt;ComputeAsync(async, \u0026amp;state-\u0026gt;ctx, done); } }; // Enqueue this asyn GPU op.  async_gpu_op_queue_lock_.lock(); async_gpu_op_queue.emplace_back(async_gpu_kernel); //添加kernel到op队列  num_queued_op.fetch_add(1); //加一  async_gpu_op_queue_lock_.unlock(); } else { //2 如果不是异步的OP 则不加入  // Do not enqueue this op.  AsyncOpKernel* async = item.kernel-\u0026gt;AsAsync(); DCHECK(async != nullptr); AsyncState* state = new AsyncState(params, tagged_node, \u0026amp;item, first_input, stats); auto done = [this, state]() { Device* device = impl_-\u0026gt;params_.device; NodeExecStatsInterface* stats = state-\u0026gt;stats; // Shorthand  Entry* first_input = state-\u0026gt;first_input; // Shorthand  nodestats::SetOpEnd(stats); EntryVector outputs; Status s = ProcessOutputs(*state-\u0026gt;item, \u0026amp;state-\u0026gt;ctx, \u0026amp;outputs, stats); nodestats::SetMemory(stats, \u0026amp;state-\u0026gt;ctx); if (vlog_) { VLOG(2) \u0026lt;\u0026lt; \u0026#34;Async kernel done: \u0026#34; \u0026lt;\u0026lt; state-\u0026gt;item-\u0026gt;node-\u0026gt;id() \u0026lt;\u0026lt; \u0026#34; step \u0026#34; \u0026lt;\u0026lt; step_id_ \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; SummarizeNode(*state-\u0026gt;item-\u0026gt;node) \u0026lt;\u0026lt; (state-\u0026gt;tagged_node.is_dead ? \u0026#34; is dead\u0026#34; : \u0026#34;\u0026#34;) \u0026lt;\u0026lt; \u0026#34; device: \u0026#34; \u0026lt;\u0026lt; device-\u0026gt;name(); } // Clears inputs.  const int num_inputs = state-\u0026gt;item-\u0026gt;num_inputs; for (int i = 0; i \u0026lt; num_inputs; ++i) { (first_input + i)-\u0026gt;ClearVal(); } FrameState* input_frame = state-\u0026gt;tagged_node.input_frame; const int64 input_iter = state-\u0026gt;tagged_node.input_iter; const int id = state-\u0026gt;tagged_node.node-\u0026gt;id(); MaybeMarkCompleted(input_frame, input_iter, id); TaggedNodeSeq ready; if (s.ok()) { PropagateOutputs(state-\u0026gt;tagged_node, state-\u0026gt;item, \u0026amp;outputs, \u0026amp;ready); } outputs.clear(); if (s.ok() \u0026amp;\u0026amp; impl_-\u0026gt;device_record_tensor_accesses_) { // Get the list of all tensors accessed during the execution  TensorReferenceVector accessed; state-\u0026gt;ctx.retrieve_accessed_tensors(\u0026amp;accessed); nodestats::SetReferencedTensors(stats, accessed); // callee takes ownership of the vector  device-\u0026gt;ConsumeListOfAccessedTensors(state-\u0026gt;ctx.op_device_context(), accessed); } const bool completed = NodeDone(s, state-\u0026gt;item-\u0026gt;node, ready, stats, nullptr); delete state; if (completed) ScheduleFinish(); }; nodestats::SetOpStart(stats); { profiler::TraceMe activity( [\u0026amp;] { return strings::StrCat( op_kernel-\u0026gt;name(), \u0026#34;:\u0026#34;, op_kernel-\u0026gt;type_string(), \u0026#34;#id=\u0026#34;, step_container_ ? step_container_-\u0026gt;step_id() : 0, \u0026#34;,device=\u0026#34;, device-\u0026gt;name(), \u0026#34;,async=true#\u0026#34;); }, profiler::GetTFTraceMeLevel(op_kernel-\u0026gt;IsExpensive())); device-\u0026gt;ComputeAsync(async, \u0026amp;state-\u0026gt;ctx, done); } } 结束函数 // Finish():  if (gpu_op_manager_thread_ != nullptr) { terminate_op_magager_thread_ = true; if (gpu_op_manager_thread_-\u0026gt;joinable()) { gpu_op_manager_thread_-\u0026gt;join(); } delete gpu_op_manager_thread_; terminate_op_magager_thread_ = false; } 总结 graph TD\rAgpu_resource_manage_file]\rB[SessionRunRegistry]\rC[SessionRunAction] D[Executor]\rE[GPUResouceManagement]\rF[GPU Statistic]\rG[GpuOpManager]\rH[GpuUsageAdjustment]\rI(dump gpu statistic)\rJ[GPU Process State]\rK[GPUVMemAllocator]\rL[GPUAdjustableAllocator]\rA --|FileListener| E\rB --|Register| E\rE --|need_to_adjust_memory_| H\rH --|new| L\rH --|get| K\rC --|Derive| E\rC --|Derive| F\rB --|Register| F\rF --|need_to_dump_statistics_| I\rB --|Run| C\rJ --|maybe_create_gpu_vmem_allocator|K\rD --|run thread| G\rE --|GetEstimatedIdleTime| G\r ","date":"2022-12-04T23:08:20+08:00","permalink":"https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90antman%E5%AF%B9tensorflow%E7%9A%84%E4%BF%AE%E6%94%B9/","title":"【代码分析】Antman对Tensorflow的修改"},{"content":"EasyScale 论文阅读笔记 Abstract  分布式同步GPU训练通常被用于深度学习。  使用固定GPU的资源约束  使得大规模的深度学习训练工作受到影响 降低了集群的利用率   纳入资源弹性  往往会引入模型精度的非确定性\u0026lt;\u0026mdash;\u0026ndash;缺乏隔离能力     本文介绍EasyScale，  这是一个弹性框架  可以在异构GPU上扩展分布式训练 同时产生确定性的深度学习模型   实现了弹性的精度一致的模型训练。  EasyScale严格遵循数据并行训练流程 仔细追踪与精度相关的因素 有效利用深度学习特性进行上下文切换   为了使异构GPU的计算能力达到饱和  EasyScale根据我们的作业内和作业间调度策略动态地分配工人 最大限度地减少GPU的空闲时间 并相应地提高综合作业的吞吐量。   实验  部署在CompanyA的一个在线服务集群中 EasyScale为弹性深度学习训练作业提供动力，使其适时地利用空闲的GPU 在不违反SLA的情况下将集群的整体利用率提高了62.1%      Introduction   弹性深度学习框架很少在行业中使用\n 根本障碍：在使用不同资源进行训练时模型准确性不一致 资源弹性对训练程序和模型收敛都引入了非确定性。  通过调整特定基准中的超参数（如学习率或批量大小）收敛到类似的精度不能说服用户，因为预期的计算流程已经隐性改变。 在改变数据集或模型结构时，对模型准确性的非确定性的担忧仍然没有得到解决，这使得深度学习从业者在拥抱资源弹性时犹豫不决      EasyScale：\n 第一个在同构和异构 GPU 的资源弹性上实现一致模型精度的训练框架  提高了整体集群效率\u0026lt;\u0026mdash;-通过尽最大努力利用空闲 GPU 来进行弹性模型训练   将深度学习模型训练视为科学实验，将确定性和可重复性作为第一流的目标 EasyScale探讨了将分布式模型训练过程与硬件资源解耦的可能性  无论分配的GPU数量和类型如何，都能产生位数一致的模型 这是通过一个名为EasyScaleThread的抽象来实现的  它封装了从数据加载、采样、计算到通信的所有阶段 并使它们与在固定的GPU中执行的完全一样   EasyScale利用深度学习的特点  实现了快速的上下文切换 解决了训练工作者状态的潜在非确定性 并在资源重新配置时有效地执行追踪和检查点   EasyScale引入了一个工作内策略\u0026mdash;-\u0026gt;以负载平衡的方式在异构GPU上安排训练工人 进一步优化了工作间的资源分配\u0026mdash;-\u0026gt;以最大限度地提高总的吞吐量。      贡献如下\n 我们调查了现有的弹性深度学习框架和异构环境中的非确定性行为，并对散布在整个DLT软件堆栈中的位数差异进行了溯源。 我们介绍用于弹性分布式模型训练的EasyScale，它以确定性的方式实现了一致的准确性。EasyScale利用EasyScaleThread来保持与PyTorch DDP相同的弹性训练行为，并以可忽略不计的开销有效地进行上下文切换。 我们引入了新的作业内和作业间调度策略，以提高单个EasyScale作业的吞吐量和聚合集群的吞吐量，从而灵活有效地利用异构的GPU。 我们在生产集群中全面部署了EasyScale，将弹性训练作业与在线模型服务放在一起，在满足模型服务SLA的约束下，显著提高了集群利用率。    Motivation 弹性训练带来的不确定性   不一致的模型精度\n  \rimage-20221110093121968\r\n  用弹性框架进行模型训练的多次运行，未能在使用不同数量的资源时产生一致的模型精度。\n  图2说明了ResNet18在CIFAR10上的验证精度  实验条件  这是一个用不同数量的V100 GPU训练的弹性模型 超参数和随机种子与默认值相同，只使用不同分配的GPU TorchElastic（TE）被配置为调整学习率的线性缩放规则 Pollux可以自动决定相应的学习率和批次大小   实验结论  与固定GPU上的分布式模型训练相比，资源弹性带来了不同的训练行为。 Pollux在产生模型质量方面引入了相对较小的差异，然而，这种差异仍然是不可忽视的     图3中报告了总体和每类的继续训练到100个epoch准确率  结果  总体准确率的差异仍然很明显：TorchElastic和Pollux分别为0.6%和2.8%。 每类准确率的差异甚至更大：达到7.4%和17.3%   结论  这表明弹性训练的模型与使用固定GPU的模型相比，偏差不同。 其他指标仍未披露，模型质量差距的上限仍然未知           很难理解超参带来的影响\n  \rimage-20221110093137841\r\n  研究人员很难推理出gamma如何影响训练损失曲线\n  图4显示了ResNet50在CIFAR10上的实验  实验设置  比较对象  固定4个GPU上的DDP训练 1/2/4个GPU上的使用Pollux的弹性模型训练   除了学习率调度器的一个超参数gamma外，其他配置都是一致的  gamma决定了本实验中20个epochs后的学习率降低比例 DDP实验以相同的设置运行了三次4-GPU训练，但gamma值分别为0.1、0.3和0.5 Pollux也运行同样的实验，1个GPU的gamma为0.1，2个GPU的gamma为0.3，而4个GPU的gamma为0.5     实验结论  使用DDP，可以清楚地推断出超参数gamma是如何影响模型训练过程的。  训练损失在前20个 epochs的三次运行中保持一致 之后，较小的gamma导致较小的训练损失   使用Pollux，研究人员很难推理出gamma如何影响训练损失曲线。           总结\n DL 模型现在是算法，框架和计算资源组合的结果，这是限制 DLT 作业使用弹性资源的根本原因 现有的弹性 DL 框架缺乏将资源与模型超参数解耦的能力\u0026mdash;-\u0026gt;无法提供与弹性训练一致的模型精度 我们需要支持弹性训练，同时保持一致的模型精度    Design Overview   弹性训练应该生成与使用固定数量的GPU进行的DDP训练相同的模型参数\n  实现弹性准确度一致的模型训练的关键挑战是找到一种实用的方法，将一个GPU有效地分享给多个worker\n  EasyScaleThread   EasyScaleThread： 捕捉深度学习的训练过程并将其与硬件资源解耦\n 每个GPU都由一个EasyScale PyTorch worker启动。 原始训练worker的执行被视为EST的执行，它可以动态地分配给PyTorch worker进程。 在一个worker中，多个EasyScaleThreads轮流占用GPU进行计算。 使用  通过使用白盒方法，EasyScale通过用户注释将模型训练的关键步骤挂钩，数据加载、反向传播和模型更新，因此在mini-batch边界进行精度一致的上下文切换。 用户定义的模型训练语义，包括模型结构、数据增量、批次大小、优化等，都照常保留。 至于编程，用户考虑总的逻辑训练工作者的数量来决定超参数（如全局批处理量和学习率），这与他们使用固定GPU的经验相同，但自动受益于EasyScale提供的弹性。      执行\n mini-batch的执行：  输入数据被分割到所有的EasyScaleThreads中 一个GPU上每次执行一个EST时， 其他EST被冻结 在所有EasyScaleThreads完成后，mini-batch就完成了   当两个EasyScaleThreads切换时  EasyScaleThread的训练状态需要被保存到GPU之外，因此要确保有足够的GPU内存给下一个EasyScaleThread，这可能是昂贵的。 在GPU上为每个minibatch有效切换EasyScaleThreads的关键是减少上下文切换所需的状态。 而EasyScale选择在完成前向后向计算后切换EasyScaleThreads，最大限度地减少GPUCPU内存拷贝。   我们通过以下方式最小化状态的大小：  i）定位影响最终精度的非确定性来源，最小化需要记录的必要状态 ii）利用DL的数据并行行为，最小化数据交换的工作集。   EasyScaleThread的GPU内存中的工作集  可以分为时间张量和激活、模型参数和优化状态以及梯度， 处理  首先，对于时间张量和激活，它们在前向步骤中创建，在完成梯度生成后在后向步骤中销毁。  因此，它们会在小批处理结束时自动释放出来。   其次，关于模型参数和优化器状态，每个数据并行工作者在训练过程中都会保留一个副本，并且在一个小批处理结束时进行更新。  因此，在EasyScale中，当切换EasyScaleThreads时，它们可以被重复使用。   最后，梯度是根据EasyScaleThreads的不同数据输入计算的，因此不能重复使用。然而，梯度通常很小，并且只在minibatch结束时的分布式梯度同步中使用。  因此，在EasyScale中，我们在上下文切换时将梯度迁移到主机DRAM中，并与下一个EasyScaleThread的计算重叠。       通过这种方式，我们交替执行EasyScaleThreads，直到所有计算完成。之后，分布式同步被触发，模型更新被进行一次以完成小批量的计算。    重配置\n 当资源重新配置被触发时，EasyScale采用按需检查点的方式来持续保持最小和必要的状态。 检查点包含了  所有EasyScaleThreads的上下文 额外的状态（包括训练进度和其他实现精度一致性的状态） 深度学习参数（例如，模型、优化器和学习率调度器）   与EasyScaleThread上下文不同的是  额外的状态和参数只需要一个副本，因为它们在mini-batch结束时对所有EasyScaleThread都是一样的。   请注意，在重新启动模型训练后，每个GPU的EasyScale运行时  会加载额外的状态和模型参数的副本 以及重新分布的EasyScaleThreads的相应上下文      优化\n  为了overlap数据加载和GPU训练\n data loader在独立的处理器中执行（即PyTorch中的加载器工作者进程），异步加载训练样本并执行数据增强（例如，裁剪或旋转图像）以建立训练批次。 在EasyScale中，我们优化了所有EasyScaleThreads之间共享data loader，因为每次只有一个EasyScaleThread在GPU上进行训练。 尽管共享了多个EasyScaleThreads，但数据消耗率与专用GPU中的数据消耗率相似。    为了实现data loader的共享\n  EasyScale采用了一个分布式数据采样器，该采样器共同考虑了EasyScaleThreads的全局index和时分模式，在一个队列中生成数据index\n  然后，这些数据索引被数据工作者有序地处理\n \rimage-20221110211808582\r\n 图7显示了将三个数据工作者共享给两个EasyScaleThreads的情况，其中EasyScaleThread的总数量为四个（即图6中的2-GPU训练）。 EST0和EST1的训练批次为：小批0的b0和b1，小批1的b4和b5。 在专用GPU中为EasyScaleThread i处理数据指数的数据工作者j的状态被表示为Ri-j 为了平衡负载，EasyScale中的数据工作者轮流从队列缓冲区中获取给定数据指数的相应状态（即Ri-j）进行预处理，完成后将状态提交回队列缓冲区中。     注意，由于data loader的异步执行\u0026ndash;\u0026gt;\n data loader的进程通常在训练进度之前\u0026mdash;\u0026gt; 为了跟踪和保持弹性的一致状态，引入了一个排队缓冲区来记录未被消耗的小批的必要状态\u0026mdash;\u0026gt; worker的状态根据训练进度从队列缓冲区中去排队，然后在检查点中被视为额外状态的一部分        不确定性的溯源与解决  自上而下的方法来比较EasyScale和DDP， 我们发现非确定性的根本原因分散在训练管道的几乎整个软件栈中，从训练框架到通信，再到GPU内核。  首先，在训练框架层面  框架有一些状态需要在整个训练过程中保持一致，以保证确定性 尽管深度学习训练在DAG图中组织运算符（例如卷积、批量归一化），但一些运算符隐含地依赖一些状态，而不是其前辈的输出。例如  Dropout依赖于GPU中的随机数发生器（RNG）状态； BatchNorm通过考虑工作者的等级来跟踪其运行状态； 数据加载器和数据增强的转化器依赖于Python、NumPy和PyTorch的随机状态，等等。     第二，在通信层面  通过全还原的梯度同步在资源弹性下是不确定的 在同步过程中，梯度被聚集到通信桶中，以优化通信性能。 梯度到桶的映射最初由DAG图的静态反转拓扑顺序决定。 它在第一个小批处理结束时根据收到的梯度张量的顺序进行重构。 然而，在弹性训练期间，工作者重新启动将重建通信渠道，这可能会影响第一个恢复的小批的梯度聚合顺序。 由于环形Allreduce的实现，这最终会引入非确定性。   最后，在GPU内核层面  为同一运算器选择不同的内核也会导致结果的细微差别 导致不同内核选择的原因有两个。  首先是框架、编译器或供应商库中的一些基于剖析的优化，在小批量中应用不同的内核实现，以收集性能统计数据，找到最佳匹配。 另一个是内核的实现可以与硬件相关。例如，一些内核实现是基于流处理器单元的数量、硬件特定的低位组件等，因此不能应用于所有类型的GPU。       确定性的等级与应对方案  EasyScale定义了不同级别的弹性训练的确定性，为用户提供明确的一致性保证，并设计了相应的处理方法来实现它们。 D0：固定DoP的确定性  \u0026ndash;用固定的GPU资源进行多次训练应该产生相同的模型 实现D0需要训练框架及其所选内核的一致行为。  对于框架，我们在训练开始时固定RNG的随机种子，并在数据加载工作者状态中记录RNG的状态，在上下文中记录EasyScaleThreads的状态，以便EasyScaleThreads自动保持状态一致。 我们还禁用了最适合的算法选择，并选择了确定性的算法（例如，没有原子指令）。     D1：弹性确定性  \u0026ndash;在检查点重启的情况下，用数量不断变化的同质GPU进行多次训练，应该产生相同的模型 在D0之外，D1需要解决通信层面的非确定性。  为此，我们为每个EasyScaleThread分配了一个固定的虚拟通信等级。 我们还将形成梯度桶的索引记录到检查点中。 重新启动后，在训练之前，首先用记录的指数重建桶。 后面的通信通道重建被禁止。     D2：异质性确定性  \u0026ndash;用不同类型的GPU进行的多次训练应该产生相同的模型。 为了实现D2，我们开发了与硬件无关的GPU内核。具体来说，  1）我们修改内核实现（例如PyTorch中的reduce、dropout），限制SM和线程的数量； 2）我们通过向高层调用传递algo_id，强制选择相同的低层实现（例如cuDNN中的convolution，以及cuBLAS中的gemm、gemv）。       如何确定确定性等级  在EasyScale中，D0和D1是默认启用的，因为我们的实现对实现它们的开销可以忽略不计 实现D2对于某些类型的模型（如CV模型）可能会有较高的开销，因为它们不能对某些GPU类型使用一些供应商优化的内核（如卷积）。  EasyScale可以透明地分析一个模型（通过扫描PyTorch nn.Modules），并识别它是否依赖于需要硬件特定内核优化的运算符。 如果不是，我们就启用D2，允许它使用异构的GPU，否则就限制它使用同构的GPU。      调度原则  用户在提交EasyScale作业时  可以指定一个maxP：即要启动的最大工作者数量，这也是作业执行过程中EasyScaleThreads的数量 用户还可以指定一个minP（\u0026gt;=0），表示所需的保证GPU 设置minP == maxP意味着作业将回到使用与DDP相同的固定DoP   小结：ESTi抽象 -\u0026gt; 确定性处理 -\u0026gt; 调度EST  EasyScaleThreads（EST）的抽象将DL训练和底层GPU资源解耦，因此与DDP兼容的训练作业可以持续地在同质GPU的弹性数量上运行。 确定性处理实现了弹性训练下的精度一致性，即使是在异构GPU上   在异构GPU上调度EST的关键挑战在于计算能力的异质性和GPU内存的异质性。  Pollux和VirtualFlow  采用为每一种GPU单独扩展批次大小的方法 对于EasyScale来说是完全不可接受的，因为它改变了固有的训练超参数，从而破坏了精度的一致性   EST消耗固定计算能力与固定内存消耗  在EST执行过程中，每个EST消耗固定的计算能力，其中GPU的微架构特征（即SM数量和缓存大小）决定了理论能力。具有较高计算能力的GPU可以在一定时间内执行更多的EST 而每个EST都有固定大小的GPU内存使用峰值。此外，由于同一GPU执行器中的EST的内存是完全重复使用/共享的，执行器的内存用量可以代表EST的内存用量 为简洁起见，我们将固定的计算能力和内存用量分别定义为计算单元（CU）和内存单元（MU）   复杂的分析规划：避免因为严重的负载不平衡而造成重大的性能浪费    异构感知的EST规划   Planning CUs\n 分配策略  当分配的GPU是同质的，并且数量是maxP的一个因素时，在这些GPU上均匀地分配CU，因为所有CU都有相同的计算时间 在异构GPU的情况下，需要根据GPU的计算能力来分配CU以达到平衡，并最多消除空闲周期   我们提出了一个新的指标，叫做浪费，  即由于CU的整数倍和GPU的实际连续能力不匹配而浪费的计算能力，或者说是负载不平衡。 浪费主要包括两个方面：  ∂异构GPU之间的负载不平衡，是由于用CU的整数倍不准确地逼近连续的实际计算能力造成的； ∑同构GPU之间的负载不平衡，是由于没有足够的CU分配来充分利用所有的GPU，因为CU的总量被maxP所限制。   因此，我们建立一个分析模型来量化浪费。  符号表示  可用的GPU数量表示为Ni，其中下标i代表GPU类型。 与工作负载相关的计算能力Ci被估计为每秒的小批处理数量。 分配给GPU类型i的CU的最大数目被表示为Ai。   \rimage-20221112203217230\r 为了确保所有EasyScaleThreads被执行，异构GPU上的最大CU总数（CU_capacity）应该大于或等于maxP（公式1a）。 过载系数foverload代表所请求的异构GPU的最大过载，其中过载被定义为每个计算能力的CU（等式1b）。 如果一个GPU类型承担了太多的CU，它就会成为性能瓶颈，并由于Sync-SGD而拖慢其他GPU的速度。因此，浪费被表述为：∂Ci和Ai之间的差距按foverload缩放，∑超额配置的CU_capacity按foverload缩放（公式1c）。 为了进一步区分当前Ni、Ci和Ai下的CU分配效率，得出了归一化的浪费百分比（公式1d）。 还得出了估计性能（等式1e）。        Planning MUs\n  异构GPU之间的内存容量也存在异质性\n 将MU分配给每个GPU的单个执行器可以最大限度地减少整体内存占用，因为EasyScaleThreads引入的内存开销可以忽略不计。 而且它们的MU可以完全重复使用。 因此，所有的GPU都显示出与MU相同的峰值内存使用量，导致具有较大内存容量的GPU出现闲置内存。    我们提出了一个多执行器的设计\n  它允许在GPU上分配一个以上的执行器，这样就可以同时执行多个EST。\n  具有较大内存的GPU可以权衡执行器数量和每个执行器的EST数量\n 同时保持 （#执行器 × #EST） 不变 例如，在分配了两个EST的情况下，有两种选择：  a）\u0026lt;1执行器×2EST\u0026gt; b）\u0026lt;2执行器×1EST\u0026gt;      它拓宽了高效场景，即运行更多的执行器不会超过GPU资源（SM核、内存），即使考虑到干扰，仍然有助于提高性能。\n 推荐模型（如Wide\u0026amp;Deep）的训练通常显示出对GPU计算能力的利用不足，通常低于50%。在这种情况下，分配多个执行器可以利用剩余的计算能力，而且执行器之间不会产生不利影响，从而提高综合吞吐量。\n     我们还对（等式1）进行了调整，以对多个执行器的浪费进行建模。\n 与工作负载相关的计算能力Ci被MCi = m × Ci × Ii所取代，表示m个执行器的整体能力，其中包括干扰Ii。 分配给GPU Ai的CU数量由MAi = m × Ai取代，代表m个执行器的总CU数量。      EeayScale 调度器  如图9所示，EasyScale采用了一个分层调度架构。  每个作业都包含一个作业内的调度器，名为AIMaster，它负责：  a）在同质和异质GPU之间分配EasyScaleThreads，尽量减少浪费，使资源利用率最大化； b）通过估计潜在的速度提升，提出所需的资源进行扩展。   此外，一个集群调度器以全局模式行事，协调作业之间的资源。    \rimage-20221110214614664\r\n Intra-Job scheduler  基本职责：在给定的GPU下生成EST分配配置  首先，在当前可用的GPU下，它选择估计吞吐量最高的配置，并相应分配EST。 其次，它试图用一个增量的GPU来扩展，从而产生新的配置，并选择top-K的配置作为提交给集群调度器的建议。   配置组成与约束  由\u0026lt;nums, executors, threads, waste, perf\u0026gt;组成  nums, executors, threads是具有相同长度的GPU类型的数组，分别代表GPU数量、执行器数量和每个执行器的EST数量 waste和perf代表该配置通过分析模型估计的浪费和性能   这些配置应该满足集群总资源、minP、maxP和归一化浪费的阈值（实际为30%）的约束。   寻找可用配置  不同的工作负载使用不同GPU的吞吐量有差异，但在没有实际执行的情况下很难预测 因此，AIMaster模块使用作业的运行时执行统计数据来了解工作负载差异，并确定每个GPU类型i的工作负载相关计算能力Ci。 鉴于每个GPU类型i的剖析计算能力Ci，我们计算它们的整数近似值（例如，ceil(t×Ci), floor(t×Ci)），假设每个能力承担k个EST，并形成它们的组合 然后我们遍历这些组合，找到可用的配置。对于具有相同\u0026lt;数、执行器、线程\u0026gt;的配置，选择具有最小浪费的配置，其他配置则被过滤掉   配置回退  对浪费的估计有时可能是不正确的，这可能会导致更差的训练性能 一旦在重新配置后观察到性能下降，我们就会退回到使用以前的资源并释放新分配的资源     Inter-job cluster scheduler   它通过考虑资源可用性和提案的优先级来响应AIMaster提案。\n 为了提高集群的整体利用率和聚合作业的吞吐量 它采用了一种启发式算法，倾向于接受每个GPU具有较高速度的建议，如算法1所示。  作业间调度器按照报告的平均加速比对建议进行降序排序。 然后，它对这些建议进行循环，开始接受最高的建议。 如果多个建议引入了相同的平均GPU加速，我们的调度器会优先考虑拥有更多GPU的建议。    \rimage-20221112210504940\r\n  集群调度器允许弹性作业最好地利用空闲资源\n 这些资源通常属于其他人，但暂时是闲置的。 然而，如果这些GPU需要返回，可能会触发抢占。  在这种情况下，集群调度器将尝试把与抢占的GPU相同的GPU分配给弹性作业。 当分配超时时，EasyScale作业会回落到利用它目前拥有的可用GPU。        Implementation   DLT 作业在具有 EasyScale 实现的 Docker 容器中运行。\n 在 Kubernetes 上实现了一个原型自定义集群调度器以进行评估。 EasyScale 在我们内部的 GPU 集群调度器中得到了充分的实现，它是 Kubernetes 调度器的一个优化版本，可以为日常的 GPU 生产任务提供服务。    DL 框架中 EasyScale 的实现与 PyTorch 1.8 LTS 兼容。\n 它需要大约1,200行 Python 代码和2,000行 PyTorch 中的 C + + 修改代码，以及一个基于 PyTorch 实现的插件库。 PyTorch 框架的 C + + 实现包括一个支持弹性的分布式数据并行通信库 ElasticDDP，它可以支持多个 EasyScaleThread 之间的通信，以全面减少梯度，并在触发资源弹性时在重新启动任务时始终如一地构建通信桶。 执行流控制和上下文切换作为 PyTorch 的附加组件在 Python 模块中实现。    实现了 AIMaster\n 为了决定负载平衡分配并控制作业以使用更多的 GPU 进行扩展 三个组成部分。  首先，我们通过一个 rpc 库收集 EasyScale 运行时报告的性能分析。 其次，我们提出资源建议并监视状态，从而通过 Kubernetes Python 告密者了解资源分配超时。 第三，实现策略控制器来计算增量资源请求并将其提交给集群调度器。为了支持资源弹性时的持续工作培训，我们采用按需检查点记录用户定义的模型，时代和小批量状态以及基本上下文切换状态      一个半自动剖析工具来执行张量之间的按位比较\n 从而找到运算符不一致的结果，识别资源弹性中不确定性的来源。    ","date":"2022-11-09T15:09:23+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202211091510408.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0easyscale%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】EasyScale论文阅读笔记"},{"content":"Gandiva 论文阅读笔记 Abstract   Gandiva: 一个集群调度框架，使用特定领域知识，优化了GPU集群训练深度学习模型的延迟与效率\n  深度学习job的特征\n 1）反馈驱动的探索：  一个用户经常运行一组作业(或 a multi-job)来获得特定任务的最佳结果 并使用关于准确性的早期反馈来动态优先考虑或杀死一个作业子集 同步发生的多个作业的早期反馈是至关重要的   2）深度学习工作在资源使用方面的异构，这使得它很难实现最适合的先验。 3）作业内可预测性：因为作业会重复执行叫做mini-batch的迭代  Gandiva利用这个特征解决了1）2）两个问题 利用可预测性对GPU进行多个job间进行时分复用， 这提供了低延迟 这种预测性还可以用于内省job性能并动态迁移到最合适的GPU上，提高了集群效率      我们通过一个原型实现和微基准测试表明\n Gandiva 可以在深度学习过程加快超参数搜索一个数量级 并通过透明迁移和job时分实现更好的利用，使job与资源的更好地匹配。 在一个运行在180-GPU 集群中的实际工作负载中，Gandiva 将集群的总利用率提高了26% 这为深度学习提供了一种管理大型 GPU 集群的新方法。    Introduction   DLT job的特征\n  反馈驱动\n  超参数搜索：\n 用户通常会尝试一个作业的几个配置（a multi-job），并利用这些作业的早期反馈来决定是否优先处理或杀死其中的一些子集。     传统的调度器运行一个工作子集， 完成前其他工作排队\n 这种模式不适合multi-jobs，因为a multi-job中的所有工作需要同时得到早期反馈。 另外，伴随着multi-jobs，其他已经确定了正确的超参数的DLT作业，运行了几个小时到几天，导致了行头阻塞，因为长期运行的作业对GPU拥有独家访问权，直到完成，而取决于早期反馈的多作业则在队列中等待。 长的排队时间迫使用户要么使用预留的GPU，要么要求集群超额配置，从而降低集群效率。     异构\n Jobs之间的固有差异  显存使用 核使用 带宽敏感性 job之间的干扰     传统调度器将job视作黑箱，无法取得最优的集群效率\n   作业内可预测\n    Gandiva\n Gandiva利用可预测性来执行剖析驱动的自省。  它使用小批量的进度率来不断反省其决策，以提高集群效率。 例如，只有在内存和GPU利用率较低时，它才会将多个作业装箱在同一个GPU上 它动态地将通信密集型作业迁移到更多的亲和力强的GPU上 它还会适时地 \u0026ldquo;增长 \u0026ldquo;作业的并行程度，以利用空闲资源，并在空闲资源消失后缩小作业。 我们目前实施的自省策略是一个有状态的试错策略，它是可行的，因为它的预测能力很强。   Gandiva除了特定的内省核调度策略，还提供了一些API API：（a）高效的挂起恢复或时间切片，（b）低延迟迁移，（c）细粒度监控，（d）弹性，以及（e）动态优先级。  这些原语高效的关键是Gandiva的协同设计：跨越调度器层与DLT工具包层（如pytorch， tensorflow)   通过利用GPU集群的专用性，Gandiva为深度学习的特定工作负载定制了调度器，从而为调度器提供了对工作的更多可见性和控制，同时仍然实现了对任意DLT工作的通用性。 Gandiva的实现  修改两个流行的框架 PyTorch 和 Tensorflow ，为调度程序提供必要的新原语 并在 Kubernetes 和 Docker 容器之上实现了一个初始调度策略管理器      本文贡献\n 我们说明了深度学习工作流程的各种独特特征，并将其映射到集群调度所需的具体要求。 我们确定了DLT作业调度策略可以使用的通用原语，并提供了应用感知技术，通过利用DL特有的作业内周期性知识，使时间切割和迁移等原语的效率提高了一个数量级，从而变得实用。 我们提出并评估了一个新的自省式调度框架，该框架利用DLT工作的特定领域知识来不断完善其调度决策，从而显著改善早期反馈时间并提供高集群效率。    Backgroud   反馈驱动的探索。\n 实现高精确度的一个前提条件是模型的选择。新模型的发现，如ResNet或Inception，如今大多是一个试错的过程，尽管使之自动化的方法是一个活跃的研究领域。 除了模型结构外，还有一些参数，称为超参数，也需要作为DLT工作的一部分被指定。超参数包括模型中的层数/权重、最小批量大小、学习率等。这些参数通常由用户根据领域知识和试错来选择，有时甚至会导致早期训练失败。 因此，DLT工作的早期反馈是至关重要的，特别是在训练的初始阶段。    multi-job\n  一旦用户确定了要进一步探索的特定模型，用户通常会进行超参数搜索以提高任务的准确性。\n  这可以在超参数空间上使用各种搜索技术来完成；也就是说，用户生成多个DLT任务或多任务，每个任务使用一组超参数或配置进行全面训练。由于用户通常会探索数百个这样的配置，这个过程在计算上是很昂贵的。\n  因此，文献中出现了复杂版本的超参数搜索，如HyperOpt和Hyperband。\n 例如，Hyperband最初可能会产生128个DLT作业，并在每一轮（例如100个小批量迭代）中，杀死一半精度最低的作业。\n   同样，对于这些算法来说，对整个作业集的早期反馈是至关重要的，因为否则他们将无法做出有效的训练决定。\n    DLT job的特征 对位置（locality)敏感  多GPU DLT工作的性能取决于分配的GPU的亲和力。  不同的DLT工作对GPU间的亲和力表现出不同程度的敏感性。 即使是同一台机器上的GPU，由于不对称的架构，我们观察到不同程度的GPU之间的亲和力  两个GPU可能位于不同的CPU插槽（表示为DiffSocket） 在同一个CPU插槽，但在不同的PCIe Switch（表示为SameSocket） 在同一个PCIe Switch（表示为SamePCIeSw）      \rimage-20221108193230029\r\n 图1显示了两个模型VGG16[44]和ResNet-50[24]对服务器内定位的不同敏感性。  当使用Tensorflow的两个P100 GPU进行训练时，VGG16在不良定位下受到很大影响。 在最差的定位下，当两个GPU位于不同的CPU插座上时，VGG16只实现了最佳定位配置的60%，即两个GPU被放置在同一个PCIe开关下。 另一方面，在这种设置下，ResNet-50不受GPU定位的影响。这是因为VGG16是一个比ResNet-50更大的神经模型，因此在每个小批次中的模型同步会在底层PCIe总线上产生更高的通信负荷。   我们在分布式环境中观察到类似的趋势。图2显示了一个4GPU Tensorflow作业的性能，它以不同的服务器间定位运行，训练ResNet-50和InceptionV3[46]模型。  即使是用40G InfiniBand网络互连，当作业被分配到4个GPU时，性能差异明显，  其中它们均匀地分散在4台服务器（表示为4*1-GPU） 2台服务器（表示为2*2-GPU） 以及全部在一台服务器（表示为本地4GPU） 尽管两个模型对位置性的敏感性不同。     因此，DLT调度器在分配GPU时必须考虑到作业对位置的敏感性。  对干扰敏感  当在一个共享的执行环境中运行时，DLT工作可能会因为资源争夺而相互干扰。我们再次观察到，不同的DLT工作表现出不同程度的干扰。  \rimage-20221108193107517\r\n  即使对于单GPU作业，也存在干扰。\n 图3显示了: 当把一个语言模型作业（标记为LM）与另一个作业放在同一个PCI-e交换机下时，由于服务器内的干扰而导致的性能下降情况。  当两个LM一起运行时，两个作业都会遭受19%的减速。 然而，ResNet-50并没有受到GPU与LM共处的影响。 神经机器翻译（GNMT）[51]对LM的干扰程度不大。 同样地，我们也观察到不同类型的训练模型对多GPU训练的不同程度的干扰。      图4显示了用40G InfiniBand网络连接的两个4GPU服务器上的服务器间干扰。\n 当运行多个2-GPU作业时，每个GPU被放在不同的服务器上，  ResNet-50显示出高达47%的减速， InceptionV3显示出30%的减速， 而DeepSpeech[23]仅显示出5%的减速。      总之，不同应用领域的流行深度学习模型，如视觉、语言和语音，表现出对位置性和干扰的不同程度的敏感性。为了迎合这些挑战，Gandiva利用了DLT工作的一个关键特征，我们接下来会详细说明。\n  job内可预测性 \rimage-20221108193308480\r\n 一个DLT作业包括许多小批量的迭代， 呈现对应的周期性。  图5(a)显示了在四个K80 GPU上使用ResNet-50模型对ImageNet数据进行训练的20s快照期间使用的GPU总内存。  所使用的GPU内存明显遵循一个周期性模式。 每个周期都对应着一个小批次的处理（大约1.5s），内存在前向传递中增加，在后向传递中减少。 使用的最大和最小的GPU内存分别为23GB和0.3GB，或77倍的系数。 这个比例随着迷你批处理量的增加而扩大（通常在16到256之间；本例中为128）。   图5(b)显示了在一个K80 GPU上使用GNMT模型时，对WMT'14英语德语数据集进行训练的20s快照所使用的GPU总内存。  虽然小批量迭代与ImageNet的例子不完全相同（由于不同的句子长度和PyTorch中使用的动态图），但该图具有类似的循环性质。 最大值和最小值之间的差异较小（3倍），主要是由于较大的模型（0.4GB）和较小的迷你批次大小（本例中为16）。   除了这里显示的图像和语言模型外，其他训练领域，如语音、生成式逆向网络（GAN）和变异自动编码器都遵循类似的循环模式（由于空间限制没有显示），因为训练的核心是梯度下降算法，执行许多小批量迭代。   充分利用可预测性。  首先，一个DLT作业可以被自动分割成小批量的迭代，这些迭代在60秒内的集合，例如一个微任务，形成一个调度间隔。 第二，通过在内存周期的最小值上执行暂停操作，可以大大减少从GPU复制到CPU中的内存量，从而使暂停/恢复和迁移的效率比naive的实现要高一个数量级。 第三，可以对小批量的进度进行分析，并将其作为评估装箱或迁移等机制的有效性的代理。    设计 \rimage-20221108193438681\r\n 如今的集群中出现高延迟和低利用率的问题  是因为DLT作业被专门分配了一组固定的GPU。 独占访问 GPU 的原因行头阻塞，阻塞了早期反馈，导致作业的排队时间过长。 当作业无法完全利用其分配的GPU时，对一组固定的GPU的独家访问也会导致GPU的低利用率。    机制  三种方式消除GPU对DLT作业的排他性和固定分配来解决低效率问题  首先，在过载期间，Gandiva允许后来的工作与现有的工作共享GPU  而不是等待当前工作的离开。 这是为DLT作业定制的挂起-重启机制和选择性装箱而实现的。   第二，Gandiva支持DLT作业从一组GPU到另一组的高效迁移  迁移允许时间碎片化的作业迁移到其他（最近空出的）GPU上 或者对集群进行去碎片化处理，从而使后来的作业被分配到具有良好位置性的GPU上。   第三，Gandiva支持GPU增长-缩减机制  这样空闲的GPU就可以适时地被使用。 为了有效地支持这些机制并实现有效的资源管理，Gandiva通过不断地剖析DLT作业的资源使用情况并估计其性能，对DLT作业进行内省。      挂起-重启与装箱   Suspend-resume\n 挂起-重启是Gandiva用来消除一组GPU对DLT作业的独占性的一种机制。  Gandiva利用这种机制，增加了对GPU时间分割的自定义支持。   Gandiva的关键思想是利用这种周期性行为，在DLT作业的GPU显存使用量最低时暂停-恢复。  1）发出暂停调用 2）DLT工具包会等到内存使用周期的最小值，将存储在GPU中的对象复制到CPU，释放其所有GPU内存分配（包括缓存） 3）然后调用经典的CPU暂停机制 4）之后，当CPU恢复工作时，DLT框架首先分配适当的GPU内存，将存储的对象复制回GPU，然后恢复工作。   Suspend-resume也可以在同一台服务器中启动GPU的更换  虽然更换GPU的成本很高，但我们可以把这个延迟从关键路径中隐藏  典型的图像分类工作，暂停-恢复一起可以在100ms内完成 而对于大型语言翻译工作，暂停-恢复可能需要1s   考虑到1分钟的时间切分间隔，这相当于2%或更少的开销   延迟  Gandiva中的suspend可能最多延迟DLT作业的一个小批次间隔 值得  由于减少了GPU-CPU的复制成本和更少的CPU使用的内存，它的开销明显减少 在这个延迟期间还完成了有用的工作。   调度器跟踪这一延迟，并相应地调整时间分割的间隔，以实现公平性      装箱\n 暂停-恢复的另一种方法是装箱  在一个GPU上同时运行多个DLT作业，让GPU分担作业的时间。   有效性  只有当装箱的作业不超过GPU的资源，并且不会对彼此产生不利影响时，GPU中的装箱才是有效的。 如果作业相互干扰，装箱就会比暂停-恢复差很多。   装箱的使用  当DLT作业有排他性的访问时，我们使用Profiling来监测它们的资源和进度 如果两个作业被确定为装箱的候选者，我们就把它们装箱在一起，并继续监控它们 如果给定的装箱结果对作业的性能产生了不利影响，我们就解除这些作业的装箱并恢复到暂停-恢复状态      迁移  迁移是Gandiva用来改变分配给DLT作业的GPU集的机制。  迁移在几种情况下是有用的  i）将时间分割的作业移到集群中任何地方的空闲GPU上 ii）将相互干扰的作业移开 iii）对集群进行去碎片化处理，使进入的作业获得具有良好位置性的GPU   我们评估了两种解决DLT进程状态迁移的方法。  在第一种方法中，我们利用一个通用的进程迁移机制，如CRIU。  CRIU本身不支持使用GPU设备的进程迁移  我们首先对GPU对象进行检查 调用CRIU之前从进程中删除所有GPU状态   因为CRIU检查点和恢复整个进程的内存  对于这些使用PyTorch的DLT作业来说，检查点的大小是GB级别的。 对于单GPU作业来说，所产生的迁移开销约为810s，对于多GPU作业来说则更高。     我们考虑的第二种方法是使用具有检查点意识的DLT作业。  Tensorflow等DLT框架已经支持允许自动检查点和恢复模型的API 通过在迁移前对目的地进行预热，并且只迁移必要的训练状态。我们可以降低迁移开销小到一两秒     无论采用哪种方法，我们发现，与它在提高GPU总体利用率方面提供的好处相比，服务器间迁移的开销是值得的    增长与收缩  Gandiva用来消除GPU对DLT作业的排他性的第三个机制是增长-收缩。  这种机制主要针对集群可能没有被完全利用的情况 基本思想  在空闲时间内，适时地增加可用于作业的GPU数量 在负载增加时相应地减少可用的GPU数量   许多DLT工作，特别是在图像领域，随着GPU数量的增加，会看到线性的性能扩展。  Gandiva只对那些特别声明他们有足够的适应性来利用这些增长机会的DLT工作应用这一机制。 当多个DLT工作符合这一标准时，Gandiva使用Profiling来估计每个工作的进度，然后相应地分配GPU。      Profiling  Gandiva监控资源使用情况  如CPU和GPU利用率，CPU/GPU内存等。 Gandiva的独特之处在于，它还以一种应用感知的方式内省  利用了DLT作业表现出的规律内省 利用周期性以估计DLT进度   Gandiva估计DLT作业的mini_batch时间  即对一批输入数据做一次前向/后向传递的时间，作为GPU内存使用周期的两个最小值之间的时间 由于DLT作业在其生命周期中通常会执行数百万次这样的小批量操作，调度器会在调度决策之前和之后比较DLT的mini_batch时间以确定其有效性。   Gandiva可以决定装箱是否有效  通过比较装箱前后两个DLT作业的小批量时间 如果没有这样的剖析，为了做出装箱的决定  人们不仅要对两个DLT作业在不同GPU上的性能进行建模， 还要对它们可能相互干扰的各种方式进行建模（例如，缓存、内存带宽等） 这不是一项简单的任务        调度原则   定义：在我们描述调度器的细节之前，我们定义一些术语。\n DLT作业  被封装在容器中, 包括  所需的GPU数量 优先级（可以是动态的 ） 一个指示作业是否能够增长-收缩的标志   我们假设一个作业所要求的GPU数量是2的幂。   集群  一个集群由一个或多个服务器组成 每个服务器有一个或多个GPU 我们假设一个专门的GPU集群用于DLT工作   服务器的高度  定义为⌈M/N]， 其中M是分配的GPU数量，N是总GPU的数量。 只有当服务器的高度超过1时，才会使用暂停/恢复机制。   集群的高度  被定义为其所有服务器的最大高度。 当集群的高度大于1时，就会出现过载；即所有作业的请求/分配的GPU之和大于GPU的总数。   服务器的亲和力  被定义为分配给该服务器的作业类型。 例如，最初服务器的亲和力为零，如果一个需要两个GPU的作业被分配到一个服务器上，那么该服务器的亲和力就会变成两个。 这个参数被调度器用来将具有类似GPU需求的作业分配到同一台服务器上。      目标\n Gandiva调度器的主要设计目标是为作业提供早期反馈。  在流行的调度器中，作业在过载期间会在队列中等待。 Gandiva通过立即为新作业分配GPU并使用suspend-resume机制提供早期结果来支持超额订阅。   第二个设计目标是集群效率。  通过一个持续的优化过程来实现，该过程使用了剖析和贪婪的启发式，利用了诸如装箱、迁移和增长-收缩等机制。   集群级的公平性不是Gandiva的设计目标。  只关注使用暂停-恢复机制在每个服务器上提供作业之间的公平性 集群级的公平性留给未来工作   为了实现这些目标，Gandiva调度器以两种模式运行：  响应模式：调度器对诸如工作到达、离开、机器故障等事件做出反应 内省模式： 一个持续的过程，过程中，调度器的目标是提高集群的利用率和作业完成时间 请注意，调度器可以同时在两种模式下运行。      响应模式 响应模式被设计用来处理诸如工作到达、离开和机器故障的事件\n\rimage-20221108220646915\r\n当一个新的作业到达时，调度器为该作业分配服务器/GPU。\n  Gandiva使用的节点分配策略如算法1所示。\n findNodes是一个函数，用于返回满足作业请求的节点候选者，并有一个亲和力约束的可选参数。 最初，Gandiva试图找到与新工作具有相同亲和力的节点，并在这些节点中找到具有最小负载的节点。如果存在这样的节点，并且它们的高度小于1（第5-6行），该节点将被分配。 否则，Gandiva试图找到并分配未亲和的节点（第7-8行）。 如果没有这样的空闲服务器，第三个选择是寻找有空闲GPU的节点，同时忽略亲和力（第9-10行）。 这可能会导致多个节点之间的碎片化分配，但正如我们将在后面看到的，迁移可以用于碎片化。如果上述方法都不奏效，这意味着集群中没有可用的GPU。在这种情况下，如果存在具有相同亲和力的节点，它们将被用于暂停-恢复（第11-12行）； 如果没有，作业将被排队（第13-14行）。    放置job\n   传统的调度器将使用作业离开来触发从等待队列中挑选下一个作业。\n   Gandiva检查集群的高度是否可以减少\n 将被暂停的作业迁移到新的空闲GPU上 这个作业可能来自同一台服务器或集群中的任何其他服务器。 作业的离开也可以触发迁移，以改善位置性      Gandiva的工作安排Policy时考虑到了两个因素。\n Gandiva允许超额认购。  当一个服务器被超额认购时，我们会进行加权轮流调度，给每个作业公平的时间份额。   GPU分配不是作业到达时的一次性事件，Gandiva使用自省模式来持续改善集群的利用率。  因此，Gandiva依靠一个简单的作业安置策略来快速分配GPU资源给新作业，从而实现早期反馈。      内省模式 在自省模式下，Gandiva持续监控并优化作业在集群中的GPU上的位置，以提高DLT作业的整体利用率和完成时间。\n  装箱\n 只有在过载时才会考虑装箱。  基本思想：在GPU上同时运行两个或多个作业以提高效率。 可能无效  如果装箱工作的内存需求加起来高于GPU的内存，那么从CPU内存中 \u0026ldquo;分页 \u0026ldquo;的开销就会很高，装箱就没有效果。 当两个或更多作业的内存需求小于GPU内存时，装箱仍然可能不比暂停-恢复更有效。     鉴于DLT作业的异质性，对装箱的性能进行分析建模是一个具有挑战性的问题。  相反，Gandiva依靠一种贪婪的启发式方法来装箱job。  当job到达时，我们总是使用暂停-恢复的独占模式运行它们，并收集剖析信息（GPU利用率、内存和作业进展率）。 基于剖析数据，调度器维护一个按GPU利用率排序的作业列表。 调度器贪婪地挑选出GPU利用率最低的作业，并试图将其装箱到具有最低GPU利用率的GPU上。我们只在装箱作业的总内存利用率不超过GPU的总内存时才这样做。 当装箱作业的总吞吐量大于时间切割时，装箱被认为是成功的。 如果装箱不成功，我们就撤消装箱，并尝试下一个利用率最低的GPU。 如果装箱成功，我们找到下一个利用率较低的作业并重复这个过程。   根据我们的评估，我们发现这个简单的贪婪的启发式方法实现了26%的效率提升。      迁移\n GPU的位置性在一些作业的性能中起着重要作用。 在Gandiva中  每当作业离开时，我们都会使用迁移来改善位置性， 同时也作为一个后台进程来 \u0026ldquo;整理 \u0026ldquo;集群的内容。 为了改善位置性，我们挑选那些不在同一地点的作业，并试图找到一个新的同一地点的位置。     \rimage-20221109111201868\r\n 图8展示了一个集群实验的例子（第6.4节）。  当一个有4个作业、每个作业需要2个GPU的多作业被安排时，它的GPU亲和性很差；  只有J0的两个GPU被安排在一起，而多作业中的其他3个作业（J1、J2和J3）被分配到不同的GPU。 三分钟后，一个背景训练作业DeepSpeech完成了，并释放了它的8个GPU。 8个GPU中的3个，在图8中标记为D，位于三个不同的服务器（服务器1、3和4），可以提高多任务的训练效率。 因此，Gandiva启动了迁移过程，将J1、J2和J3重新安置到同地的GPU。        去碎片化  我们在所有非空闲的服务器中挑选出拥有最多空闲GPU的服务器。 然后我们尝试将运行在该服务器上的作业迁移到其他服务器上。 只要性能损失可以忽略不计，作业就会被迁移到另一个空闲GPU较少的服务器上。 我们重复这个过程，直到每台非空闲服务器上的空闲GPU数量少于阈值（在我们的实验中是4个中的3个），或者没有作业会从迁移中受益。      扩缩\n 增长-收缩的条件  集群未被充分利用 DLT作业明确指出自己可以进行增长-收缩   限制  在我们目前的系统中，我们只让作业增长到单台服务器中可用的最大GPU数量。 此外，我们只在空闲一段时间后触发增长，以避免惊扰， 在新作业可能需要GPU时立即收缩。      时分\n 我们在每个服务器中支持轮回调度，以公平地分享GPU。 当作业有多个优先级时，较高优先级的作业将不会被暂停以适应较低优先级的作业。 如果一台服务器被较高优先级的作业完全利用，如果可行的话，较低优先级的作业将被迁移到另一台服务器。    实现 DLT作业被封装为Docker容器，其中包含我们定制的DL工具箱和Gandiva客户端的版本。这些工作被提交给Kubernetes系统。Gandiva还实现了一个定制的调度器，然后对这些作业进行调度。\n调度器   Gandiva由一个定制的中央调度器和一个客户端组件组成，客户端是每个DLT工作容器的一部分。\n  调度器只是另一个由Kubernetes管理的容器。\n Kubernetes负责整体集群管理， Gandiva调度器管理DLT作业的调度。 Gandiva调度器使用Kubernetes API来获取集群节点和容器信息，每当提交一个新的容器时，调度器会根据调度策略将其分配给集群中的一个或多个GPU。    当一个容器被安排在一个节点上时\n 最初只有Gandiva客户端开始执行。 然后，它轮询Gandiva调度器，以确定哪些GPU可用于DLT工作， 并使用暂停/恢复和迁移命令控制DLT工作的执行。    虽然我们集群中所有GPU的调度完全由中央调度器控制，但如果弹性成为我们关心的问题，可能需要一个分层的方法。\n  DL工具的修改   PyTorch的时分。\n  发出SIGTSTP信信号\n Gandiva client会发出一个SIGTSTP信号，表示工具包必须暂停进程。 它还指示恢复是否应该通过内存文件在新的GPU中发生。 收到信号后，工具包会设置一个暂停标志，并且只在一个小批处理边界的末端执行暂停。    识别mini-batch边界\n 在Tensorflow这个定义并运行的工具包中，mini-batch的边界很容易识别（session.run()的结束）。 在PyTorch这个逐一定义的工具包中，  我们通过跟踪GPU内存使用周期来确定mini-batch的边界， 这是PyTorch的GPU内存管理器（THCCachingAllocator）的一部分， 每当GPU内存被释放时，我们就会寻找一个周期最小值。      执行暂停\n 一旦检测到周期最小值，工具包 i）将所有存储的对象从GPU复制到CPU，ii）释放GPU分配，以及iii）暂停进程。    恢复进程\n 当Gandiva客户端发出SIGCONT信号时，工具包会分配GPU内存，将存储的对象从CPU复制到GPU，并恢复进程。 为了处理重新启动时的地址改变，我们跟踪工具包中的GPU对象，并用新的地址对其进行修补。 改变GPU需要调用cudaDeviceReset和CudaInit，这可能需要5-10秒。我们通过在 \u0026ldquo;暂停 \u0026ldquo;时在后台执行这些操作来隐藏这个延时      Tensorflow迁移\n 我们在每个服务器上部署了一个迁移助手，以支持按需checkpoint和迁移。  当收到来自调度器的迁移命令时，目的地助手首先预热TF会话并等待检查点的到来。 然后，源帮助者要求TF保存检查点，在跨服务器迁移的情况下将检查点移到目的地，最后恢复训练会话。 为了加快迁移过程，我们采用Ramdisk将检查点保存在内存中。在跨服务器的情况下，修改后的TF通过网络文件系统（NFS）协议将检查点直接保存到远程Ramdisk。   当Migration Helper要求作业执行checkpoint时  修改后的TF会在一个小批处理结束时调用tf.Saver。 对于数据的并行性，检查点只包括一个GPU中的模型，而不考虑训练中使用的GPU数量。 为了进一步加快TF的迁移，我们在检查点中不包括元图结构，因为它可以根据用户代码进行重建。   在预热阶段，修改后的TF检查GPU配置并重建元图。  它进一步创建Executor来运行预热操作，以确保初始化不会被懒惰地推迟。 当恢复训练过程时，修改后的TF加载检查点，由多个GPU并行加载，并继续进行训练。      ","date":"2022-11-08T09:59:52+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202211081933118.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0gandiva%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】Gandiva论文阅读笔记"},{"content":"docker 的使用方法 docker的安装 docker的版本比较多， 大家可以自行搜索安装方式， 以下只是参考。\nUbuntu 安装docker.io比较方便\n查询可安装版本 apt-cache madison docker 安装docker sudo apt install docker.io 指定安装版本 sudo apt install docker.io=18.09.1-0ubuntu1 启动docker sudo systemctl start docker 重启docker sudo systemctl daemon-reload sudo systemctl restart docker 设置开机自启动 $ sudo systemctl enable docker 或 $ sudo systemctl enable docker.service --now 验证docker systemctl status docker docker --version centOS 需要安装docker-ce， docker社区版\n卸载旧版本 yum remove docker docker-common docker-selinux docker-engine 安装依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 设置源 阿里源\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 清华源\nyum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装 查看可安装版本\nyum list docker-ce --showduplicates | sort -r 选择版本安装\nyum -y install docker-ce-18.03.1.ce 启动docker systemctl start docker systemctl enable docker docker 的基本命令 本地镜像管理  images rmi tag build history save load import  容器生命周期管理  run start/stop/restart kill rm pause/unpause create exec  容器操作  ps inspect top attach events logs wait export port stats  容器rootfs命令  commit cp diff  镜像仓库   login\n  pull\n  push\n  search\n  https://www.runoob.com/docker/docker-import-command.html)\n  info|version  info version  docker 部署实例 Hello World runoob@runoob:~$ docker run ubuntu:15.10 /bin/echo \u0026quot;Hello world\u0026quot;\rHello world\rRedis docker pull redis:latest\rdocker run -itd --name redis-test -p 6379:6379 redis\rdocker exec -it redis-test /bin/bash\r相关问题 1. docker.io 和docker-ce 的区别  维护者：\n docker-ce 是 docker 官方维护的 docker.io 是 Debian 团队维护的  依赖管理：\n docker.io 采用 apt 的方式管理依赖 docker-ce 用 go 的方式管理依赖，会自己管理所有的依赖。   2. ","date":"2022-11-01T16:44:33+08:00","permalink":"https://tweakzx.github.io/p/dockerdocker%E7%94%A8%E6%B3%95/","title":"【Docker】Docker用法"},{"content":"突发恶疾 关于手头的工作 现在在工位上摸鱼，准备在别人的代码（gaiaGPU）上增加一些功能。\n但是自己看相关的代码，论文，知识已经过去好久了，始终没有下手写出代码。\n似乎折腾了很久，确实也没有弄懂什么。\n这个项目是课题组唯一的项目了，按道理来说我应该有着大量的精力来做这件事，但不知为什么总是精神郁郁。\n关于为什么会突发恶疾 所谓突发， 无非是突然间什么都不想做了，进入到一种抑郁颓废的状态。\n听到后面课题组的学姐在吐槽她带的一个学生能力很差，无法推进工作。\n我听到之后觉得好像是有很多蚂蚁在身上爬，毕竟我也能力很差，项目做了一段时间但是没有推进。\n另一方面就是导师把我手头的工作好像偷偷交给一个学弟来做了，这是让我更加有压力的一件事。\n毕竟导师是一个擅长阴阳怪气揶揄讽刺的人。\n他偷偷的这样做虽然不一定是出于坏心，但是学弟如果做出来了，进度比我快的话，那我一定是免不了收到一些不大友善的评论的。\n什么是我想要的呢 这个项目涉及到k8s， gpu， docker，机器学习，似乎项目涉及的东西很多，我一时间摸不到该怎么扩充自己的知识。\n实际要做的功能又似乎什么都不需要懂，只要在别人的代码上加一点逻辑就行，可是我看源码又是摸不到该在什么地方加。\n似乎我总是在制造障碍，阻碍自己做到这件事。\n我总是希望有个懂做项目的人来指导一下我，点破当前的迷津。\n可是导师肯定是指望不上了，他几乎什么都不懂，组里的同学做的东西又都不一样。\n现实往往是\u0026hellip; 不光是导师水平奇差，没有指导。\n自己在学习这件事上也是不太擅长，每次打算要学习什么东西，总是感觉自己学不会。\n或者说根本体会不到在学习的感觉，更别谈学会什么的感觉了。\n可能是多年的高考应试思维吧，几乎是不学什么真正的知识，而是利用记忆力去完成一些事情。\n大学几年下来没有将什么知识真正转化成我的技能。\n差不多都是在考前一天利用记忆力通过考试，分数不会很高但也不会很低。\n​\n我觉得长期的抑郁情绪已经严重影响到我的记忆能力了。\n我开始频繁的发现自己的记忆卡壳。\n本来就没有实际技能的我，失去了原本的一些优势之后就真的什么都没有了。\n现在要自己上手做一些开发，对我来说真的就是很困难。\n这样一门动手的学科我却恰恰缺乏动手的能力。\n​\n将来的工作似乎也无法获得解脱。\n在工作上，如果也是自己去漫无边际的摸索，那对我来说将是一场苦旅。\n在这样容易迷失的荒漠之中，我急切需要一个愿意提供指导的引路人。\n从来没有成长，只有伤痕的增加 人们常说吃一堑长一智，这句话对我来说似乎不太起作用。\n我似乎在成长这件事上也做的很是吃力，痛苦随着年龄的增加似乎出现了指数级的增长。\n我总是在毕业之后才能明白，有些孤独与矛盾是没有必要存在的。\n关于本科阶段为什么停滞 似乎高考留下的后遗症，大一的时候一直卷高数，卷完高数，卷大物，还要卷大学的其他课程\n从来没有胆量考虑过自己要做一个不能加学分的项目，目光一直盯着比赛，大创，以及很水的本科课程\n结果就是成绩还不错，专业前十保研到了计算所\n让我回望本科，后悔的事情有很多，\n一是没有一个感兴趣的方向，然后努力学习下去\n二是没有很好的和同学做好朋友，大概是竞争导致的，和同学有所疏远\n三是没有开始刷题，如果当时多刷一点题的话，保研和找实习的笔试都会比较顺利\n关于第一件，我实在是见到的，知道的太少了\n似乎从来都缺少一个契机，漫无目的，没有方向\n在不知道什么的竞争心理作用下，被看不见的锁链困在原地，不敢真正的做点有用的东西\n学习当然也可能没有那么重要，毕竟人的变化不一定能跑赢环境的变化\n第二件的话，实话说和同学相处的都还可以，起码没有关系很差的同学\n但也很少有关系很好的同学\n关于室友，起初可能就是一些细小的矛盾和危机感导致的\n关系还不错，但是总是有一些生疏，就是很有礼貌的生疏\n大家都各自有了朋友，可能是太多缺乏安全感导致的，我对朋友去交别的朋友这件事十分敏感\n现在想来真的是，明明把很多事情说开，活得会轻松很多，朋友之间相处也会自在很多\n积郁着，总是淡淡愁，如同雨季里听不见远方的笑声\n一个人在细雨打湿的长椅上，看着微微透着光亮的阴天\n最喜欢在校园里行走，浓荫遮蔽的交大校园，适合浮躁的人散散心\n关于我的敏感，焦虑，危机感导致和朋友疏远这件事\n我想我总是负着一些愧疚的，或许我轻松一点，我周围的朋友也可以轻松一点\n其实隐藏在这背后的，软院的学习氛围其实推动了一些同学包括我的心理的扭曲\n学院不算很卷，但是资源并不多，实话说课程质量其实不太能提高学生的实际水平\n学院提供不了学生需要的成长与发展，尤其在计算机这个看重能力的领域\n导致学生出现焦虑也很正常\n大家都想快速获得一些发展，超越同龄人，导致了一些黑暗森林法则的事情的发生\n不要出声，自己在学什么，自己在做什么，不要出声\n闷声发大财的理念是大家的默契\n然而闷声的很多，发财的很少\n关于第三件事，没有好好刷题，其实是根本没有刷题的方法与紧迫感\n当时的紧迫感主要来自周围的同学都在做什么\n刷leetcode题对绩点能有什么帮助呢？\n即使我们都知道找工作要刷算法题，保研也要过算法考试\n但是大三前大家没有这样做的打算，大三后也已经来不及\n另外就是我当时不知道刷算法的路线与教程\n自己打开lc，随便看几道都是不会做的题\n甚至想到解法，代码的实现细节还是无法落地\n现在还好刷了400多道题，虽说不是刷题高手，但是起码不会害怕题目了\n这件事为什么会导致我的后悔呢？\n没有刷题导致我错过了很多机会\n关于保研的时候为什么做出错误的选择 保研的时候，一方面自己的简历没有出众的地方\n甚至关于保研的准备就是拿到了保研资格，保研之后的找导师几乎是没有准备的\n面试也都是几乎没有准备的，自己简历空有绩点而无实质\n现在的导师，是让我很后悔的一个选择\n导师的学术能力很差，课题组没有实际的科研方向，捞钱是唯一的方向\n除了提供一些十分牵强离谱的idea，提供不了什么指导\n在他身上我看到了自卑和自负是一枚硬币的正反两面\n然而我自己也没有什么想法，或许选择做科研并不是一条适合我的路\n科研嘛，要么导师有好的想法，要么自己有\n要是都没有，那就不好搞了，尤其是导师沉浸于宏大叙事而无实事求是能力的情况下\n选择他，主要还是因为当时并不知道，中科院计算所的导师水平可以这么差\n原本以为导师脾气差一点无所谓，脾气差可能是要求高，对自己不一定是一件坏事\n但后来我发现，脾气差往往是无能的表现，脾气越差的人越是无能狂怒\n当然可能是网络上被不太好的评价刺激到了\n导师现在十分收敛，对学生天天是点赞，夸奖，尬吹，（阴阳）\n大家对他的夸奖感到十分的厌烦，可以说导师的夸人的情商非常低\n一个错误的选择真的耽误了我很长的发展时间\n甚至未来两年也不会太乐观\n关于刚刚被骗钱的事 对了，倒霉的事对我来说可不止一件\n9月底，我经历电信诈骗，被骗走了很多钱\n刚刚问了警察同志，反正钱大概率是追不回来了\n算了，就当丢了吧\n损失这么惨重，导致我10月其实有一点消沉\n不过还好，工作不顺利才是我头疼的主要原因\n紧接着被拉去酒店自费隔离，是真的，本不富裕的生活雪上加霜\n交完酒店的钱，山穷水尽了，啊不，我还有外债\n去他妈的人生\n关于对别人的羡慕 一个负面情绪很多的人对正常人的自然羡慕 我真的负面情绪很多吗？我想说不\n我表现出来的很多，但更多的是无奈与疲惫\n这些都是可以解决的，如果我能力突飞猛进\n或者有一个适合我成长的环境，很多问题都会得到缓解\n我常常看到周围的同学目标明确飞速进步而感到焦虑\n但每个人都有不同的发展空间嘛，发展的时间节点也不太一致\n大不了给自己一点时间，晚几年也没什么的\n我的焦虑是现实原因导致的，几乎无法缓解\n周围的同学可以自由的成长和生活，免不了让我心生很多羡慕\n一切都来得及 但是我还是会许愿\n一切都来得及，每个人都有不同的人生课题\n人这一辈子自己是唯一的长期观众\n","date":"2022-10-21T15:52:45+08:00","permalink":"https://tweakzx.github.io/p/%E5%BF%83%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%BE%80%E4%BA%8B/","title":"【心及】一些往事"},{"content":"《AntMan: Dynamic Scaling on GPU Clusters for Deep Learning》论文阅读笔记 Abstract   如何在大规模GPU集群上有效调度深度学习工作， 对于工作性能，系统吞吐量和硬件利用率至关重要。\n 随着深度学习的工作量变得更加复杂，它变得越来越具有挑战性。    本文将介绍Antman， 这是一种深入学习的基础设施，该基础架构共同设计了集群调度程序，并已在阿里巴巴部署在生产中，以管理数以万计的每日深度学习工作。\n Antman适应深度学习训练的波动资源需求。因此，它利用备用GPU资源在共享GPU上共同执行多个作业。 Antman利用深度学习训练的独特特征，在深度学习框架内为显存和计算资源引入动态缩放机制。这允许job之间的细粒度协调并防止工作干扰。 评估表明，Antman在我们的多租户集群中不损害公平性的情况下，整体将显存利用率提高了42％，计算资源利用率提高了34％，为有效利用大规模的GPU提出了新方法。    Introduction  在共享多租户的DL集群， 许多工作排队等待资源的时候会导致GPU利用率低下，有两个原因  大多数的训练任务在执行过程中不能完全利用GPU资源  训练一个DL模型通常需要多种计算的混合 当使用分布式的训练的时候，90%的时间会被浪费到网络通信上   基于资源预留的集群调度方案导致显著的GPU空闲，DL工作中总有部分资源没有投入使用  例如，随机梯度下降是同步的，需要获取所有的资源以进行gang-scheduling， 在得到所有资源之前，已得到的部分资源就会陷入空闲     在共享GPU上进行packing job  可以提高GPU的利用率，可以使得同样的集群整体上胜任更多的job。 但是这个策略在生产集群上很少使用， 原因  尽管提升GPU的利用率是有利的，但也要保证resource-guarantee jobs（RGJ，资源保证性job）的性能。同一个GPU上同时执行多个job会导致干扰\u0026ndash;\u0026gt;RGJ性能出现显著下降 job packing策略可以给并发job引入内存竞争。job需要的资源陡然增加的话，有可能导致训练任务failure。   因此，job资源的独占分配在显存的GPU集群生产环境中比较典型   我们提出AntMan  简述  一个DL系统提高GPU集群的利用率 同时保证公平性与RGJ的性能 通过合作性的资源扩缩来减少job干扰   DL系统中引入了新的分配机制在job训练过程中来动态地精确分配所需资源（显存和计算单元） 使用超卖机制使得任何空闲的GPU资源（显存和计算单元）都能被利用 重新设计了集群调度器和DL框架来适应生产job的波动的资源特点  通过  框架信息感知调度 透明显存扩展 快速可持续的任务间协调   使用这个结构，Antman为DL任务的同时执行的policy design 开辟了空间   AntMan采用了简单且有效的策略来最大化集群吞吐  为RGJ提供资源保证的同时 分配一些偶然性的任务来尽力而为的利用GPU资源（低优先级且不保证资源）     本文主要贡献如下  对生产环境的DL集群进行调研，发现低利用率来自于三个方面：硬件，集群调度，job行为 在DL框架中为显存和计算单元管理引入了两种动态放缩机制，来解决GPU共享问题。新机制利用DL的工作特征来动态调整DL job的资源使用情况，在作业执行期间。 通过共同设计集群调度器和DL框架以利用动态缩放机制，我们为GPU共享引入了一种新的工业方法。这在多租户集群中维护工作服务级协议（SLA），同时通过机会调度来改善集群利用率。 在超过5000个GPU上进行了实验    Motivation deep learning Training  深度学习训练通常包括数百万次迭代，每个迭代过程都称为mini-batch。  通常，训练mini-batch可以分为三个阶段。  首先，计算样品和模型权重以产生一组得分，称为forward pass 其次，使用目标函数在产生的分数和所需的分数之间计算loss error。然后，损失通过模型向后扩散，以计算梯度，称为backward pass。 最后，通过由优化器定义的学习率来缩放梯度，以更新模型参数。   正向通行的计算输出通常包括许多数据输出，每个数据输出称为张量。这些张量应暂时保存在内存中，并被向后通过以计算梯度。通常，为了监视培训中的模型质量，定期触发评估.   为了使用大量数据培训模型，DL通常在多个GPU中采用数据并行性，其中每个GPU负责并行处理数据子集，同时在模型更新之前执行每个迷你批次梯度同步。 在大型公司中，多租户集群通常用于改善硬件利用率，用户有时可以超额订阅GPU资源配额，尤其是当GPU要求爆发时。  Characterizing Production DL Cluster 我们从三个角度研究生产集群中的资源使用率：硬件，集群调度和job行为。\n  在使用中的GPU的低利用率\n \rimage-20220831200958715\r 内存容量。如图所示，只有20％的GPU正在运行消耗一半以上GPU存储器的应用程序。 关于计算单元的使用，只有10％的GPU获得了高于80％的GPU利用率。 该统计数据表明，GPU内存和计算单元均未得到充分利用，因此浪费了昂贵的硬件资源。    对gang-schedule的空闲等待\n Gang scheduling是在并发系统中将多个相关联的进程调度到不同处理器上同时运行的策略，其最主要的原则是保证所有相关联的进程能够同时启动，防止部分进程的异常，导致整个关联进程组的阻塞。例如，您提交一个批量Job，这个批量Job包含多个任务，要么这多个任务全部调度成功，要么一个都调度不成功。这种All-or-Nothing调度场景，就被称作Gang scheduling。\n  \rimage-20220831201015900\r 多GPU训练工作需要Gang scheduling，这意味着除非同时提供所有必需的GPU，否则job将不会开始训练 由于GPU资源往往不能同时全部获得， 所以会出现idle waiting。 一个job需要的GPU越多， 它的平均闲置时间就越长 **资源到达的不可预测导致了预留资源的idle waiting。**一个幼稚的解决方案是在GPU idle waiting的时候执行别的job。但是这会导致资源需求大job的饥饿，影响分配公平性。另外一旦资源需要被满足导致的GPU需求激增或许导致GPU之间的资源冲突，导致工作fail    动态资源需求\n \rimage-20220831201047910\r 在一个DL job生命周期中， GPU资源往往未被充分利用。 图3：在Criteo数据集上运行DEEPFM [20]时的前10分钟。一开始，数据集上的预处理仅需要CPU。但是，GPU的SM利用率和内存使用量在275秒时启动。 图4：训练可以包含几个阶段，不同阶段的SM和Memory的使用率都是不一样的 资源需求的动态变化与固定的资源分配和漫长的训练时间相矛盾。资源需要满足job的峰值需要，导致这个昂贵的硬件被低效利用。显存的DL框架的memory caching设计隐藏了显存使用随时间的变化，一定程度上阻止了GPU的潜在共享。    Opportunities in DL Uniqueness   超卖有机会提高集群的吞吐\n 不可预测的job内和job间的需求激增为安全的资源共享引入了挑战  因为资源竞争， jobs可能会把显存用光 在多租户集群中，当jobs在共享环境执行时，为持有定额资源的job提供性能隔离是十分重要的   AntMan利用DL training的特性来利用这个机会    我们在生产环境集群的10k个task中取样发现\n \rimage-20220905143619325\r 只有一小部分用来存储模型， 且90%的模型大小不超过500M 大部分显存在同一个mini-batch中被分配和释放 除此之外，一个mini-batch的消耗时间很短， 80%的任务在600ms之内消耗一个mini-batch    我们通过多种方式利用这种独特的特征来安排共享GPU上的作业。\n 首先，根据通常的模型大小，大部分显存在共同执行的作业中可以拿来调度 其次，mini-batch的周期通常很短，可以在每个mini-batch边界处进行细粒度的GPU内存和计算资源的调度。这可以进一步允许job间的快速资源协调。 第三，mini-batch通常进行相似的计算，这可以被利用去描述job性能，因此进度率可以被创建作为性能矩阵来量化干扰。    Design AntMan深入共同设计集群调度程序和DL框架， 本部分将介绍三部分：DL 框架的修改，调度器与调度原则\nDynamic Scaling in DL Frameworks  低使用效率的GPU集群有着执行更多任务的潜力，但需要解决一下挑战  使用最下需求执行job的同时防止GPU内存使用的爆发导致的失效 适应计算单元的使用波动的同时限制潜在的干扰   DL框架是专注于GPU执行， 缺乏job合作的能力。这激发了动态缩放机制的设计，机制包含内存和计算两部分  Memory Management AntMan以tensor为单位，在GPU和CPU的内存间进行动态内存管理。类似于操作系统的虚拟内存，AntMan以tensor为单位进行了显存虚拟化，通过这种方式，DL框架可以提供超出上限的显存。\n  显存分配\n \rimage-20220905151151692\r 在张量被销毁后，GPU内存被缓存在DL框架内的一个全局内存分配器中，普遍情况下，一些张量只在DL训练的某些阶段使用（如数据预处理、评估），不再需要了。然而，这部分缓存的GPU内存不会被释放（图6c）。DL框架中的这种缓存内存设计优化了单个作业的性能，但代价是失去了共享潜力。 AntMan转向了扩展GPU内存上限的方法。它主动检测使用中的内存，以缩小缓存的内存，从而自省地将GPU内存的使用调整合适。这是通过监测应用性能和处理小批量时的内存需求来实现的（图6d）。AntMan使用其最大的努力在GPU设备上分配张量，然而，如果GPU内存仍然缺乏，张量可以在GPU之外用主机内存分配（图6e）。当GPU内存的上限增加时，Tensors可以自动分配回GPU（图6f）。    显存上限动态调整\n  \rimage-20220905151235705\r\n  图7a说明了内存扩展如何解决突发需求。在T0，正在运行的DL训练作业的内存需求增加，由于GPU内存的有限上限，一些张量不能放在GPU内存中，而是使用主机内存创建。AntMan检测到主机内存的使用情况，在T1，它根据主机内存的使用情况提高该作业的GPU内存的上限，虽然在这一个小批次中，这个运行作业的性能可能会减慢，因为张量被放置在主机内存中，但是后续的tensor都会申请到显存上。考虑到一个典型的DL训练往往需要数百万个小批次，这种性能开销是可以忽略不计的。\n    运行时显存细粒度调整\n 此外，AntMan在运行时提供细粒度的GPU内存调度。如图7b所示，一个训练作业可能会收缩以确保其他作业的内存资源，并在其他作业完成后再增长。它说明了一个DL作业在T0时缩减，在T1时增加，代价是在主机内存上分配了一些张量。因此，在同一共享GPU中运行的作业在T0和T1之间对剩余GPU内存的使用是有保障的。    Computation Management  动态计算单元管理，用于控制DL训练作业的GPU利用率。   类似cgroup，可以在运行时动态地隔离DL特定进程的GPU计算资源访问。\n  当多个DL作业在同一个GPU上启动时，干扰主要是由潜在的GPU内核排队延迟和PCIe总线争用引起的，这会导致所有作业的性能一致下降，如果packing job是在相同的模型和配置上运行的话。\n  **如果不同的作业被打包在一起，作业会以不同的方式变慢。**这是因为作业在获取GPU计算单元方面有不同的能力。因此，作业性能在GPU共享中几乎无法保证或预测，导致多租户集群的GPU共享部署困难。\n  AntMan中，GPU运算器的执行是由一个新引入的模块负责的，称为GpuOpManager。\n 当GPU运算器准备执行时，它被添加到GpuOpManager，而不是直接启动。 GpuOpManager的主要思想是通过延迟执行GPU运算符来控制启动频率。GpuOpManager通过这种方式来限制DL训练作业的GPU利用率。GpuOpManager不断对GPU运算器的执行时间进行分析，并在启动GPU运算器之前简单地分配空闲的时间段。 请注意，GpuOpManager只是延迟了GPU内核的执行。因此，运算符（包括GPU运算符和CPU运算符）之间的潜在依赖关系被保留下来，这意味着如果可能的话，CPU运算符可以继续。    \rimage-20220905151222951\r\n 图8说明了在同一GPU上执行的两个作业的GPU计算单元干扰的例子。图8a说明了Job-A是如何以细粒度的方式在GPU上执行的。简而言之，GPU内核将被按顺序放置，并由GPU计算单元逐一处理。请注意，在图8中，Job-A可能无法使GPU完全饱和，导致GPU周期闲置，GPU利用率低，有可能被其他作业使用。\n因此，作业-B被安排在这个GPU上（图8b）。Job-B的GPU操作者启动在GPU中执行的内核（绿色块），这可以填满它，从而延迟其他GPU内核（蓝色块）的执行，导致Job-A的性能不佳。\n这种干扰主要来自于缺乏控制GPU内核执行频率的能力。为了解决这个问题，我们在DL框架中引入了一个GPU运算器管理器（图8c）。\n现有的DL框架一旦满足了GPU运算器的控制依赖性，就会在GPU运算器中发布GPU内核。在如图8c所示，第三个CPU操作符没有被阻止，然而，第四个操作符被延迟了，因为它依赖于第二个GPU操作符，它的执行被GpuOpManager延迟了。\n     Collaborative Scheduler   AntMan的整体架构\n \rimage-20220905152947633\r AntMan采用了一个分层结构  一个全局调度器负责job 调度 每个工作服务器都包含一个本地协调器，负责通过考虑DL框架报告的统计数据，使用动态资源扩展的基元管理作业的执行   AntMan是为多租户GPU集群设计的。  在多租户集群中，每个租户通常拥有一定的资源，被注释为资源配额（即GPU的数量），这是可以分配给该租户的作业的并发性能保证资源。 每个租户的GPU资源配额之和小于等于一个GPU集群的总容量。   在AntMan中，工作被分为两种，由全局调度器应用不同的调度策略  资源保证型工作：资源保障型作业会消耗其相应租户的一定数量的GPU资源配额。AntMan确保资源保证作业的性能应该与独占执行的性能一致。 机会型工作：机会型作业则不会。      AntMan各模块的运作方式\n 调度决策可以被视为一个自上而下的控制流  在AntMan中，与传统的集群调度器类似，调度决策由全局调度器派发给本地协调器 本地协调器使用动态缩放机制对GPU资源进行内省式调度，以达到DL训练作业的目的   数据统计流信息由本地协调器的统计模块收集，并以自下而上的方式汇总到集群统计模块上  信息  硬件信息  GPU利用率 GPU内存使用率   DL框架报告的详细作业信息  小型批次持续时间 峰值内存使用率 最小内存使用率 主机内存消耗等     这些信息可以帮助全局调度器做出作业调度决策  峰值内存和最小内存使用量是用来指示可以快速提供的GPU内存大小 批处理时间显示GPU内存多久可以用于另一个DL训练作业     当作业在GPU服务器上启动，本地调度器管理其端到端执行  由于DL训练作业的负载波动，本地协调器以自省(introspective)的方式行事，对DL框架进行持续的作业控制 它从硬件和DL框架中收集所有作业的统计数据 -\u0026gt; 使用我们在第3.1节中介绍的新原语 \u0026ndash;\u0026gt; 通过资源使用调整（例如，收缩GPU内存）\u0026ndash;\u0026gt; 来控制作业性能      Scheduling Policy   目标\n  由于集群上的负载和作业的资源需求不断波动，在提供公平性（如确保DL作业的SLA，保证资源）和实现高资源利用率（如GPU利用率）之间存在着固有的矛盾。 普遍的生产型DL集群调度器经常以某些方式用公平性换取效率。  例如，空闲资源被分配给超额配置的租户。 然而，这样的GPU资源在没有抢占的情况下很难拿回来。一般来说，抢占很少被使用，因为它使正在运行的作业失败，同时浪费了昂贵的GPU周期。 此外，还报告了歧视大型作业的失序行为（即分配更多的GPU），导致倾向于小型作业的不公平。      首要目标：多租户公平性  AntMan通过在全局调度器和本地协调器中实施的政策实现了公平性 这些政策由动态扩展机制提供支持。   第二优先：提高集群效率，从而实现更高的吞吐量  AntMan中还引入了GPU机会主义工作，以窃取GPU中的空闲周期，从而最大限度地提高集群的利用率。      全局调度器：维护着工作到达的多个租户队列，决定为工作分配的GPU的位置\n  调度策略\n 对于资源保证型工作和机会型工作，AntMan应用不同的调度策略，如算法1所示。 \rimage-20220905153001724\r findNodes是一个函数，它返回满足工作请求的节点和GPU候选者，并有一个可选的参数来指定约束。 全局调度器在有足够的GPU资源的情况下公平地分配资源保证作业。 除此之外，资源保证作业还可以使用空闲的GPU资源来最大化作业性能 。 如果一个作业的资源请求只能部分满足，全局调度器就会为这个作业保留资源。 保留的资源将永远不会被其他资源保证的作业占用，但是它们可以被机会主义作业所利用。    机会主义作业\n 默认情况下，全局调度器将估计没有设置GPU配额的作业的排队时间。排队时间长的作业将被自动作为机会主义作业执行。 目的：为了最大限度地利用自由资源。 它通过考虑实际的GPU利用率在GPU上分配机会主义作业。只有在过去10秒内利用率**低于M（目前设定为80%）**的GPU可以被选为候选。 在最空闲的候选人上分配机会主义作业（即minLoadNodes，第9-10行）。 分配在同一个GPU上的作业由本地协调者管理    AntMan默认会自动选择机会主义作业，但它也允许用户在提交时手动确定作业类型\n 明确指定为资源保证作业，以确保SLA 一个作业也可以被指定为机会主义作业，永远不会占用租户的资源配额   在实践中，用户通常以机会主义模式提交作业，以避免潜在的排队延迟，目的是进行调试和超参数调整，这都是由早期反馈驱动的。\n     本地调度器：协作执行共享GPU上的作业\n  如何在共享执行中确保资源保证作业的性能\n 一个GPU只会分配给一个资源保证作业，因为它消耗GPU配额 本地协调者首先限制机会主义工作使用GPU，防止资源保证工作受到干扰 在启动DL训练作业时，需要由DL框架初始化GPU设备  如果GPU处于高负荷状态，则需要更多时间。（初始化时占用更多资源） 一旦资源保证作业稳定执行，本地协调器将把剩余的GPU内存分配给机会主义作业。   通过监测作业性能（即小批量时间），在不干扰资源保证作业的情况下，逐步增加机会主义作业的GPU计算单元使用量 同样，当一个机会主义作业到达共享GPU时，本地协调器在不影响资源保证作业的情况下，以阶梯式的方式提高其GPU资源使用率。    如何处理资源保证作业的资源需求激增\n 为了意识到动态的资源需求，本地协调器监测DL框架报告的指标 当一个资源保证作业增加了GPU内存需求时，由于有了通用内存，张量被暂时使用主机内存存储。 本地协调者缩减其他机会主义作业的GPU内存使用量，并提高资源保证作业的GPU内存限制，以恢复其性能。 这对GPU计算单元的使用协调是类似的。 🚨AntMan依靠应用层面的指标（即迷你批处理时间）来表明资源保证作业的性能。如果它观察到资源保证作业的性能不稳定，它就会采取悲观的策略来限制其他机会主义作业对GPU资源的使用。    当一个GPU只被机会主义工作所共享时，最大限度地提高聚合工作的性能。\n  如果只有一个机会主义作业，那么GPU资源就可以被这个作业充分利用，而没有任何约束。\n  有时，一个GPU有可能被多个机会主义工作占用。\n  AntMan通过最大限度地提高GPU内存效率来优化聚合作业的性能。\n  在启用动态缩放机制后，我们发现不同的工作负载在内存限制带来的性能下降方面表现出不同的敏感性\n   \rimage-20220905153042457\r\n  如图10所示，\n SR模型即使在其设备内存减少90%的情况下，也只遭受了大约25%的性能下降 Cifar10数据集上的VGG16[43]模型（VGG）即使在设备内存减少一半后，也能保持其大部分的原始性能。 ImageNet数据集（ResNet）上的ResNet50[22]对内存缩减很敏感；10%的内存缩减会带来60%以上的速度下降。       当机会主义作业的总的GPU内存需求超过了GPU的内存容量时\n 将GPU内存分配给最能提高作业性能（Normalized Performance）的作业          Job 升级\n 机会主义工作虽然以best effort执行， 但这是在没有SLA保证的情况下。 全局调度器会在有足够资源的情况下升级这些作业，以快速完成它们。 全局调度器通知本地协调器，将其标记为资源保证作业，并消耗租户的GPU配额来完成作业升级。    对于分布式同步DL训练来说，部分升级没有帮助，因为一个工作者的性能下降可能会广播到整个作业。 因此，全局调度器检查所有GPU是否都被机会主义工作填满。 一旦所有的任务实例都准备好升级，并且资源配额足够，AntMan更愿意将机会主义工作升级，而不是新启一个工作。     Implementation Deep Learning Framework  动态缩放机制在两个流行的深度学习框架中实现  Pytorch Tensorflow   DL框架的修改主要体现在三个部分：  内存分配器  为了实现动态的通用内存，(tensorflow ::BFCAllocator, PyTorch::CUDACachingAllocator）被修改以引入可调整的内存上限。内存分配器会跟踪内存分配的总字节数，并在总字节数超过上限时触发内存不足。 此外，还为内存分配器引入了一个新的接口，允许在任何时候清空缓存内存。 还增加了一个新的通用内存分配器UniversalAllocator，以包裹GPU内存分配器和主机内存分配器（即使用cudaHostMalloc进行内存分配）。  当张量的请求触发了内存分配时，UniversalAllocator试图使用GPU内存分配器来分配内存， 如果GPU内存剩余不足，则将CPU内存分配器作为备份。 🚨UniversalAllocator维护了一个集合数据结构，记录了由GPU分配的内存区域的指针，用来对内存指针进行分类，以便于回收。     执行器  为了实现动态计算单元的扩展，在DL框架中引入了一个带有运算器处理队列的GpuOpManager，它在一个独立的线程中运行。 TensorFlow的运算器被相应地修改，以插入GPU Op到GpuOpManager队列中，从而将GPU运算器的执行专门交给它。 GpuOpManager可能会根据计算能力的有限百分比来延迟GPU运算符的实际执行。   接口   内存使用模式的统计数据和执行信息被汇总到本地协调器上  DL框架和本地协调器通过文件系统进行通信 他们都有一个监控线程来检查文件，以接收工作统计数据或控制信号 为了最大限度地减少内存管理的开销，内存的动态缩放是在mini-batch的边界（session.run()的结束）触发的    Cluster Scheduler  在Kubernetes上实现了一个自定义调度器，作为评估AntMan的原型。  Kubernetes负责集群管理和执行Docker容器中的作业。 我们的全局调度器使用Python APIs来监控Kubernetes的API服务器中的事件，以便进行调度。 本地协调器作为DaemonSet部署在Kubernetes中。每个协调器监控文件系统的某些路径，以收集每个作业的报告信息。 汇总的作业和设备信息存储在ETCD中，这是Kubernetes中内置的分布式键值存储。因此，全局调度器在做调度决策时直接读取ETCD中的状态。    Evaluation Benchmark Trace Experiment Cluster Experiment Ralated Work Conclusion 参考资料\n 论文地址：AntMan: Dynamic Scaling on GPU Clusters for Deep Learning | USENIX 代码地址：https://github.com/alibaba/GPU-scheduler-for-deep-learning 参考：OSDI'20 论文赏：ANTMAN: DYNAMIC SCALING ON GPU CLUSTERS FOR DEEP LEARNING | 高策 (gaocegege.com)  ","date":"2022-08-29T15:41:42+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/20220829154241.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0antman%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】AntMan论文阅读笔记"},{"content":"《GaiaGPU：Sharing GPUs in Container Clouds》论文笔记 Abstract  对于云服务的提供商， 如何在容器间共享GPU， 是一个有吸引力的问题  容器的轻量与伸缩性 GPU强大的并行计算能力 在云环境，容器需要使用一个或多个GPU来满足资源需要的同时， 容器独占式的GPU往往使用率很低   我们提出GaiaGPU，能够在容器间共享显存和算力  将物理GPU划分为多个虚拟GPU 采用弹性资源分配和动态资源分配来提高资源利用率 实验结果显示， 有效的实现了容器间资源的分配和隔离的同时，平均只增加了1.015%的开销。    Introduction  容器化是一种虚拟化技术  涉及到量身定制一个标准操作系统，方便它在一个物理机上运行由多个用户处理的不同应用程序 与VM模拟底层硬件不同  容器模拟的是操作系统 轻量，可伸缩，易部署 微服务打包与发布应用的事实标准   云服务提供商整合容器编排框架（如k8s）到基础架构中来提供容器云   GPU 图像处理单元  有很强的并行处理能力  因为一个芯片上集成了数以千计的计算核 GPU被广泛用于计算密集型任务，以加快计算 随着技术的发展趋势，现代GPU内将集成入越来越多的计算资源   CUDA是多功能GPU最流行的平台，提供了API方便GPU的使用 卓越的性能吸引了很多云提供商将GPU引入云环境  在云环境中，部署在容器中的一个应用程序可能需要一个或多个GPU才能执行， 而另一方面，应用程序的专用GPU资源导致资源不足。 因此，如何在不同的容器中共享GPU对大多数云提供商都非常感兴趣     GPU虚拟化技术是在隔离的虚拟环境（例如VM， 容器）之间共享GPU的技术  多数的GPU虚拟化技术应用于VM， 容器间的虚拟化技术还在起始阶段 现阶段的基于容器的GPU虚拟化技术有以下局限性  需要特定的硬件设备（NVIDIA GRID） 将一整个GPU分配给单个容器， 不能共享 （NVIDIA Docker） 容器间只能共享GPU显存 （ConvGPU） 只支持单个GPU （ConvGPU）     我们提出GaiaGPU，能够在容器间透明地共享显存和算力  用户不用修改容器镜像来共享底层GPU  我们使用k8s的device plugin 框架将物理GPU划分为多个虚拟GPU   每个镜像可以按需分配一个或者多个vGPU 提供了两者方式在运行时更改镜像资源  弹性资源分配：暂时改变资源 动态资源分配：永久改变资源     vGPU包括GPU显存和计算资源  共享显存  容器包含GPU显存的一小部分 vGPU分配的是GPU的物理内存   共享计算资源  共享计算资源意味着每个容器都拥有GPU线程的一部分以并行执行计算。 VGPU的计算资源由GPU的利用率衡量（采样时段内， 容器使用GPU的时间比例）     总结：本文做了如下贡献  提出了GaiaGPU：一种在容器间透明共享显存与算力的方法 采用弹性分配和动态分配的方式提高了资源的利用率 进行了四个实验来验证GaiaGPU的性能。结果：实现了容器间资源的分配和隔离的同时，平均只增加了1.015%的开销。    Related Work GPU虚拟化   被应用于在多个虚拟环境之间分享GPU， 极大地提高了应用性能\n  现存的多数GPU虚拟化技术基于VM，主要有三种虚拟化GPU\n API remoting  vCUDA 和 rCUDA 创建GPU的封装库-\u0026gt;劫持GPU调用-\u0026gt;将重定向到host   para and full virtualization  GPUvm GPUvm将GPU显存和MMIO（存储器映射输入输出）分成几片，将片分给VM   硬件支持的虚拟化  NVIDIA GRID 硬件虚拟化，创建虚拟GPU，挂在容器      与VM相比， 容器使用主机的操作系统-\u0026gt;容器可以直接使用宿主机的GPU驱动-\u0026gt;性能接近原生环境\n NVIDIA GRID  需要特定的硬件和与虚拟GPU要有相同的资源配置 一个容器只能分配一个虚拟GPU   NVIDIA Docker  使得Docker镜像可以使用GPU 允许GPU驱动对CUDA镜像不感知 提供了一个命令行的封装-\u0026gt;当容器启动时可以挂载driver的用户态组件和GPU设备文件 不能共享：只能把一整个GPU分配给一个容器   ConvGPU  容器间共享GPU显存 它拦截了CUDA库来管理每个镜像显存的分配和回收 仅支持分享显存 而且只能虚拟化单个物理GPU   GaiaGPU  软件层虚拟化，没有硬件限制 每个虚拟GPU的资源可以是不一样的 一个容器可以分配多个虚拟GPU 可以同时共享显存和计算资源 可以虚拟化多个物理GPU      Device Plugin  致力于征聘各种计算资源（GPUs， NICs， FPGAs）供集群使用 无需改变k8s的核心代码 工作流  资源发现  实现Device Plugin 通过gRPC将device注册到Kubelet 成功注册后，device plugin发送设备列表 Kubelet负责将这个扩展资源推广给Master   资源分配  用户在容器的specification中请求设备 master的scheduler选择一个k8s节点的device plugin发送device请求 device plugin分配对应的设备给容器   \rimg\r   Vaucher通过利用设备插件框架来实现SGX（Software Guard Extensions）设备虚拟化，  该插件框架修改了Kubelet和SGX代码以限制虚拟SGX设备的设备内存的使用 仅处理内存资源 对内存施加严重限制   GaiaGPU也采用了device plugin框架以在容器之间共享资源  对资源采用弹性限制而不是硬限制-\u0026gt;来改善利用率    Design And Implementation 设计与实现  目标：在容器间共享显存与计算资源，使用最小的成本获得最大的提升 挑战：  透明性：  GaiaGPU不应该修改k8s的代码或者容器镜像 使用共享GPU就如同使用物理GPU一样   低开销  使用共享GPU的性能尽可能接近使用物理GPU   隔离  GaiaGPU应该管理GPU资源在每个容器的分配与回收 共享GPU时容器之间完全隔离     结构：  \rimage-20220828143547571\r 两层资源管理  host层：  GPU Manager负责创建vGPUs GPU Scheduler 负责分配物理GPU资源到vGPUs   容器层  vGPU Library负责管理具体容器的GPU资源     组件  GPU Manager  device plugin：运行在host上负责创建vGPUs，使用gRPC与Kubelet通信 \rimage-20220828171002587\r Register：GPU Manager将自身注册到Kubelet以通知其存在 ListAndWatch：成功注册后，Kubelet调用ListAndWatch获取设备信息List  显存：256M 作为一个单元， 每个单元都被称作一个vmemory设备 计算资源：一个GPU被分作100份vprocessor设备， 每个vprocessor都有百分之一的利用率   Allocate：  映射过程  Kubelet发送随机选择的设备IDs到GPU Manager GPU Manager根据得到的设备IDs计算所需的物理GPU资源 GPU Manage 发送一个请求到 GPU Scheduler GPU Scheduler 返回要分配给容器的物理GPU   映射完成后，GPU manager返回一个allocateResponse， 包含获取分配到的设备的Configurations  容器的环境变量 挂载到容器的目录和文件 分配的设备   Kubelet将这些配置发送到容器运行时     GPU Scheduler  负责处理GPU Manager发来的调度请求 基于拓扑树分配GPU，树的根节点是host， 树的叶节点是物理GPU 当所需要的资源少于一个物理GPU，分配策略会最大程度减少资源碎片 当所需的资源等于一个物理GPU时，采用了最小化单叶节点（即没有兄弟姐妹节点的叶子节点）的分配策略。 当所需的资源不止一个物理GPU时，分配策略的目标是最大程度地降低GPU的通信成本。通信成本取决于两个GPU的连接模式。   vGPU Manager  运行在host，传递容器配置并且监管分配了vGPUs的容器 当容器申请GPU资源时，GPU Manager将配置发送给vGPU Manager，例如  需要的GPU资源 该容器的name   接收到配置后，在主机上为容器命创建一个唯一的路径  路径名为容器名 这个路径包含在allocateResponse里 容器的配置也包含在这个路径里，所以可以通过Kubelet传递到容器   vGPU Manager 和 vGPU Library是 服务器-客户端模式  vGPU Manager 维护一个活着的且分配了GPU资源的容器的list 会定期检查这些容器是否存活 如果容器死了，从list中移除这个容器的信息，并且删除目录     vGPU Library  运行在容器中，管理容器的GPU资源 在容器第一次执行GPU程序时被加载 启动后vGPU Libraray 向 vGPU Manager注册自身 利用LD_LIBRARY_PATH机制拦截CUDA库中内存与计算相关的API  LD_LIBRARY_PATH是一个linux系统的环境变量 可以影响程序的runtime link 允许某些路径先于standard set of directories（？标准库目录？）加载 \rimage-20220828222012346\r   当容器需要的GPU资源超出它申请的资源时，为了避免耗尽整个GPU，有两种资源限制策略  硬限制（hard limiit）：如果容器资源消耗量超过配额，就不再给该容器分配资源 弹性限制（elastic limit）：如果容器资源消耗量超过配额，但是系统中还有空闲资源，那么该容器仍然能够得到更多的资源   内存资源的限制采用硬限制策略， 因为  内存资源大小能决定一个程序能否运行，但对运行的效率影响较小 GPU是计算设备，它们采用一次性资源分配策略。仅在获取所有内存资源后才可以执行应用程序，并且在完成内存之前不会释放。如果使用弹性内存分配，则具有较大内存需求的应用程序将饿死 撤回过度分配的内存资源的唯一方法是通过抛出out-of-memory exception来杀死该过程   计算资源采用弹性限制策略，因为  计算资源对程序运行的影响很大 计算资源（GPU 线程）在执行之后会立刻释放掉         总结：  Step 1：GPU Manager向Kubelet注册自身，并报告vGPU的信息（ListAndWatch） Step 2：Kubelet接收到来自Master的创建一个GPU容器的请求 Step 3：Kubelet发送一个allocateRequest到GPU Manager Step 4：GPU Manager发送一个vGPU调度请求到GPU Scheduler，GPU Scheduler根据调度策略选择实际提供资源的物理GPU。如果调度成功返回一个包含该物理GPU的信息的reponse Step 5：GPU Manager将容器配置信息发送到vGPU Manager Step 6：GPU Manager将容器环境变量，挂载信息和设备信息通过allocateResponse返回给Kubelet Step 7：Kubelet根据allocateResponse创建并且初始化一个容器 Step 8：vGPU Library向vGPU Manager注册自身并且管理其所在容器的GPU资源 Step 9：vGPU Manager持续监控GPU容器状态 \rimage-20220828143547571\r    优化   容器的资源不仅会影响应用程序的性能，而且还会确定应用程序的状态 用户在创建容器时无法准确估算所需的资源 因此，我们提供两种方法来更改运行时容器的资源。弹性资源分配会暂时修改容器的计算资源限制，而动态资源分配永久改变了容器的资源。     弹性分配策略\n 目的是使用闲置的计算资源以提高资源利用率 \rimage-20220828224059056\r nanosleep()是Linux内核函数，会挂起当前线程等待一段时间或者直到接收到调用当前线程的信号（Line 2） 为了防止过载， $GU_{max}$ 默认的最大值是90% （Line 3） 如果物理GPU计算卡仍然有空闲资源，也就是说GU free \u0026gt; 0，即使容器的资源请求已经超出其配额，vGPU Library也会继续给容器分配计算资源（Line 4-5）。如果系统没有剩余的空闲资源（*GU free \u0026lt;= 0*）并且容器的消耗的资源大于其配额，vGPU Library会逐渐收回超额（over-allocated）资源（Line 6-7） 超额资源的回收采用非抢占式策略（non-preemptive strategy），就是说会等到容器中占用线程的核函数执行完后再回收线程资源。CU cores*可以被理解为一种token，容器执行核函数时需要消耗该token，当核函数执行完成释放线程资源时容器又回重新拥有该token。CU cores的初始值等于容器的计算资源配额，CU cores为零时（系统没有空闲的资源，并且该容器的计算资源配额都正在被用于执行核函数），vGPU Library不会再给容器分配任何计算资源，直到容器的CU cores大于零（其他容器释放了空闲资源，或者该容器有核函数完成释放了资源，容器重新获得CU cores*） 弹性分配的样例  \rimage-20220828233026263\r 首先，容器A申请了0.3个GPU，并且被GPU Scheduler调度到了一个完全空闲的GPU上 由于GPU完全空闲，所以容器A会逐渐占用所有的空闲资源，默认最大可占用90% 此时容器B申请了0.7个GPU，也被调度到了此GPU上，但是由于容器A占用了所有的空闲资源，所以需要从容器A回收超额线程资源并分配给容器B 重复经过几次资源的重新分配，容器A和容器B所占用的资源与其资源配额相同      动态分配策略\n 动态资源分配会修改容器资源，包括内存和计算资源，而无需停止容器。 动态资源分配旨在解决两个问题  在硬限制下更新容器的内存和计算资源 在弹性限制下将内存资源添加到容器中   vGPU Library 通过将容器的资源配置与容器的实际使用进行比较来限制容器资源 要永久更改容器的资源：修改容器的资源配置—\u0026gt;通知GPU Scheduler-\u0026gt;更新相应的物理GPU分配    Experiments \u0026hellip;\nConclusion \u0026hellip;\n参考资料   论文地址 https://ieeexplore.ieee.org/abstract/document/8672318\n  Github开源代码 https://github.com/tkestack/gpu-manager\n  ","date":"2022-08-26T17:20:32+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/20220826142857.png","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0gaiagpu%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"【论文笔记】GaiaGPU论文阅读笔记"},{"content":"在ubuntu上k8s集群部署实践  centos 上安装建议看 后端 - CentOS 搭建 K8S，一次性成功，收藏了！_个人文章 - SegmentFault 思否\n 一、机器配置 配置主机名 sudo hostnamectl set-hostname \u0026#34;k8s-master\u0026#34; // Run this command on masternode cat /etc/hostname sudo hostnamectl set-hostname \u0026#34;k8s-node1\u0026#34; // Run this command on node-0 sudo hostnamectl set-hostname \u0026#34;k8s-node2\u0026#34; // Run this command on node-1 配置/etc/hosts sudo vi /etc/hosts 10.1.13.106 k8s-master #10.1.13.107 k8s-node1 #10.1.13.108 k8s-node1 配置免密登录（可以省略） 想让机器 A 访问机器 B，就把机器 A 的公钥放到机器 B 的~/.ssh/authorized_keys 文件里就行了。\n首先我们在master上生成一个密钥，输入下述命令后一路回车即可：\nssh-keygen 然后登录worker，并依次输入下述两条命令将其复制并写入到worker的authorized_keys中，注意我下面的scp命令中使用了worker别名，要提前进行配置：\n# 复制到 worker scp root@k8s-master:~/.ssh/id_rsa.pub /home # 写入到 authorized_keys 中 cat /home/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 然后在master上使用ssh worker登录就可以发现直接连接上而不需要密码了。\n关闭防火墙 sudo systemctl stop firewalld sudo systemctl disable firewalld 禁用swap 这个swap其实可以类比成 windows 上的虚拟内存，它可以让服务器在内存吃满的情况下可以保持低效运行，而不是直接卡死。但是 k8s 的较新版本都要求关闭swap。所以咱们直接动手，修改/etc/fstab文件：\nsudo vi /etc/fstab 你应该可以看到如下内容，把第二条用#注释掉就好了，注意第一条别注释了，不然重启之后系统有可能会报file system read-only错误。\nUUID=e2048966-750b-4795-a9a2-7b477d6681bf / ext4 errors=remount-ro 0 1 # /dev/fd0 /media/floppy0 auto rw,user,noauto,exec,utf8 0 0 然后输入reboot重启即可，重启后使用top命令查看任务管理器，如果看到如下KiB Swap后均为 0 就说明关闭成功了。\n关闭swap之后使用top命令看到 KiB Swap 全部为0\n\rimage-20220613000133107\r\n上面说的是永久关闭swap内存，其实也可以暂时关闭，使用swapoff -a命令即可，效果会在重启后消失。\n禁用Selinux sudo apt install selinux-utils setenforce 0 确保时区和时间正确 sudo timedatectl set-timezone Asia/Shanghai sudo systemctl restart rsyslog sudo apt-get install ntpdate –y sudo ntpdate time.windows.com 配置net.bridge.bridge-nf-call-iptables cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 设置rp_filter的值 sudo vi /etc/sysctl.d/10-network-security.conf # 将下面两个参数的值从2修改为1 # net.ipv4.conf.default.rp_filter=1 # net.ipv4.conf.all.rp_filter=1 sudo sysctl --system 二、安装docker docker 是 k8s 的基础，在安装完成之后也需要修改一些配置来适配 k8s ，所以本章分为 docker 的安装 与 docker 的配置 两部分。如果你已经安装并使用了一段时间的 docker 了话，建议使用docker -v查看已安装的 docker 版本，并在 k8s 官网上查询适合该版本的 k8s 进行安装。这一步两台主机都需要进行安装。\n安装docker sudo apt install docker.io sudo apt install docker.io=18.09.1-0ubuntu1 启动docker sudo systemctl start docker 配置docker sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://n73pm3wf.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [ \u0026#34;native.cgroupdriver=systemd\u0026#34; ] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 设置开机自启动 $ sudo systemctl enable docker 或 $ sudo systemctl enable docker.service --now 验证docker systemctl status docker docker --version docker info | grep Cgroup 修改后的 docker cgroup 状态，发现变为systemd即为修改成功。\n重启docker（当有配置改动后执行） sudo pkill -SIGHUP dockerd sudo systemctl restart docker 卸载docker sudo apt-get remove docker.io 三、安装k8s 安装k8s # 使得 apt 支持 ssl 传输 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https # 下载 gpg 密钥 这个需要root用户否则会报错 curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - # 添加 k8s 镜像源 这个需要root用户否则会报错 cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF # 更新源列表 apt-get update # 下载 kubectl，kubeadm以及 kubelet # 安装最新版本 apt-get install -y kubelet kubeadm kubectl # 安装指定版本 apt-get install -y kubelet=1.15.1-00 kubeadm=1.15.1-00 kubectl=1.15.1-00 删除已安装的k8s sudo apt-get remove -y --allow-change-held-packages kubeadm kubectl kubelet 查询可安装版本 apt-cache madison kubeadm apt-cache madison kubelet apt-cache madison kubectl apt-cache madison kubernetes-cni 设置不随系统更新而更新 sudo apt-mark hold kubelet kubeadm kubectl 四、初始化master kubeadm init kubeadm init \\ --apiserver-advertise-address=10.10.0.2 \\ --kubernetes-version=1.15.1 \\ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 参数解释：\n \u0026ndash;kubernetes-version：指定kubernetes的版本，与上面kubelet，kubeadm，kubectl工具版本保持一致。 \u0026ndash;apiserver-advertise-address：apiserver所在的节点(master)的ip。 \u0026ndash;image-repository=registry.aliyuncs.com/google_containers：由于国外地址无法访问，所以使用阿里云仓库地址 \u0026ndash;server-cidr：service之间访问使用的网络ip段 \u0026ndash;pod-network-cidr：pod之间网络访问使用的网络ip,与下文部署的CNI网络组件yml中保持一致  初始化成功之后你将看到\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.10.0.2:6443 --token juojiv.q4gwfmkj4cwgscnt \\  --discovery-token-ca-cert-hash sha256:381b61262d3b174cfce0e5cfbd0b4b171e270c506d82c4d82334d0a4e2c2ac47 初始化失败 重置之后重新初始化\nkubeadm reset 记得保存kubeadm join 命令 kubeadm join 10.10.0.2:6443 --token qc7drh.47mloh5ierij84xi \\  --discovery-token-ca-cert-hash sha256:90f147ef85ae06d9a46bed91d64109abbd697b6878ab9ca16a376e11816f9a0d 如果忘记可以使用下面的命令重新生成\nkubeadm token create --print-join-command 配置kubectl 按照上图中的命令配置，可以直接从命令行中复制\n 非root用户  mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 查看已加入的节点 kubectl get nodes # 查看集群状态 kubectl get cs  root用户  export KUBECONFIG=/etc/kubernetes/admin.conf 部署网络 网络插件可以选择calico或flannel。这里使用Flannel作为Kubernetes容器网络方案，解决容器跨主机网络通信。\n 部署flannel网络  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml 也可以把这个文件下载到本地，注意修改下图中的Network，与kubeadm inti中的\u0026ndash;pod-network-cidr 10.244.0.0/16参数保持一致，然后\nkubectl apply -f kube-flannel.yml \rimage-20220613013117789\r\n将会看到\npodsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created 查看pod状态，应该全部为running。可能需要等待一段时间\nkubectl get pod --all-namespaces -o wide \rimage-20220613012658569\r\n 部署calico网络  下载yml文件\nwget https://docs.projectcalico.org/v3.11/manifests/calico.yaml vi calico.yaml #修改CALICO_IPV4POOL_CIDR，为10.244.0.0/16（要与kubeadm inti中的\u0026ndash;pod-network-cidr 10.244.0.0/16参数保持一致，默认为192.168.0.0/16\nkubectl apply -f calico.yaml 五、添加worker节点 使用之前保存的kubeadm join命令\nkubeadm join 10.10.0.2:6443 --token ryyy8k.pr8u7cjm4btqdx4v --discovery-token-ca-cert-hash sha256:6b596c66745b162190475b63686ca46c60cc4f39473 然后在master节点上，查看\nkubectl get nodes 可以看到\n\rimage-20220613010009663\r\n六、验证 创建nginx kubectl create deployment nginx-web --image=nginx 获取pod信息 kubectl get pod -o wide \rimage-20220613013920336\r\n可以看到该节点的ip为10.244.1.2\n测试nginx root@k8s-master:/home# curl http://10.244.1.2 可以看到\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 删除pod kubectl delete pod nginx-web-xxxxxx -n nginx-web 七、删除节点 首先，确定想要清空的节点的名称。可以用以下命令列出集群中的所有节点:\nkubectl get nodes 接下来，告诉 Kubernetes 清空节点：\nkubectl drain \u0026lt;node name\u0026gt; 一旦它返回（没有报错）， 你就可以下线此节点（或者等价地，如果在云平台上，删除支持该节点的虚拟机）。 如果要在维护操作期间将节点留在集群中，则需要运行：\nkubectl uncordon \u0026lt;node name\u0026gt; 然后告诉 Kubernetes，它可以继续在此节点上调度新的 Pods。\n","date":"2022-06-13T14:57:42+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/20220823230439.png","permalink":"https://tweakzx.github.io/p/kubernetesubuntu%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","title":"【Kubernetes】ubuntu安装k8s集群"},{"content":"题目描述 请你来实现一个 myAtoi(string s) 函数，使其能将字符串转换成一个 32 位有符号整数（类似 C/C++ 中的 atoi 函数）。\n函数 myAtoi(string s) 的算法如下：\n 读入字符串并丢弃无用的前导空格 检查下一个字符（假设还未到字符末尾）为正还是负号，读取该字符（如果有）。 确定最终结果是负数还是正数。 如果两者都不存在，则假定结果为正。 读入下一个字符，直到到达下一个非数字字符或到达输入的结尾。字符串的其余部分将被忽略。 将前面步骤读入的这些数字转换为整数（即，\u0026ldquo;123\u0026rdquo; -\u0026gt; 123， \u0026ldquo;0032\u0026rdquo; -\u0026gt; 32）。如果没有读入数字，则整数为 0 。必要时更改符号（从步骤 2 开始）。 如果整数数超过 32 位有符号整数范围 [−231, 231 − 1] ，需要截断这个整数，使其保持在这个范围内。具体来说，小于 −231 的整数应该被固定为 −231 ，大于 231 − 1 的整数应该被固定为 231 − 1 。 返回整数作为最终结果。  注意：\n  本题中的空白字符只包括空格字符 ' ' 。\n  除前导空格或数字后的其余字符串外，请勿忽略 任何其他字符。\n  示例 1：\n 输入：s = \u0026ldquo;42\u0026rdquo; 输出：42 解释：加粗的字符串为已经读入的字符，插入符号是当前读取的字符。 第 1 步：\u0026ldquo;42\u0026rdquo;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026ldquo;42\u0026rdquo;（当前没有读入字符，因为这里不存在 \u0026lsquo;-\u0026rsquo; 或者 \u0026lsquo;+'） ^ 第 3 步：\u0026ldquo;42\u0026rdquo;（读入 \u0026ldquo;42\u0026rdquo;） ^ 解析得到整数 42 。 由于 \u0026ldquo;42\u0026rdquo; 在范围 [-231, 231 - 1] 内，最终结果为 42 。\n 示例 2：\n 输入：s = \u0026quot; -42\u0026quot; 输出：-42 解释： 第 1 步：\u0026quot; -42\u0026quot;（读入前导空格，但忽视掉） ^ 第 2 步：\u0026quot; -42\u0026quot;（读入 \u0026lsquo;-\u0026rsquo; 字符，所以结果应该是负数） ^ 第 3 步：\u0026quot; -42\u0026quot;（读入 \u0026ldquo;42\u0026rdquo;） ^ 解析得到整数 -42 。 由于 \u0026ldquo;-42\u0026rdquo; 在范围 [-231, 231 - 1] 内，最终结果为 -42 。\n 示例 3：\n 输入：s = \u0026ldquo;4193 with words\u0026rdquo; 输出：4193 解释： 第 1 步：\u0026ldquo;4193 with words\u0026rdquo;（当前没有读入字符，因为没有前导空格） ^ 第 2 步：\u0026ldquo;4193 with words\u0026rdquo;（当前没有读入字符，因为这里不存在 \u0026lsquo;-\u0026rsquo; 或者 \u0026lsquo;+'） ^ 第 3 步：\u0026ldquo;4193 with words\u0026rdquo;（读入 \u0026ldquo;4193\u0026rdquo;；由于下一个字符不是一个数字，所以读入停止） ^ 解析得到整数 4193 。 由于 \u0026ldquo;4193\u0026rdquo; 在范围 [-231, 231 - 1] 内，最终结果为 4193 。\n 提示：\n 0 \u0026lt;= s.length \u0026lt;= 200 s 由英文字母（大写和小写）、数字（0-9）、\u0026rsquo; \u0026lsquo;、'+'、'-\u0026rsquo; 和 \u0026lsquo;.\u0026rsquo; 组成  来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/string-to-integer-atoi\n题解 大家可以自行去看leetcode的官方题解，就是用一个自动机来解决这个问题。我第一次用Go来解决这个问题，所以决定记录下来\ntype Automaton struct{ state string sign int ans int table map[string][]string } func(a *Automaton) init(){ //因为Go不能设置类型变量的初始值（我没找到相关代码），所以我用了初始化函数来代替  a.state = \u0026#34;start\u0026#34; a.sign = 1 a.ans = 0 a.table = map[string][]string{ \u0026#34;start\u0026#34; : []string{\u0026#34;start\u0026#34;,\u0026#34;signed\u0026#34;,\u0026#34;in_number\u0026#34;,\u0026#34;end\u0026#34;}, \u0026#34;signed\u0026#34; : []string{\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;in_number\u0026#34;,\u0026#34;end\u0026#34;}, \u0026#34;in_number\u0026#34; : []string{\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;in_number\u0026#34;,\u0026#34;end\u0026#34;}, \u0026#34;end\u0026#34; : []string{\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;,\u0026#34;end\u0026#34;}, } } func(a *Automaton) get(c byte){ //实现状态转移的方法  a.state = a.table[a.state][a.get_col(c)] switch a.state{ case \u0026#34;in_number\u0026#34;: a.ans = a.ans*10 + int(c-\u0026#39;0\u0026#39;) if a.sign\u0026gt;0{ a.ans = min(a.ans, math.MaxInt32) }else{ a.ans = min(a.ans, -math.MinInt32) } case \u0026#34;signed\u0026#34;: if c == \u0026#39;-\u0026#39;{ a.sign = -1 } } } func(a *Automaton) get_col(c byte ) int{//获得输入字符的类型  switch { case c==\u0026#39; \u0026#39;: return 0 case c==\u0026#39;+\u0026#39;||c==\u0026#39;-\u0026#39;: return 1 case \u0026#39;0\u0026#39;\u0026lt;=c\u0026amp;\u0026amp;c\u0026lt;=\u0026#39;9\u0026#39;: return 2 default: return 3 } } func(a *Automaton) get_ans() int{ //获得结果  return a.sign * a.ans } func myAtoi(s string) int { a := \u0026amp;Automaton{} a.init() for i:=0; i\u0026lt;len(s); i++{ a.get(s[i]) } return a.get_ans() } func min(a,b int) int{ if a\u0026lt;b{ return a } return b } ","date":"2022-03-12T16:59:39+08:00","permalink":"https://tweakzx.github.io/p/golang%E8%87%AA%E5%8A%A8%E6%9C%BA%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"【Golang】自动机的实现"},{"content":"旅行起点 Go 语言之旅 (go-zh.org)\n上方链接是一个Go语言学习的Playground，快点击它，开启一场Go语言之旅吧\n旅行开始 练习：循环与函数 为了练习函数与循环，我们来实现一个平方根函数：用牛顿法实现平方根函数。\n计算机通常使用循环来计算 x 的平方根。从某个猜测的值 z 开始，我们可以根据 z² 与 x 的近似度来调整 z，产生一个更好的猜测：\nz -= (z*z - x) / (2*z)\r重复调整的过程，猜测的结果会越来越精确，得到的答案也会尽可能接近实际的平方根。\n在提供的 func Sqrt 中实现它。无论输入是什么，对 z 的一个恰当的猜测为 1。 要开始，请重复计算 10 次并随之打印每次的 z 值。观察对于不同的值 x（1、2、3 \u0026hellip;）， 你得到的答案是如何逼近结果的，猜测提升的速度有多快。\n提示：用类型转换或浮点数语法来声明并初始化一个浮点数值：\nz := 1.0\rz := float64(1)\r然后，修改循环条件，使得当值停止改变（或改变非常小）的时候退出循环。观察迭代次数大于还是小于 10。 尝试改变 z 的初始猜测，如 x 或 x/2。你的函数结果与标准库中的 math.Sqrt 接近吗？\n（注： 如果你对该算法的细节感兴趣，上面的 z² − x 是 z² 到它所要到达的值（即 x）的距离， 除以的 2z 为 z² 的导数，我们通过 z² 的变化速度来改变 z 的调整量。 这种通用方法叫做牛顿法。 它对很多函数，特别是平方根而言非常有效。）\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func Sqrt(x float64) (z float64) { z = float64(1) for math.Abs(z*z-x)\u0026gt;0.000001 { z -= (z*z-x)/(z*2) } return } func main() { fmt.Println(Sqrt(2)) } 练习：切片 实现 Pic。它应当返回一个长度为 dy 的切片，其中每个元素是一个长度为 dx，元素类型为 uint8 的切片。当你运行此程序时，它会将每个整数解释为灰度值（好吧，其实是蓝度值）并显示它所对应的图像。\n图像的选择由你来定。几个有趣的函数包括 (x+y)/2, x*y, x^y, x*log(y) 和 x%(y+1)。\n（提示：需要使用循环来分配 [][]uint8 中的每个 []uint8；请使用 uint8(intValue) 在类型之间转换；你可能会用到 math 包中的函数。）\npackage main import \u0026#34;golang.org/x/tour/pic\u0026#34; func Pic(dx, dy int) [][]uint8 { picture := make([][]uint8,dy) for x:=range picture{ line := make([]uint8,dx) for y:=range line{ line[y] = uint8((x+y)/2) } picture[x] = line } return picture } func main() { pic.Show(Pic) } 练习：映射 实现 WordCount。它应当返回一个映射，其中包含字符串 s 中每个“单词”的个数。函数 wc.Test 会对此函数执行一系列测试用例，并输出成功还是失败。\n你会发现 strings.Fields 很有帮助。\npackage main import ( \u0026#34;golang.org/x/tour/wc\u0026#34; \u0026#34;strings\u0026#34; ) func WordCount(s string) map[string]int { words := strings.Fields(s) ans := make(map[string]int) for _,w :=range words{ //v,ok := ans[w] \tans[w] = ans[w] + 1 } return ans } func main() { wc.Test(WordCount) } 练习：斐波纳契闭包 让我们用函数做些好玩的事情。\n实现一个 fibonacci 函数，它返回一个函数（闭包），该闭包返回一个斐波纳契数列 (0, 1, 1, 2, 3, 5, ...)。\npackage main import \u0026#34;fmt\u0026#34; // 返回一个“返回int的函数” func fibonacci() func() int { first := 2 second := 1 //根据公式倒推出的first和second \treturn func() int{ first = second - first second = second + first //斐波那契公式 \treturn second } } func main() { f := fibonacci() for i := 0; i \u0026lt; 10; i++ { fmt.Println(f()) } } 练习：Stringer 通过让 IPAddr 类型实现 fmt.Stringer 来打印点号分隔的地址。\n例如，IPAddr{1, 2, 3, 4} 应当打印为 \u0026quot;1.2.3.4\u0026quot;。\npackage main import \u0026#34;fmt\u0026#34; type IPAddr [4]byte // TODO: 给 IPAddr 添加一个 \u0026#34;String() string\u0026#34; 方法 func (ip IPAddr) String() string{ return fmt.Sprintf(\u0026#34;%v.%v.%v.%v\\n\u0026#34;,ip[0],ip[1],ip[2],ip[3]) } func main() { hosts := map[string]IPAddr{ \u0026#34;loopback\u0026#34;: {127, 0, 0, 1}, \u0026#34;googleDNS\u0026#34;: {8, 8, 8, 8}, } for name, ip := range hosts { fmt.Printf(\u0026#34;%v: %v\\n\u0026#34;, name, ip) } } 练习：错误 从之前的练习中复制 Sqrt 函数，修改它使其返回 error 值。\nSqrt 接受到一个负数时，应当返回一个非 nil 的错误值。复数同样也不被支持。\n创建一个新的类型\ntype ErrNegativeSqrt float64\r并为其实现\nfunc (e ErrNegativeSqrt) Error() string\r方法使其拥有 error 值，通过 ErrNegativeSqrt(-2).Error() 调用该方法应返回 \u0026quot;cannot Sqrt negative number: -2\u0026quot;。\n注意: 在 Error 方法内调用 fmt.Sprint(e) 会让程序陷入死循环。可以通过先转换 e 来避免这个问题：fmt.Sprint(float64(e))。这是为什么呢？\n修改 Sqrt 函数，使其接受一个负数时，返回 ErrNegativeSqrt 值。\npackage main import ( \u0026#34;fmt\u0026#34; ) type ErrNegativeSqrt float64 func (e ErrNegativeSqrt) Error() string{ return fmt.Sprintf(\u0026#34;cannot Sqrt negative number: %v\u0026#34;,float64(e)) } func Sqrt(x float64) (float64, error) { if x\u0026gt;0{ return 0, nil }else{ var e ErrNegativeSqrt e = ErrNegativeSqrt(x) return x,e } } func main() { fmt.Println(Sqrt(2)) fmt.Println(Sqrt(-2)) } 练习：Reader 实现一个 Reader 类型，它产生一个 ASCII 字符 'A' 的无限流。\npackage main import \u0026#34;golang.org/x/tour/reader\u0026#34; type MyReader struct{} // TODO: 给 MyReader 添加一个 Read([]byte) (int, error) 方法 func (mr MyReader) Read(buf []byte) (int, error) { for i :=range buf{ buf[i] = \u0026#39;A\u0026#39; } return 1, nil } func main() { reader.Validate(MyReader{}) } 练习：rot13Reader 有种常见的模式是一个 io.Reader 包装另一个 io.Reader，然后通过某种方式修改其数据流。\n例如，gzip.NewReader 函数接受一个 io.Reader（已压缩的数据流）并返回一个同样实现了 io.Reader 的 *gzip.Reader（解压后的数据流）。\n编写一个实现了 io.Reader 并从另一个 io.Reader 中读取数据的 rot13Reader，通过应用 rot13 代换密码对数据流进行修改。\nrot13Reader 类型已经提供。实现 Read 方法以满足 io.Reader。\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) type rot13Reader struct { r io.Reader } func ( rot rot13Reader) Read(buf []byte) (int, error){ len,ok := rot.r.Read(buf) for i,v := range buf{ switch{ case (\u0026#39;a\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;m\u0026#39;)||(\u0026#39;A\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;M\u0026#39;): buf[i] = v+13 case (\u0026#39;n\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;z\u0026#39;)||(\u0026#39;N\u0026#39;\u0026lt;=v \u0026amp;\u0026amp; v\u0026lt;=\u0026#39;Z\u0026#39;): buf[i] = v-13 default: } } return len,ok } func main() { s := strings.NewReader(\u0026#34;Lbh penpxrq gur pbqr!\u0026#34;) r := rot13Reader{s} io.Copy(os.Stdout, \u0026amp;r) } 练习：图像 还记得之前编写的图片生成器 吗？我们再来编写另外一个，不过这次它将会返回一个 image.Image 的实现而非一个数据切片。\n定义你自己的 Image 类型，实现必要的方法并调用 pic.ShowImage。\nBounds 应当返回一个 image.Rectangle ，例如 image.Rect(0, 0, w, h)。\nColorModel 应当返回 color.RGBAModel。\nAt 应当返回一个颜色。上一个图片生成器的值 v 对应于此次的 color.RGBA{v, v, 255, 255}。\npackage main import ( \u0026#34;golang.org/x/tour/pic\u0026#34; \u0026#34;image\u0026#34; \u0026#34;image/color\u0026#34; ) type Image struct{ w,h int pixels [][]uint8 } func (self Image) Bounds()(image.Rectangle){ return image.Rect(0, 0, self.w, self.h) } func (self Image) ColorModel()(color.Model){ return color.RGBAModel } func (self Image) At(x int ,y int)(color.Color){ v := self.pixels[y][x] return color.RGBA{v,v, 255, 255} } func Pic(dx, dy int) [][]uint8 { img := make([][]uint8, dy) for y := 0; y \u0026lt; dy; y++ { img[y] = make([]uint8, dx) for x := 0; x \u0026lt; dx; x++ { img[y][x] = (uint8)(x^y) } } return img } func main() { m := Image{256,256,Pic(256,256)} pic.ShowImage(m) } 练习：等价二叉查找树 1. 实现 Walk 函数。\n2. 测试 Walk 函数。\n函数 tree.New(k) 用于构造一个随机结构的已排序二叉查找树，它保存了值 k, 2k, 3k, \u0026hellip;, 10k。\n创建一个新的信道 ch 并且对其进行步进：\ngo Walk(tree.New(1), ch)\r然后从信道中读取并打印 10 个值。应当是数字 1, 2, 3, ..., 10。\n3. 用 Walk 实现 Same 函数来检测 t1 和 t2 是否存储了相同的值。\n4. 测试 Same 函数。\nSame(tree.New(1), tree.New(1)) 应当返回 true，而 Same(tree.New(1), tree.New(2)) 应当返回 false。\nTree 的文档可在这里找到。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;golang.org/x/tour/tree\u0026#34; ) // Walk 步进 tree t 将所有的值从 tree 发送到 channel ch。 func Walk(t *tree.Tree, ch chan int) { dfs(t, ch) close(ch) } func dfs(t *tree.Tree, ch chan int) { if t == nil { return } dfs(t.Left, ch) ch \u0026lt;- t.Value dfs(t.Right, ch) } // Same 检测树 t1 和 t2 是否含有相同的值。 func Same(t1, t2 *tree.Tree) bool { ch1, ch2 := make(chan int), make(chan int) go Walk(t1, ch1) go Walk(t2, ch2) for i := range ch1 { // ch1 关闭后 for循环自动跳出 \tif i != \u0026lt;-ch2 { return false } } return true } func main() { fmt.Println(Same(tree.New(1), tree.New(1))) } 练习：Web 爬虫 在这个练习中，我们将会使用 Go 的并发特性来并行化一个 Web 爬虫。\n修改 Crawl 函数来并行地抓取 URL，并且保证不重复。\n提示：你可以用一个 map 来缓存已经获取的 URL，但是要注意 map 本身并不是并发安全的！\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) type Fetcher interface { // Fetch 返回 URL 的 body 内容，并且将在这个页面上找到的 URL 放到一个 slice 中。 \tFetch(url string) (body string, urls []string, err error) } // Crawl 使用 fetcher 从某个 URL 开始递归的爬取页面，直到达到最大深度。  type CrawlRecord struct{ m map[string]int mux sync.Mutex wg sync.WaitGroup } var\tcr = CrawlRecord{m: make(map[string]int)} func Crawl(url string, depth int, fetcher Fetcher) { defer cr.wg.Done() // TODO: 并行的抓取 URL。 \t// TODO: 不重复抓取页面。  // 下面并没有实现上面两种情况： \tif depth \u0026lt;= 0 { return } cr.mux.Lock() cr.m[url]++ cr.mux.Unlock() body, urls, err := fetcher.Fetch(url) if err != nil { fmt.Println(err) return } fmt.Printf(\u0026#34;found: %s %q\\n\u0026#34;, url, body) for _, u := range urls { cr.mux.Lock() if _,ok := cr.m[u]; !ok{ cr.wg.Add(1) go Crawl(u, depth-1, fetcher) } cr.mux.Unlock() } return } func main() { cr.wg.Add(1) Crawl(\u0026#34;https://golang.org/\u0026#34;, 4, fetcher) cr.wg.Wait() } // fakeFetcher 是返回若干结果的 Fetcher。 type fakeFetcher map[string]*fakeResult type fakeResult struct { body string urls []string } func (f fakeFetcher) Fetch(url string) (string, []string, error) { if res, ok := f[url]; ok { return res.body, res.urls, nil } return \u0026#34;\u0026#34;, nil, fmt.Errorf(\u0026#34;not found: %s\u0026#34;, url) } // fetcher 是填充后的 fakeFetcher。 var fetcher = fakeFetcher{ \u0026#34;https://golang.org/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;The Go Programming Language\u0026#34;, []string{ \u0026#34;https://golang.org/pkg/\u0026#34;, \u0026#34;https://golang.org/cmd/\u0026#34;, }, }, \u0026#34;https://golang.org/pkg/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;Packages\u0026#34;, []string{ \u0026#34;https://golang.org/\u0026#34;, \u0026#34;https://golang.org/cmd/\u0026#34;, \u0026#34;https://golang.org/pkg/fmt/\u0026#34;, \u0026#34;https://golang.org/pkg/os/\u0026#34;, }, }, \u0026#34;https://golang.org/pkg/fmt/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;Package fmt\u0026#34;, []string{ \u0026#34;https://golang.org/\u0026#34;, \u0026#34;https://golang.org/pkg/\u0026#34;, }, }, \u0026#34;https://golang.org/pkg/os/\u0026#34;: \u0026amp;fakeResult{ \u0026#34;Package os\u0026#34;, []string{ \u0026#34;https://golang.org/\u0026#34;, \u0026#34;https://golang.org/pkg/\u0026#34;, }, }, } 旅行终点 你可以从安装 Go 开始。\nwget -c https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz -O - | sudo tar -xz -C /usr/local\rvim /etc/profile\rexport PATH=$PATH:/usr/local/go/bin\rexport GOPROXY=https://goproxy.cn,direct\rexport GO111MODULE=on\rsource /etc/profile\rgo version\r一旦安装了 Go，Go 文档是一个极好的 应当继续阅读的内容。 它包含了参考、指南、视频等等更多资料。\n了解如何组织 Go 代码并在其上工作，参阅此视频，或者阅读如何编写 Go 代码。\n如果你需要标准库方面的帮助，请参考包手册。如果是语言本身的帮助，阅读语言规范是件令人愉快的事情。\n进一步探索 Go 的并发模型，参阅 Go 并发模型(幻灯片)以及深入 Go 并发模型(幻灯片)并阅读通过通信共享内存的代码之旅。\n想要开始编写 Web 应用，请参阅一个简单的编程环境(幻灯片)并阅读编写 Web 应用的指南。\n函数：Go 中的一等公民展示了有趣的函数类型。\nGo 博客有着众多关于 Go 的文章和信息。\nmikespook 的博客中有大量中文的关于 Go 的文章和翻译。\n开源电子书 Go Web 编程和 Go 入门指南能够帮助你更加深入的了解和学习 Go 语言。\n访问 go-zh.org 了解更多内容。\n","date":"2022-02-22T17:20:32+08:00","image":"https://go.dev/images/gophers/biplane.svg","permalink":"https://tweakzx.github.io/p/golanggo%E8%AF%AD%E8%A8%80%E4%B9%8B%E6%97%85/","title":"【Golang】Go语言之旅"},{"content":"PolarDB Serverless论文阅读报告 摘要 数据库管理系统的上云是近期很火的研究趋势，因为这样可以获得更高的弹性，可用性以及更低的成本，传统的独块的数据库架构很难满足这样的要求。高速网络与新的内存技术（例如RDMA）的发展，给分散式数据库带来了可能：它将原先的独块的服务器资源分离解耦到资源池中，再通过高速网络连接起来。下一代的云原生数据库就是为了分散化的数据中心而设计。\nPolarDB Serverless 遵循分散式的设计范式而设计，解耦了计算节点的CPU，内存，存储资源。\n 每种资源可以随需求而独立的增长或缩小，保障可靠性的同时，可以进行多维度的按需供应。 同时采用了优化策略如乐观锁和预取策略来改善性能。 还可以实现更快的故障恢复。  介绍 使用云数据库的三个好处：  按需付费可以使得用户减小开支。 更高的资源弹性可以应对短暂的资源使用高峰期。 更快的升级与更快的错误修复。快速的升级迭代可以保证产品竞争力，错误修复可以在不影响产品可用性的前提下进行。  经典的云数据库的架构  monolithic machine 独块的机器  特点：所有的资源都是紧耦合的 问题：  在进行数据库实例到机器的分发过程中要解决一个装箱问题 总是有碎片化的资源，难以达到高的使用率 运行时不能根据负载调整资源 资源间的命运共享，一个资源的故障会导致其他资源的故障，不能独立透明地修复故障，导致修复时间很长     存算分离的架构：  两种：  virtual machine with remote disk 搭载远程硬盘的虚拟机 shared storage 共享存储   优点  DBaaS可以提高存储池的使用率 共享存储可以进一步减少成本：原数据与只读备份可以共享存储   问题  CPU和内存的装箱问题依旧存在 缺乏灵活可放缩的内存资源 每个只读备份在内存中都要有冗余的内存开销      本文提出了一种新架构，分散架构（disaggragation architecture)  运行在分散的数据中心（DDC），CPU、内存和存储解耦，资源间通过高速网络连接 效果  每一种资源都可以提高其利用率，可以独立地放缩其资源量 解决了资源间的命运共享问题，资源故障可以被独立修复 数据页可以在远程内存池中被共享，所以解决了备份的内存冗余问题    云原生数据库 多数的云数据库是基于共享存储的架构，内存和CPU绑成最小的资源单元，只能按照最小资源单元的粒度来增长和释放资源，这会带来很多的资源浪费。\nPolarDB Serverless则是遵循分散架构的一个云原生数据库\n  引入了多租户可放缩的内存池，可以进行内存页的分配与生命周期管理\n  节点组成：\n 一个 RW node(存原数据页节点) 多个 RO node（存只读备份节点）    面临的挑战\n 添加远程内存之后，系统可以正确地处理事务  当写后读时，系统不应该错过任何一个update。使用缓存失效机制来保证写后所有地节点会更新 在更新B+树地索引的时候，使用global latch全局页锁来保证RO节点不能看到不一致的索引结构 使用读视图来确保RO节点不会读到未提交的事务   高效地执行事务  广泛使用了RDMA操作，尤其是one-sided RDMA verbs 为了提高并发，使用了乐观锁 存储方面使用了page materialization offloading技术，使用redo日志生成page 利用了预取来提高本地命中率   构建可信系统  对不同的节点都设计了策略来处理单点故障 因为内存和存储的状态是解耦的，所以修复崩溃的速度比独块架构要快5.3倍      背景 PolarDB PolarDB是一个基于共享内存架构云原生数据库\n PolarFS是一个持久化的原子操作的可伸缩的分布式共享存储。是统一的存储资源池，其提供虚拟的volume，每个volume划分为10GB大小的chunk分布在不同的节点。每个volume最多10000个chunk，即100T的容量。每个chunk三副本，用parallel raft提供线性一致性。 RW和RO节点之间通过redo日志来同步内存状态，通过LSN（log sequence number）来协调一致性。RW和RO节点上有负责处理SQL语句的处理器和事务引擎（InnoDB，X-Engine），以及一个缓存池来服务查询与事务。 有多个无状态的代理节点负责透明的负载均衡  \rimage-20211222201006368\r\n分散化的数据中心DDC   连接技术\n  在分散化的数据中心，计算节点、内存结点以及存储节点都是由高速网络连接。\n  RDMA（Remote Direct Memory Access）技术给大型的DDC带来了可能\n    层级结构\n 分为三层：spine layer、leaf layer、ToR layer 每一个ToR节点链接48台主机，ToR的交换机分别连接Leaf节点的交换机，然后Leaf的交换机又去链接Spine层的交换机。 每台主机都会配有双端口的RDMA网卡，用于链接两个ToR节点来避免单点故障。 一个leaf 交换机组包含互为备份的同时工作的多个leaf交换机 由一个leaf交换机组管理控制的所有的交换机与服务器称为一个PoD（Point of Delivery）。一个PoD最多有32个leaf交换机。    资源部署方式\n 单个数据库实例所需的内存和存储会部署在一个PoD下 不同实例部署在不同的PoD下 计算和内存资源总是倾向于部署在同一个ToR下，以获得更低的延迟与更少的页抖动    \rimage-20211222201043429\r\n无服务数据库 无服务数据库是云原生数据库的高弹性变种，主要目的是为了实现资源的按需分配\n 自动扩缩 auto-scaling  现存的无服务数据库的扩缩容因为是基于共享存储的架构，所以扩缩容受到限制。由于内存和CPU资源总是深度绑定在一个资源单位上，扩缩容也只能按照资源单位作为调动粒度。所以CPU和内存资源总是不能得到充分利用。 分析性数据库对内存的要求比较高，只需要少量的CPU资源来定期同步更新数据 事务性数据库少量的内存就可以保证缓存命中率，但需要更多的CPU资源来应对访问的高峰时刻   自动暂停 auto-pause  现存的无服务数据库的自动暂停也是受限的，CPU和内存资源必须被同时释放 在分散式架构下，CPU和内存不再共享命运。业务低峰期，内存可以不必释放，避免了重新加载。   扩容透明性 scaling transparency  透明性即在扩缩容的时候需要保证客户的场景不能中断或者性能出现严重影响 分散式架构下，中间临时状态如脏页、事务状态如版本信息timestamp、逻辑锁、query中间结果都可以保存在共享内存层，给实现扩缩容的透明性提供了更好的条件。 PolarDB Serverless 目前将脏页存在共享内存里，其他的临时状态期待后续的工作。    设计 PolarDB Serverless 基于PolarDB开发的分散式架构的云原生数据库。\n 组成：多个代理节点，一个RW节点，多个RO节点 使用PolarFS来做共享存储 和PolarDB的最大不同在于使用了远程内存池（共享内存）  使用共享内存\n 好处  RW和RO共用数据页，省去了每个节点自己保存副本，提高了内存的使用效率   坏处（performance penalty）  远程内存的访问速度远远低于本地内存；解决方案：分层内存系统和预取技术 私有数据放在公共资源上，需要有跨节点的互斥保护；解决方案：广泛使用单边的RDMA谓词和乐观协议来避免使用全局锁（global latch） 页的传输和网络带来了负担。解决方案：先将redo log写到存储层，再异步地通过日志将页物质化    分散化内存 远程内存访问接口  page_register: 页的引用计数增一 page_unregister: 页的引用计数减一 page_read: 使用单边RDMA谓词将页从远程内存读取到本地 page_write: 使用单边RDMA谓词将页从本地写到远程内存 page_invalidation: 使所有RO节点的本地的内存副本失效  远程内存管理 内存的分配单元是一个slab，一个slab的大小是1GB\n几种数据结构如下：\n Page Array（PA）  每一个slab由页数组组成，页数组是物理地址连续的由16KB的页组成的数组。 PA的地址会注册到RDMA的网卡上，所以可以被RDMA远程访问 负责提供slab访问的节点也叫slab node，第一个slab node 也叫home node   Page Address Table（PAT）  一个哈希表，存储每一页的位置（slab id 与物理地址）   Page Invalidation Bitmap (PIB)  一个位图，对应PAT的每一项记录invalidation bit，0表示内存中的是最新版本，1表示不是最新版本   Page Reference Directory(PRD)  一个map，对应于每个PAT项，记录引用了这个页的节点   Page Latch Table(PLT)  对于PAT中的每一项，管理其页锁（page latch）。 是一个全局的物理锁，用于多个数据库节点之间进行保护与同步读写 尤其用于保护B+树的结构一致性    page分配过程：\n 数据库向home node 发送page register请求 如果不存在，则遍历slab找到有空闲的slab，如果都没有空闲则使用LRU算法淘汰掉一些page 写入后，将page的为位置信息写入到PAT，返回page的远程地址和page latch  扩缩容：\n 扩容：home node 请求DBaaS分配新的slab，扩展buffer pool，PAT，PIB和PRD 缩容：page通过LRU进行淘汰，无用的slab将被回收  本地缓存  以page为单位将远程内存缓存到本地 如果page不在远程内存上，进程会从Polar FS读取内容到本地缓存，然后再写回到远程内存。（存储和内存不发生直接接触） 不是所有的page都要写入远程内存，例如全表扫描，这些page不太会再次访问 本地发生miss，进程要等待读取远程内存（可以利用预取技术） 本地的缓存写满后使用LRU算法进行淘汰。如果page没有修改过可以直接释放。如果page修改过，那就写回后再释放。引用计数减一。  缓存一致性  RO节点不能直接访问RW节点的本地缓存，只能通过获取最新的redo log来在本地物质化page来获取最新的数据。 如果RW节点将修改过的数据写回远程内存后，RO节点自然是不需要读取redo log，可是系统如果每次修改都要立刻写回，网络开销会很大。所以RW节点不会立刻写回，而是使用了页失效的方法。  \rimage-20211225120332727\r\n 具体过程如下：  RW修改page后，调用page_invalidation 发送请求到home node PIB中对应页项设置为1 查询PRD，查看page在哪些RO节点上存在 向所有的存在这个page的RO发送请求，将PIB设置为1 设置PIB过期是一个同步阻塞操作 只有全部的RO节点设置成功才返回成功 如果有个RO节点超时，那么就把他踢出集群来保证该操作成功   在写回到Polar FS（远程存储）的时候，同样会使用page_invalid来保证远程内存中不会存在比远程存储中更老版本的数据。  B+树结构一致性   物理一致性\n问题是：多线程访问B+树的Index的时候如何做到并发控制\n解决方案：\n  只有RW节点可以修改page，所以不需要管理多节点的写冲突。但是SMO（结构修改操作）会同时修改多个page，其他节点在遍历B+树的时候，不同的RO节点可能看到不一致的物理B+树结构。\n 我们使用全局页锁来解决这个问题，使用共享锁（S）和排他锁（X）。 区别于本地页锁，全局页锁用于保证多节点下的B+树索引结构一致性。使用crabbhing/lock coupling算法实现 所有参与SMO的节点会加上X锁，直到SMO结束。RO节点读取的时候要检查PLT，查看是否有X锁，并且向被读的页上加S锁。    对于RW节点要进行的insert和delete操作，我们采取两阶段做法\n 采用乐观并发控制，假设没有SMO操作，那就只需要本地的锁来作单节点并发控制。如果确实不需要进行SMO，那就顺利插入或者删除。 如果发现B+树的叶子节点相对空或者相对满，即很有可能要发生SMO操作，就启用悲观策略，这时候会对所有可能参与SMO的page加上X锁，直到SMO结束释放锁。这样锁的排序可以保证RO节点只能看到SMO之前或者之后的树结构，而不会是SMO的中间状态。      逻辑一致性\n 相同的数据可能会有多个事务进行并发处理，如何保证满足不同的快照隔离级别。    快照隔离 PolarDB Serverless 提供基于MVCC的快照隔离。事务的实现机制，是由快照的时间戳来控制事务能看到数据的版本信息。RW节点会维护一个中心化的的时间戳叫CTS，来为所有数据库节点分配全局单调递增的时间戳。\n一次读写事务需要获取两次时间戳（cts_read和cts_commit）；提交事务的时候所有记录和undo log中的记录都需要额外维护一个列来保存cts_commit;一个读写事务内的read总会返回一个cts_commit比cts_read要小的记录;每个记录都会有一个事务id字段来记录修改该记录的事务。\n一个只读事务则只需要在事务开始的时候获取cts_read时间戳即可。\n但是对于一些比较heavy的事务，无法迅速对所有记录更新cts_commit，因为这会在提交事务的时候带来大量的随机写。因此这个cts_commit的更新是异步执行的。这就导致了在并发事务处理的时候，无法得知该记录的cts_commit是否已经被写或者是否要被写，即无法得知先前的事务是否已经完成或者该记录是否与先前事务有关。\n解决方案：通过查询RW上的CTS Log数据结构，其是一个环形数组：记录着最近若干个的读写事务的提交时间戳(cts_commit)。如果先前事务没有完成，这个数组上的commit timestamp就会为null，每当一个节点读取到一个cts_commit为空的记录，就可以去查询这个全局的环形数组来得知这个记录对应的事务是否已经提交了。\n同样要去全局查这个时间戳要对应一次network request，本系统还是采用RDMA 来加速环形数组的访问获取时间戳的过程，RDMA CAS技术能够原子递增获取时间戳，并且这个数组的地址也会被注册到RDMA 的网卡上。和用RPC相比，直接走网卡不需要占用RW节点的CPU资源。\nPage Materialization Offloading 传统的DB会周期性的将脏页写入到持久性的存储中。Aurora提出了日志即数据库的概念，通过使用redo log来物质化页数据来获取最新版本的数据。Socrates在此基础上，则做到了将log与数据分离存储。\nPolarrDB Serverless和Socrates类似，将logs和pages分开存储在不同的chunks中。\n redo logs先存储在logs chunks中 异步发送到page chunks 为了PolarFS组件的重用和最小化更改，logs仅仅送到leader node。 然后leader node 重做并使用ParallelRaft保证副本的一致性  自动扩缩 在版本的升级和重启过程中，用户对Serverless服务是无感知的，断开连接，事务中断，请求超时是不允许的。\n所以在本系统中，当发生版本升级和跨节点迁移的时候，代理节点负责保持客户端连接，发送请求后会等待旧的RW节点处理掉正在进行的事务。之后把就节点的脏页flush到共享的内存池中，再关闭就RW节点。新的RW节点连接内存池，预热缓存，重做undo log来构建事务的列表。之后代理节点将连接还给新的RW节点。\n性能优化 使用分散式架构会带来性能的损失，所以要使用一些性能优化手段来进行优化。\n乐观锁 使用乐观锁可以尽可能避免并发操作对全局锁的获取，进而提高并行效率。\nRW节点会维护一个SMO counter，每次发生SMO的时候counter++；并且所有被修改的Page也会维护这个更新后的counter（SMO RW）\n每次query执行的时候去拿这个SMO counter（SMO query），一旦RO上的query发现某个Page的SMO counter比SMO query还要大，说明在query执行过程中发生了SMO。这个时候就要回滚到悲观并发控制，即获取全局PL来锁住整个B+ Tree的SMO更新。\n预取 在PolarDB Serverless中，我们提出了批处理密钥预处理(BKP)。BKP从分解的内存和存储中预取包含有趣的元组的页面，以隐藏远程I/O延迟。BKP的接口接受一组要预取的密钥。当调用该接口时，引擎将启动一个后台预取任务，从目标索引中检索所需的密钥，并在必要时从远程内存/存储中获取相应的数据页。BKP还可以优化分析工作负载。\n容错与恢复策略 数据库节点恢复 本系统采用的是ARIES的恢复算法（Algorithm for Recovery and Isolation Exploiting Semantics. ）\n RO节点的恢复：由于页数据再远程内存上，所以可以轻松地使用新的RO节点代替旧的RO节点 RW节点的恢复  无预期的节点故障  RW节点挂掉之后，集群的manager（CM）会通过心跳信号探测到，然后RO节点就可以晋升为RW节点。   有预期的节点故障  例如版本升级，前文已经讲过，不再赘述。      内存结点恢复 内存节点的数据缓存Page，在把dirty page写到内存节点前，对应的redo log已经flush到存储层了，因此内存节点重启可以用PolarFS上的redolog来进行恢复数据。\nhome node上因为包含重要的metadata如PAT，PIB，PRD和PLT；这些数据会同步备份在从副本；home node负责检测slab node上的故障，然后home node根据PAT上的信息来重建重启的slab node即可。\n集群恢复 在极少数情况下，当主节点的所有副本都不可用时，需要通过集群恢复来恢复服务。所有数据库节点和内存节点将从清除状态重新启动，所有内存状态将从存储重新启动。初始化后（连接到远程内存和存储器等），RW节点执行之前所述的并行REDO恢复，然后扫描撤销头以查找所有未完成的事务。之后，RW节点将启动服务，并在后台回滚未提交的事务。在集群恢复过程中，缓存在远程内存中的页面将被清除，因此它将忍受冷缓存问题。\n","date":"2021-12-22T14:17:12+08:00","image":"https://2021.sigmod.org/images/2021sigmod-logo1.jpg","permalink":"https://tweakzx.github.io/p/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0polardb-serverless%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/","title":"【论文笔记】PolarDB-Serverless论文阅读报告"},{"content":"1 HPL（High Performance Linpack) 假设要使用HPL程序在4个进程上解一个4096 * 4096的方程组（4096 * 4096的矩阵加一列方程组的右端项b），按照讲义第14页所示的block-cyclic方式对数据进行分配，NB=512。4个进程按1 * 4和4 * 1两种方式排布。那么，在HPL的回代部分（讲义48到55页），X的各个元素分别是由哪些进程算出的？例如，X[0..512]由进程(3, 0)求出。写出两种排布方式下X的各部分分别由哪些进程计算得到。（5分）\n 1 * 4 排布  \rimage-20211214170301242\r\n   X 进程     X[0..511] (0,0)   X[512..1023] (0,1)   X[1024..1535] (0,2)   X[1536..2047] (0,3)   X[2048..2559] (0,0)   X[2560..3071] (0,1)   X[3072..3583] (0,2)   X[3584..4095] (0,3)     4 * 1 排布  \rimage-20211214170325158\r\n   X 进程     X[0..511] (0,0)   X[512..1023] (0,1)   X[1024..1535] (0,2)   X[1536..2047] (0,3)   X[2048..2559] (0,0)   X[2560..3071] (0,1)   X[3072..3583] (0,2)   X[3584..4095] (0,3)    ","date":"2021-12-14T13:55:35+08:00","image":"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpmob0455c.pic27.websiteonline.cn%2Fupload%2Felinpack3000-1200_dps0.jpg\u0026refer=http%3A%2F%2Fpmob0455c.pic27.websiteonline.cn\u0026app=2002\u0026size=f9999,10000\u0026q=a80\u0026n=0\u0026g=0n\u0026fmt=jpeg?sec=1642054231\u0026t=55c0893a251868314596e23dae76a843","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E4%BD%9C%E4%B8%9A-3/","title":"【分布式与并行计算】并行作业-3"},{"content":"1 加速向量加法 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;assert.h\u0026gt; inline cudaError_t checkCuda(cudaError_t result) { if (result != cudaSuccess) { fprintf(stderr, \u0026#34;CUDA Runtime Error: %s\\n\u0026#34;, cudaGetErrorString(result)); assert(result == cudaSuccess); } return result; } void initWith(float num, float *a, int N) { for(int i = 0; i \u0026lt; N; ++i) { a[i] = num; } } __global__ void addVectorsInto(float *result, float *a, float *b, int N) { int initIndex = threadIdx.x + blockIdx.x * blockDim.x; int gridStride = gridDim.x * blockDim.x; for(int i = initIndex;i\u0026lt;N;i+=gridStride){ result[i] = a[i] + b[i]; } } void checkElementsAre(float target, float *array, int N) { for(int i = 0; i \u0026lt; N; i++) { if(array[i] != target) { printf(\u0026#34;FAIL: array[%d] - %0.0f does not equal %0.0f\\n\u0026#34;, i, array[i], target); exit(1); } } printf(\u0026#34;SUCCESS! All values added correctly.\\n\u0026#34;); } int main() { const int N = 2\u0026lt;\u0026lt;20; size_t size = N * sizeof(float); float *a; float *b; float *c; cudaMallocManaged(\u0026amp;a, size); cudaMallocManaged(\u0026amp;b, size); cudaMallocManaged(\u0026amp;c, size); initWith(3, a, N); initWith(4, b, N); initWith(0, c, N); size_t threads_per_block = 1024; size_t number_of_blocks = (N+threads_per_block-1)/threads_per_block; addVectorsInto\u0026lt;\u0026lt;\u0026lt;32,1024\u0026gt;\u0026gt;\u0026gt;(c, a, b, N); //addVectorsInto\u0026lt;\u0026lt;\u0026lt;number_of_blocks,threads_per_block\u0026gt;\u0026gt;\u0026gt;(c, a, b, N);  checkCuda(cudaGetLastError()); checkCuda(cudaDeviceSynchronize()); checkElementsAre(7, c, N); cudaFree(a); cudaFree(b); cudaFree(c); } 2 加速SAXPY #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;assert.h\u0026gt;#define N 2048 * 2048 // Number of elements in each vector  /* * Optimize this already-accelerated codebase. Work iteratively, * and use nsys to support your work. * * Aim to profile `saxpy` (without modifying `N`) running under * 20us. * * Some bugs have been placed in this codebase for your edification. */ inline cudaError_t checkCuda(cudaError_t result) { if (result != cudaSuccess) { fprintf(stderr, \u0026#34;CUDA Runtime Error: %s\\n\u0026#34;, cudaGetErrorString(result)); assert(result == cudaSuccess); } return result; } __global__ void saxpy(float * a, float * b, float * c) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int stride = gridDim.x * blockDim.x; for(int i = tid; i\u0026lt;N; i+=stride){ c[tid] = 2 * a[tid] + b[tid]; } } int main() { int deviceId; int numberOfSMs; cudaGetDevice(\u0026amp;deviceId); cudaDeviceGetAttribute(\u0026amp;numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); float *a, *b, *c; int size = N * sizeof (float); // The total number of bytes per vector  cudaMallocManaged(\u0026amp;a, size); cudaMallocManaged(\u0026amp;b, size); cudaMallocManaged(\u0026amp;c, size); // Initialize memory  for( int i = 0; i \u0026lt; N; ++i ) { a[i] = 2.0; b[i] = 1.0; c[i] = 0.0; } cudaMemPrefetchAsync(a, size, deviceId); cudaMemPrefetchAsync(b, size, deviceId); cudaMemPrefetchAsync(c, size, deviceId); int threads_per_block = 256; int number_of_blocks = numberOfSMs * 32 ; saxpy \u0026lt;\u0026lt;\u0026lt; number_of_blocks, threads_per_block \u0026gt;\u0026gt;\u0026gt; ( a, b, c ); checkCuda(cudaGetLastError()); checkCuda(cudaDeviceSynchronize()); // Print out the first and last 5 values of c for a quality check  for( int i = 0; i \u0026lt; 5; ++i ) printf(\u0026#34;c[%d] = %f, \u0026#34;, i, c[i]); printf (\u0026#34;\\n\u0026#34;); for( int i = N-5; i \u0026lt; N; ++i ) printf(\u0026#34;c[%d] = %f, \u0026#34;, i, c[i]); printf (\u0026#34;\\n\u0026#34;); cudaFree( a ); cudaFree( b ); cudaFree( c ); } 3 N-body #include \u0026lt;math.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026#34;timer.h\u0026#34;#include \u0026#34;files.h\u0026#34; #define SOFTENING 1e-9f  #include \u0026lt;assert.h\u0026gt;inline cudaError_t checkCuda(cudaError_t result) { if (result != cudaSuccess) { fprintf(stderr, \u0026#34;CUDA Runtime Error: %s\\n\u0026#34;, cudaGetErrorString(result)); assert(result == cudaSuccess); } return result; } /* * Each body contains x, y, and z coordinate positions, * as well as velocities in the x, y, and z directions. */ typedef struct { float x, y, z, vx, vy, vz; } Body; /* * Calculate the gravitational impact of all bodies in the system * on all others. */ __global__ void bodyForce(Body *p, float dt, int n) { int index = blockIdx.x * blockDim.x + threadIdx.x; int stride = gridDim.x * blockDim.x; for (int i = index; i \u0026lt; n; i+=stride) { float Fx = 0.0f; float Fy = 0.0f; float Fz = 0.0f; for (int j = 0; j \u0026lt; n; j++) { float dx = p[j].x - p[i].x; float dy = p[j].y - p[i].y; float dz = p[j].z - p[i].z; float distSqr = dx*dx + dy*dy + dz*dz + SOFTENING; float invDist = rsqrtf(distSqr); float invDist3 = invDist * invDist * invDist; Fx += dx * invDist3; Fy += dy * invDist3; Fz += dz * invDist3; } p[i].vx += dt*Fx; p[i].vy += dt*Fy; p[i].vz += dt*Fz; } } int main(const int argc, const char** argv) { // The assessment will test against both 2\u0026lt;11 and 2\u0026lt;15.  // Feel free to pass the command line argument 15 when you gernate ./nbody report files  int nBodies = 2\u0026lt;\u0026lt;11; if (argc \u0026gt; 1) nBodies = 2\u0026lt;\u0026lt;atoi(argv[1]); // The assessment will pass hidden initialized values to check for correctness.  // You should not make changes to these files, or else the assessment will not work.  const char * initialized_values; const char * solution_values; if (nBodies == 2\u0026lt;\u0026lt;11) { initialized_values = \u0026#34;files/initialized_4096\u0026#34;; solution_values = \u0026#34;files/solution_4096\u0026#34;; } else { // nBodies == 2\u0026lt;\u0026lt;15  initialized_values = \u0026#34;files/initialized_65536\u0026#34;; solution_values = \u0026#34;files/solution_65536\u0026#34;; } if (argc \u0026gt; 2) initialized_values = argv[2]; if (argc \u0026gt; 3) solution_values = argv[3]; const float dt = 0.01f; // Time step  const int nIters = 10; // Simulation iterations  int bytes = nBodies * sizeof(Body); float *buf; cudaMallocManaged(\u0026amp;buf, bytes); Body *p = (Body*)buf; read_values_from_file(initialized_values, buf, bytes); double totalTime = 0.0; /* * This simulation will run for 10 cycles of time, calculating gravitational * interaction amongst bodies, and adjusting their positions to reflect. */ int deviceId; int numberOfSMs; cudaGetDevice(\u0026amp;deviceId); cudaDeviceGetAttribute(\u0026amp;numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId); int threads_per_block = 128; int number_of_blocks = numberOfSMs * 25 ; for (int iter = 0; iter \u0026lt; nIters; iter++) { StartTimer(); /* * You will likely wish to refactor the work being done in `bodyForce`, * and potentially the work to integrate the positions. */ bodyForce\u0026lt;\u0026lt;\u0026lt;threads_per_block,number_of_blocks\u0026gt;\u0026gt;\u0026gt;(p, dt, nBodies); // compute interbody forces  checkCuda(cudaGetLastError()); checkCuda(cudaDeviceSynchronize()); /* * This position integration cannot occur until this round of `bodyForce` has completed. * Also, the next round of `bodyForce` cannot begin until the integration is complete. */ for (int i = 0 ; i \u0026lt; nBodies; i++) { // integrate position  p[i].x += p[i].vx*dt; p[i].y += p[i].vy*dt; p[i].z += p[i].vz*dt; } const double tElapsed = GetTimer() / 1000.0; totalTime += tElapsed; } double avgTime = totalTime / (double)(nIters); float billionsOfOpsPerSecond = 1e-9 * nBodies * nBodies / avgTime; write_values_to_file(solution_values, buf, bytes); // You will likely enjoy watching this value grow as you accelerate the application,  // but beware that a failure to correctly synchronize the device might result in  // unrealistically high values.  printf(\u0026#34;%0.3f Billion Interactions / second\u0026#34;, billionsOfOpsPerSecond); cudaFree(buf); } 4 证书 \rimage\r\n","date":"2021-12-13T19:03:20+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/image-20211215144717123.png","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97cuda%E5%8A%A0%E9%80%9F%E8%AF%BE%E7%A8%8B%E9%A2%98%E7%9B%AE/","title":"【分布式与并行计算】CUDA加速课程题目"},{"content":"1 矩阵向量乘法（6分） 矩阵向量乘法(gemv)如何用OpenMP或pthread对其并行化(OpenMP和pthread任选一种即可)？假设矩阵按行存储（每一行数据是连续的），处理器有32个核。如果矩阵是按列存储呢？具体实现如何修改？\n可用语言详细描述或写出伪代码\n  按行存储\nvoid *worker1(int row, int N, int** A, int* vec, int* result){ for(int i = row; i\u0026lt;N; i += 32){ result[i] = innerProduct(A[i],Vec); } } int main(){ ... for(int i=0;i\u0026lt;32;i++){ data_array[i].row = i; pthread_create(\u0026amp;threads[i], NULL, worker1, (void *)\u0026amp;data_array[i]) } ... }   按列存储\nvoid *worker2(int column, int N, int M, int** A, int* vec, int* result){ for(int j = column; j\u0026lt;M; j += 32){ for(int i = 0; i\u0026lt;N; i++){ result[i] += A[j][i]*Vec[j]; } } } int main(){ ... for(int i=0;i\u0026lt;32;i++){ data_array[i].column = i; pthread_create(\u0026amp;threads[i], NULL, worker1, (void *)\u0026amp;data_array[i]) } ... }   2 程序分析（4分） 以下程序运行时会出现什么现象？可以如何改写来避免此现象发生？\nint selected_thread; sem_t start1, start2, stop1, stop2; void* worker1() { sem_wait(\u0026amp;start1); sem_post(\u0026amp;stop1); } void *worker2(){ sem_wait(\u0026amp;start2); sem_post(\u0026amp;stop2); } int main(int argc, char *argv[]) { selected_thread = 2; sem_init(\u0026amp;start1); sem_init(\u0026amp;start2); sem_init(\u0026amp;stop1); sem_init(\u0026amp;stop2); pthread_create(worker1); pthread_create(worker2); sem_post(\u0026amp;start1); sem_wait(\u0026amp;stop1); sem_wait(\u0026amp;stop2); return 0; } 答： worker2会一直等待，程序无法结束。所以在主函数里加上sem_post(\u0026amp;start2)。\n","date":"2021-12-11T18:01:26+08:00","image":"https://pic1.zhimg.com/v2-f9d2f5486443cc045996753d59f80a7e_1440w.jpg?source=172ae18b","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E4%BD%9C%E4%B8%9A-2/","title":"【分布式与并行计算】并行作业-2"},{"content":"1矩阵向量乘法（4分） 讲义55页所示结果Y，如果要作为下一次矩阵向量乘法的输入X，切分到不同的列进程，并且复制到每一行进程，应如何操作？可写出伪代码，或用语言描述。\n即图(a)中的Y，变成下图(b)中的X。假设每行有P个进程，每列也是P个进程，一共P*P个进程。\n\rimage-20211210120943046\r\n答：进程$P_{ij}$和所控制的矩阵进行矩阵向量乘之后，将结果存入进程$P_{ji}$。\n2 代码填空（3分） 在测量程序性能时，我们经常要记录整个程序或程序中某一部分的运行时间。在MPI程序中，由于每个进程的运行时间不同，一般需要取各个进程运行时间的最大值，然后由0号进程保存和打印（其他进程不需要保存）。以下程序完成了这个功能，请在横线处填上函数调用语句。\nint main(int argc, char * argv) { double total_time; double time0, time1; int procs, rank; MPI_init(argc, argv); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;procs); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); time0 = MPI_Wtime(); //do some computation \ttime1 = MPI_Wtime() – time0; _______________________________________________________________ if(rank == 0) { Printf(“Total execution time is %f seconds\\n”, total_time); } } 答：\nMPI_Reduce(\u0026amp;time1,\u0026amp;total_time,1,MPI_DOUBLE,MPI_MAX,0,MPI_COMM_WORLD) 3 以下程序相当于哪个MPI聚合操作？（3分） #define N 16384 double *send_buff, *recv_buff; MPI_Status status; int i, nprocs, myid, count=N/num_procs; send_buff = (double*)malloc(N*sizeof(double)); recv_buff = (double*)malloc(N*sizeof(double)); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;nprocs); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;myid); //此处省略send_buff中数据的初始化 memcpy((void*)(recv_buff + myid * count), (void*)(send_buff + myid * count), count*sizeof(double)); for(int i = 0; i \u0026lt; nprocs;i++) { if(i!=myid) { MPI_Sendrecv(send_buff+i*count, count, MPI_DOUBLE, i, 400, recv_buff+i*count, count, MPI_DOUBLE, i, 400, MPI_COMM_WORLD, \u0026amp;status); } } 答：MPI_Alltoall()\n","date":"2021-12-10T11:28:33+08:00","image":"https://img2.baidu.com/it/u=2900542081,1673808678\u0026fm=26\u0026fmt=auto","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C%E4%BD%9C%E4%B8%9A-1/","title":"【分布式与并行计算】并行作业-1"},{"content":"1 逻辑时钟与一致割集 下图中，直线上小黑点给出了时钟计数，请分别用Lamport 逻辑时钟和向量时钟给图上的事件设置时间戳，并给出一致割集和非一致割集的例子。\n\r事件时钟计数\r\n答：\n  设置时间戳\n  Lamport逻辑时钟\n\rLamport逻辑时钟\r\n  向量时钟\n\r向量时钟\r\n    割集的例子\n  一致割集\n\r一致割集\r\n  非一致割集\n\r非一致割集\r\n包含$e_1^2$这个接收事件但是不包含$e_3^1$这个发送事件。\n    2 异步分布式系统的故障类型 考虑在异步分布式系统中使用的两个通信服务。在服务A中，消息可能丢失、被复制或延迟，校验和仅应用到消息头。在服务B中，消息可能丢失、延迟或发送地太快以致接收方无法处理它，但到达目的地的消息其内容一定正确。描述每个服务会有的故障类型。根据对有效性和完整性的影响将故障分类。服务B能被描述成一个可靠的通信服务吗？\n答：\n  服务A会有的故障类型\n 遗漏故障  消息丢失   拜占庭故障  checksum仅仅应用到消息的head，消息的body可能发生损坏 消息重复   因为这是异步分布式系统，所以不会有时序故障。  遗漏故障的消息丢失破坏了有效性。拜占庭故障的可能损坏的消息以及重复的消息破坏了完整性。\n  服务B会有的故障类型\n 遗漏故障  消息丢失 发送地太快以致接收方无法处理它，接收遗漏   因为这是异步分布式系统，所以不会有时序故障。  服务B满足完整性，但是服务B消息发生的遗漏故障，不满足有效性，所以不是可靠的通信服务。\n  3 Ricart and Agrawala 算法 请证明Ricart-Agrawala的互斥算法满足ME2和ME3。\n ME2:进入或退出临界区的请求最终都会成功\nME3:如果一个进入临界区的请求发生在先，那么进入临界区也按此顺序\nRicart-Agrawala算法：\n 一个进程申请资源时向所有其他进程发出带有时间戳的申请报文； 一个进程收到申请报文后，答复这个申请，当且仅当：1）若不在临界区并且自己未申请进入临界区,或者2)自己虽然发出了申请报文，但自己的报文的时间戳大于收到的申请报文。如果已经在临界区，或者自己的申请发送在前，则在出临界区之前将所有的申请挂起。 申请资源的进程仅在收到其它所有进程的回答报文后才进入临界区使用资源； 一个进程使用完资源后，它向所有挂起的申请发送回答报文。   证明：\n 证明满足ME2  一个进程$p_i$要进入临界区向其他进程发送请求，不想进入临界区的进程，或者已经发送了请求但是发送时间戳大于接收请求时间戳$T_i$的进程都会回复。所以进程$p_i$要等待逻辑时间戳$T_i$之前发送了请求的进程以及正在临界区的进程的答复。 在逻辑时间戳$T_i$之前发出请求的进程所等待进程的数量依次递减到1。等正在临界区的进程使用完资源并退出后，所有在等待的进程所需等待进程数量全都减1。此时有一个进程得到了全部的答复，进入临界区。 按照这个过程，只要前边的进程依次进入临界区并退出之后，进程$p_i$就可以成功进入临界区，毕竟没有进程可以插队，所需等待进程数量只能递减。 由于使用完资源后退出临界区不需要等待答复，所以可以成功退出。所以，满足ME2。   证明满足ME3  当进$p_i$给其他进程发送进入临界区的申请时 如果进程$p_j$也想进入临界区，发送的申请的时间戳小于收到的$T_i$，那么$p_j$不会发送答复给$p_i$,这样$p_i$就必须等待$p_j$进入并退出临界区之后才能得到答复，才有可能进入临界区。 如果进程$p_j$也想进入临界区，发送的申请的时间戳大于收到的$T_i$，那么他会回复$p_i$的申请，$p_i$无需等待$p_j$，且当$p_i$收到$p_j$的请求之后不会回复，$p_j$等待$p_i$。 所以，满足ME3    4 改进Ricart and Agrawala 算法 在Ricart-Agrawala的互斥算法中，原始假定系统的进程是不出故障的。请修改算法增加处理一个进程崩溃的情况。\n答：如果有进程崩溃，那么它永远不会回复，则发送请求的进程需要一直盲等。\n 所以每当接收到消息之后要做出答复，回复同意，当且仅当：1）若不在临界区并且自己未申请进入临界区,或者2)自己虽然发出了申请报文，但自己的报文的时间戳大于收到的申请报文。或者回复拒绝，当1）自己处在临界区，或者2）自己的临界区申请的逻辑时间戳小于收到的申请，把回复拒绝的进程缓存起来，等待回复。 如果超出timeout没有收到答复则认为机器故障，只需等待其余的机器全部回复同意 当一个进程退出临界区之后，要向所有回复拒绝的进程回复同意。  5改进基于环的互斥算法 改进基于环的互斥算法使得它能检测权标的丢失并重新生成权标。\n答：\n 一个进程发出申请之后，如果长时间没有拿到权标，则向下一个节点发送一个请求reqest确认权标是否还在。 一个进程收到了确认权标存在的请求request，如果权标在自己手里则向下一个节点发送一个exist，如果权标不在自己手里，则将requst传递给下一个节点 一个进程收到了一个exist消息则将它传递到下一个节点 如果发出申请的进程收到request请求，则认为权标丢失，重新生成权标。如果收到exist，则说明一切正常。  6 双向环结构的选举算法 基于环的选举算法是建立在单向环的假设之上的，为了获得更快的选举速度，现采用双向环结构，即每个节点可以同时向顺时针和逆时针两个方向发送选举消息，请列出新算法的高层描述，并用一个四节点的双向环来说明你的方法。\n答：\n 最初，所有进程标记为非参与者，任意一个进程发起选举，发起选举后，置自己为参与者（$elected_i = ⊥$)，向上下游发送一个选举消息，包含自己的标识符。 一个进程收到选举消息后，那么比较自己的标识符与收到消息中的标识符，  如果自己的标识符小于消息中的标识符，那么顺消息来的方向传递选举消息； 如果自己的标识符大于消息中的标识符  如果自己是非参与者，将消息中的标识符改为自己标识符，顺消息来的方向传递选举消息。 如果自己是参与者，则不转发消息。   如果自己的标识符等于消息中的标识符，那么说明自己的标识符最大，如果还未当选，则当选为协调者，向上游和下游发送一个当选消息，包含自己的标识符P   一个进程收到一个当选消息，如果自己是参与者，设置$elected_i = P$，置自己为非参与者，并且按照消息发送的方向传递给自己的邻居；如果自己已经知晓当选消息，则不转发。  Init elected != ⊥ for process i (i = 1,2,...N)\rin the process which start an election:\rfunction startElection():\relectingMSG \u0026lt;- MSG(type = \u0026quot;electing\u0026quot;, value = Local.id)\rLocal.elected = '⊥'\rsend(msg = electingMSG ,direction = clockwise)\rsend(msg = electingMSG , direction = counterclockwise)\rin process i:\rfunction handleMSG(MSG msg, Direction direction):\rif msg.type = \u0026quot;electing\u0026quot;:\rif Local.id \u0026lt; msg.id:\rsend(msg,direction)\rLocal.elected = '⊥'\relse if Local.id \u0026gt; msg.id:\rif Local.elected != '⊥':\rmsg.value = Local.id\rsend(msg, direction)\rLocal.elected = '⊥'\rend\relse\rif(process i is not Coordinator)\rsetCoordinator(process i)\rLocal.elected = '⊥'\relectdeMSG \u0026lt;- MSG(type = \u0026quot;elected\u0026quot;, value = Local.id)\rsend(msg = electedMSG, direction = clockwise)\rsend(msg = electedMSG, direction = counterclockwise)\rend\rend\rend if msg.type = \u0026quot;elected\u0026quot;\rif Local.elected == '⊥':\rLocal.elected = msg.value\rsend(msg, direction)\rend\rend\r\rimage-20211209165145519\r\n图中:\t白色表示初始非参与者，黄色表示参与者，红色表示知道当选结果。蓝色是选举消息，绿色是当选消息。从13号开始发起选举，最终选出23为协调者。\n 网络带宽：找到最大标识符最多需要N个消息，确认最大标识符最多需要2N个消息，通知当选最多需要N+1个消息，则最多需要消息为 4N+1 回转时间：第一轮寻找花费最多(N/2)个消息，第二轮确认需要最多花费N个消息，第三轮通知最多需要花费(N/2)+1个消息，所以最多需要花费2N+1个消息的回转时间。  7 基于生成树的选举算法 节点之间按照生成树方式连接，仅有边相连的节点能通信，请基于此网络拓扑，设计一个选举算法，给出其伪码。当仅有一个进程发起选举，你的选举算法所需的消息量是多少？\n答：假定是有向生成树，每个节点有父节点和子节点，每个节点可以向子节点或者父节点发送消息。任何一个进程都可以发起选举。算法如下：\n  收到选举消息，将发送消息的标识符和自己的标识符作比较，更改消息中的信息为较大值；如果有子节点，则向子节点发送这个选举消息；如果是叶子节点，则父亲节点返回一个回复消息，包含当前的较大标识符。\n  等到收到所有子节点的回复消息，选出最大的标识符，返回给父亲节点\n  当根节点收到最大的 标识符，则向所有的子节点发送当选信息，直到叶节点。如果有子节点发现自己的标识符等于消息中的的标识符，则成为协调者。\n  我的选举算法所需的消息量选举有N-1个，回复有N-1个，当选有N-1个，所以总共有3N-3个。\nInit Local.elected != ⊥ for process i (i = 1,2,...N)\rLocal.count =0 for process i (i = 1,2,...N)\rin the process p0 which start an election:\rfunction startElection():\relectingMSG \u0026lt;- MSG(type = \u0026quot;electing\u0026quot;, value = Local.id)\rLocal.elected = '⊥'\rfor i in son(p0):\rsend(electingMSG, dest = i)\rend\rin process p:\rfunction handleMSG(MSG msg):\rif msg.type = \u0026quot;electing\u0026quot;:\rLocal.elected = '⊥'\rif !isEmpty(son(p):\rmsg.value = max(Local.id,msg.value)\rfor i in son(p):\rsend(msg, dest = i)\rend\relse\rreplyMSG = MSG(type = \u0026quot;reply\u0026quot;, value = max(Local.id,msg.value)\rsend(relpyMSG, dest = father(p))\rend\rif msg.type = \u0026quot;reply\u0026quot;:\rLocal.count++;\rmsg.value = max(Local.id, msg.value)\rif(Local.count == son(p).size()):\rif !isEmpty(father(p)):\rsend(msg, dest = father(p))\relse\relectedMSG = MSG(type = \u0026quot;elected\u0026quot;,value = msg.value)\rfor i in son(p):\rsend(electedMSG, dest = i)\rend\rLocal.elected = msg.value\rif Local.ip == msg.value:\rsetCoordinator(process = p)\rend\rend\rend\rend\rif msg.type = \u0026quot;elected\u0026quot;:\rif Local.ip == msg.value:\rsetCoordinator(process = p)\rend\rif !isEmpty(son(p)):\rfor i in son(p):\rsend(msg, dest = i)\rend\rend\r8 法定数共识复制 在服务器X、Y和Z上使用法定数共识进行复制，这些服务器都有数据项A和B的副本。A和B副本的初始值是100，并且在X、Y和Z上A和B的选票是1。同样对于A和B，R＝W＝2。一个客户读A的值然后将它写到B上。\n1）当客户执行这些操作时，出现了一个分区，将服务器X和Y与服务器Z分隔开了。描述当客户能访问服务器X和Y时，获得的法定数和发生的操作。\n2）描述当客户仅能访问服务器Z时，获得的法定数和发生的操作。\n3）分区修复了，然后另一个分区发生了，结果X和Z与Y分隔开了。描述当客户能访问服务器X和Z时，获得的法定数和发生的操作。\n答：\n1）在数据的v0版本时，A和B副本的初始值是100，出现了一个分区，将服务器X和Y与服务器Z分隔开。\n   X Y Z     A = 100（v0) A = 100（v0) A = 100（v0)   B = 100（v0) B = 100（v0) B = 100（v0)    ​\t客户可能从X或者Y上读取A，R = 1+1 =2，读取成功。\n​\t客户需要在X和Y上写B，W = 1+1 = 2， 写成功。\n2)客户只能访问服务器Z，R = 1，客户无法读取；W = 1，客户无法写。\n3)当分区被修复后，因为之前的分区导致X和Y的数据要比Z上的数据更新，例如\n   X Y Z     A = 200（v1) A = 200（v1) A = 100（v0)   B = 300（v1) B = 300（v1) B = 100（v0)    此时，另一个分区出现，X和Z与Y分隔开，当客户试图获取法定数的时候，发现Z的数据版本过时，于是Z根据X上的最新数据更新自己。之后客户获取读法定数，R = 1+1 =2，然后读成功。客户获取写法定数，W = 1+1 =2，写成功。\n9 串行等价的交错执行 一个服务器管理对象a1, a2, \u0026hellip; an ，它为客户提供下面两种操作：read (i)返回对象ai的值。write(i, Value)将对象ai设置为值Value。\n事务T和U定义如下：\nT: x = read(j); y = read (i); write(j, 44); write(i, 33)\nU: x = read(k); write(i, 55); y = read (j); write(k, 66)\n请给出事务T和U的3个串行化等价的交错执行。\n答：我们给每一步分别表上序号：\nT: ①x = read(j); ②y = read (i); ③write(j, 44); ④write(i, 33)\nU: ⑤x = read(k); ⑥write(i, 55); ⑦y = read (j); ⑧write(k, 66)\n如果按照先T后U的顺序依次执行一个事务的话，我们发现①和⑤，②和⑦，④和⑥有写后写依赖；③和⑦有写后读依赖，①和③，②和④，②和⑥，⑤和⑧有读后写依赖，所以，我们要保证这些依赖的逻辑顺序，串行化等价的交错执行如下：\n①⑤②③④⑥⑦⑧\n①③⑤②④⑦⑧⑥\n①⑤③②⑧⑦④⑥\n10 乐观并发控制 考虑将乐观并发控制应用于下列事务T和U的情况：\n​ T: x = read(i); write(j, 44);\n​ U: write(i, 55); write(j, 66);\n如果事务T和U同时处于活动状态，试描述以下几种情况的结果如何：\n 服务器首先处理T的提交请求，使用向后验证方式。 服务器首先处理U的提交请求，使用向后验证方式。 服务器首先处理T的提交请求，使用向前验证方式。 服务器首先处理U的提交请求，使用向前验证方式。  对于上面的每种情况，描述事务T和U的操作顺序，注意写操作在验证通过之后才真正起作用。\n答：\n  服务器首先处理T的提交请求，使用向后验证方式。\nT先开始所以T的验证阶段无事发生，U进入验证阶段之后，U没有读集，所以没有冲突，可以顺利提交。\n  服务器首先处理U的提交请求，使用向后验证方式。\nT进入验证阶段，发现T的读集{i}与U的写集{i, j}有冲突，T事务被放弃。\n  服务器首先处理T的提交请求，使用向前验证方式。\nU进入验证阶段，发现U的写集{i,j}与T的读集{i}有冲突，所以推迟验证，等T的读集执行完毕之后再提交U。\n  服务器首先处理U的提交请求，使用向前验证方式。\nT进入验证阶段，发现U无读集，所以可以通过验证。\n  ","date":"2021-12-07T14:21:24+08:00","image":"https://img2.baidu.com/it/u=1163044150,1040852846\u0026fm=26\u0026fmt=auto","permalink":"https://tweakzx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A/","title":"【分布式与并行计算】分布式作业"},{"content":" 时间：2021-12-2 下午5点半\n方式：阿里会议视频面试\n岗位：研究型实习生-智能存储\n 面试过程 这次面试主要是在聊项目，聊兴趣啥的。没有什么算法或者知识点的提问。\n 先做一下自我介绍。 问我觉得自己在学习上的最大优势是什么。 问我大三实习的主要工作，这一部分我还是忽略的带过了。 讲了自己的大二时的比赛。 问了可以实习的时间。 目前在学校里都学了什么课程。 面试官问我大四在做什么，我回答自己保研后在课题组里做结项的事情。面试官比较好奇我自己都做了什么，我回答东西都是别人做的，我就是提供一些辅助性的工作。 然后询问我课题组的主要研究是什么，我回答云计算，docker，k8s之类的。面试官问那为啥你做的东西和云计算一点关系都没有，又去做了算法NLP，我回答这是老师安排的。 问我对docker，k8s了解多少。我回答仅仅会简单的使用。 后来面试官解释NLP之类的工作一般是达摩院内边来做，意思可能是这边还是偏重于开发。 问我对这个部门的工作了解吗，一面有介绍吗？我回答和存储，预训练模型好像有关。 面试官说你要自己想好自己要做什么方向，兴趣是最好的老师啥的。（我也想啊） 面试官问我对NLP了解多少，我说仅仅毕业设计和这个有关，知道预训练和蒸馏。 他说感觉你对这些方面都有一点了解，但又都不深，不知道你将来要做什么。我回答自己目前还在探索，打算先开始做在去感受自己想要什么。 感觉聊到这里，其实已经无话可聊了。  下面是反问环节\n 我说自己可能问的问题比较大不好回答：如果我有机会得到这个offer，那我需要弥补的差距在什么地方？ 面试官从两个方面建议，一个就是如果计算想搞算法的话，blabla我忘了。如果想做开发的话，可以学习一下云原生，云计算之类的知识。 面试官分析了一下之后，我回答自己的理解与看法 ，主要说了人工智能是一种拿来应用的技术，做工程的过程中拿来使用，是工程的一部分。不是所有人都要去研究模型，我们能拿来即用即可，还是要去提升工程上的开发能力。  面试总结 我其实看不出来这次的结果好坏，面试官也确实提到了欢迎来去做一些尝试，可以先进来，实习的过程中摸索自己的兴趣，感受一下各个方向是什么样的，但更像是一些 说法正确的客套话。\n目前还在等结果。（希望结果是好的吧）\n","date":"2021-12-04T18:31:16+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202305061520436.png","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%98%BF%E9%87%8C%E5%AE%9E%E4%B9%A0%E4%BA%8C%E9%9D%A2/","title":"【实习面试】阿里实习二面"},{"content":" 时间：2021-11-30 下午2点\n方式：阿里会议视频面试\n岗位：研究型实习生-智能存储\n 为什么会参加这次面试？ 因为想要亲身参与真实的科研活动或者一个真实企业内的做工程的过程，所以投递了这份简历，我也没有想到会给我安排面试。其实面试的时候我已经忘记自己投递的是哪个课题了，现在推测一下可能是大规模预训练模型的迁移啥的。\n因为面试的前一天参加了字节的面试，十分挫败，所以对面试可以说十分没有信心，加上收到的邮件里没有会议链接。无论面与不面，迟早要打电话告知，所以索性决定打电话推辞掉这个面试。打了电话，估计中午面试联系人可能在休息，所以过了一会才接到，面试联系人劝说我面试没有坏的影响，就当锻炼自己，只有好处没有坏处，不如一试。所以没有推脱，反正就一个小时，一个小时之后我的生活还会恢复原样。\n面试过程 面试的时候发现刚刚接电话的人应该就是面试官，面试官给我详细介绍了一下什么是研究型实习生以及它和其他的实习生的区别，以及对招聘的影响。然后就开始面试，先是自我介绍，还是介绍了本科，研究生的学校和专业，实习与比赛。问了我大概想做科研还是偏工程的方向，我回答工程，但后来想想应该回答我都行的其实。\n  面试了一道算法题：\n 将一个字符串按单词反转，但是对空间的开销有限制，最好是在原地址上直接修改，如果用栈，或者切割单词成数组之类的方式都是不符合空间开销要求的。\n 刚开始我想的是将单词先切出来，面试官发现我可能没有理解题目就又说了一下。\n我之后想说判断空格 然后做首尾交换，面试官提醒我单词的长度可能是不一致的，让我再想想。\n面试官说可以先说一下思路，再写代码。（说实话这点还挺赞的。）\n最后我想了想，说实话因为没有什么信心，我都想放弃算了。\n但偏偏还是想到了先把每个单词都先在局部反转，然后全局一起反转就不会产生大的空间开销。\n面试官说这个想法是对的，然后让我自己实现一下，就大概写了写代码。\n又问了我这个算法的时间复杂度是多少，我说是O(n)。\n  问平时怎么做测试，我回答用自己设计测试样例，然后print的方法和编辑器调试。\n他问我有么有用什么测试工具之类的，我说在课上学过UnitTest4，但实际上没用过。\n  知不知道多线程，pthread之类的。\n  之后聊了实习，实习时写的Json工具，日志接口开发。我都回答其实没有什么含金量。\n  然后是机器人大赛，路径规划算法用的是A* ,问我为什么用A* 而不是用机器人走迷宫的方式来操控机器人。和视觉算法的设计，以及OpenCV啥的。\n  最后聊了本科毕设，介绍了自己的毕设内容，问我觉得最有挑战的部分是啥。我回答是loss函数的设计，三段蒸馏，每段的不同的损失计算方法。\n  然后就是反问环节，我没问，确实不知道问啥。\n面试总结 大概就只记得这些了，好像忘记了很多其他的细节，但是面试完心情也比之前好了一些。\n阿里巴巴的这次面试给我最大的感受就是，面试官会确认自己的意思有没有准确传达给我，这个细节还挺令我感动的。面试官会把自己的问题或者很多要考虑的情况讲的很细致，确认我理解之后再让我思考并回答，这样其实对我这样不太熟悉面试的人十分友好的。\n面试的结果还不知道，但是无论好与坏，这次面试都给了我一些鼓舞，即便是结果不太好，我也不会气馁，不管怎么样都要继续努力。（希望结果是好的吧）\n后来接到了二面的电话，所以一面应该是过了。\n","date":"2021-12-01T21:49:57+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202305061520436.png","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%98%BF%E9%87%8C%E5%AE%9E%E4%B9%A0%E7%94%9F%E9%9D%A2%E8%AF%95/","title":"【实习面试】阿里实习生面试"},{"content":" 时间：2021-11-26 下午六点\n方式：飞书视频面试\n岗位：后端开发\n 为什么会面试蓝湖 心血来潮想要出去实习，在校友群内发了求助。24号中午，本科的同班同学涛神问我想不想试一试蓝湖，我想要一份实习来提升自己的代码水平，所以当然要抓住这次机会。\n所以中午抓紧时间写了一份简历交给了涛神帮忙内推。涛神问我要不要先准备一下，我说不了，早解决早轻松，可能是犯蠢了，也可能是太了解自己，我不是会好好规划复习的人，所以不如趁着热情直接上。\n面试的过程 面试官还是很温柔的，上来先让我做了一下自我介绍，我介绍了一下本科和硕士，以及一段实习经历，一段比赛经历。让说一下觉得最有有挑战性的工作？实习和比赛都很水，所以实话实说没啥亮点。\n算法题一道，写一下快排。当时代码没有跑起来，因为vscode好像更新了code runner的配置，所以没有跑起来，不过写的代码大概率全是bug，代码能力也是硬伤。\nclass Solution { public: int qsort(vector\u0026lt;int\u0026gt;\u0026amp; nums,int left,int right){ int l = left; int r = right; if(l\u0026gt;=r){ return l; } int randNum = rand()%(r-l+1)+l; int temp = nums[randNum]; nums[randNum] = nums[l]; nums[l] = temp; int pivot = nums[l]; while(l\u0026lt;r){ while(l\u0026lt;r\u0026amp;\u0026amp;nums[r]\u0026gt;pivot){ r--; } nums[l] = nums[r]; while(l\u0026lt;r\u0026amp;\u0026amp;nums[l]\u0026lt;=pivot){ l++; } nums[r] = nums[l]; } nums[l] = pivot; qsort(nums,left,l-1); qsort(nums,l+1,right); return l; } vector\u0026lt;int\u0026gt; sortArray(vector\u0026lt;int\u0026gt;\u0026amp; nums) { srand((unsigned)time(NULL)); qsort(nums,0,(int)nums.size()-1); return nums; } }; 然后问了一些基础问题：\n  Java和C++的区别有哪些\n  类的重写和重载\n  TCP通信的三次握手和四次挥手，为什么不能多一次或者少一次？\n  TCP长连接和短链接\n  TCP和UDP的区别\n  了解Cache吗？我答非所问，回答了操作系统的cache。\n  其实有可能是问http缓存\n  输入url地址浏览器的变化\n  问有么有用过数据库，对数据库了解多少，回答用过MySQL\n  为什么要用数据库？\n  简单介绍一下云计算是什么，为什么要用云计算\n  面试总结 面完就知道自己应该是凉了，果然12月1号收到了感谢信。其实还是老问题，自己的基础知识还是要在巩固一下可能要多看看面经，另外算法什么的还是要多加练习。\n后来涛神告诉我是HR觉得只有一个月多的实习时间太短了，不好安排。\n","date":"2021-12-01T20:56:54+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/image-20211201214555354.png","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E8%93%9D%E6%B9%96%E9%9D%A2%E8%AF%95%E5%87%89%E7%BB%8F/","title":"【实习面试】蓝湖面试凉经"},{"content":" 时间：2021-11-29下午三点\n方式：飞书视频面试\n岗位：后端开发实习生-业务中台职位\n 为什么会投递字节实习 ​\t因为中科院的研究所大多不让实习，研二想去实习是一定不行的。但是研一在雁栖湖，课题组在海淀，我研一基本不参与科研，所以想要趁没人管的时候出去实习。因为将来大概率是做软开，所以想找一份后端开发的工作。在校友群里问了一下，有学长给了一个内推的机会，所以就有了这次一面。\n面试过程 ​\t面试官上来让我做了自我介绍之后就让我写代码做题。\n 题目一：链表1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;6 转变成 链表1-\u0026gt;6-\u0026gt;2-\u0026gt;5-\u0026gt;3-\u0026gt;4\n大概意思就是将链表平分成两部分，前一部分顺序不变，后一部分反转后插空插入前一部分链表。\n ​\t说实话，自己的代码能力确实有点差，原本打算使用c++来写后来改成了Java，因为我想起Java里有LinkedList这个数据结构。理由有点荒诞哈，其实是我没有意识到要自己定义节点来实现链表，所以在想要写指针的时候就卡住了，完全不知道怎么给LinkedList的链表写指针。然后就尬住了，面试官无奈说那就换一道吧。\n 题目二：判断一个二叉树是否是平衡树\n ​\t题目不是很难，但是自己太久不写代码有些生疏了，写出来的程序不知道为什么没有编译通过。说实话，面试官让我自己实现一个单例来测试程序，其中爆出的各种错误，无一不揭示了我完全没有什么开发经验的事实，例如内部类放错了位置，static的编译问题，还有一个空指针异常。无疑是再次尬死了。\n​\t之后面试官让我讲一讲自己的项目经历，主要问了一下大三在华为的实习。可惜自己虽然实习了，但实际的工作很少，含金量也不高，面试官兴趣不大。\n​\t之后就是问我有没有什么想问他的，我已经不想说话了，就没问。\n​\t草草结束。\n总结 首先还是把这两道题的代码写写吧 题目一 /** * @author lizhixuan * @version 1.0 * @date 2021/12/1 15:50 */ public class ReverseList2 { public static class ListNode { int val; ListNode next; ListNode() {} ListNode(int val) { this.val = val; } ListNode(int val, ListNode next) { this.val = val; this.next = next; } } public static ListNode createInstance(int n){ ListNode head = new ListNode(1); ListNode current = head; for (int i = 2; i \u0026lt;= n; i++) { current.next = new ListNode(i); current = current.next; } return head; } public static void printList(ListNode head){ ListNode current = head; while(current.next!=null){ System.out.print(current.val); System.out.print(\u0026#34;-\u0026gt;\u0026#34;); current = current.next; } System.out.println(current.val); } public static ListNode reverse(ListNode head){ ListNode pre = null; ListNode current = head; ListNode next; while(current!=null){ next = current.next; current.next = pre; pre = current; current = next; } return pre; } public static ListNode mergeList(ListNode l1,ListNode l2){ ListNode head = l1; ListNode l1Next; ListNode l2Next; while(l1!=null\u0026amp;\u0026amp;l2!=null){ l1Next = l1.next; l2Next = l2.next; l1.next = l2; l2.next = l1Next; l1 = l1Next; l2 = l2Next; } return head; } public static ListNode reverseHalf(ListNode head){ ListNode fast = head; ListNode slow = head; while(fast != null){ if(fast.next == null){ break; } if(fast.next.next == null){ break; } fast = fast.next.next; slow = slow.next; } ListNode half = reverse(slow.next); slow.next = null; return mergeList(head,half); } public static void main(String[] args) { ListNode head = createInstance(7); printList(head); head = reverseHalf(head); printList(head); } } 题目二 import java.util.LinkedList; import java.util.Queue; /** * @author lizhixuan * @version 1.0 * @date 2021/12/1 17:24 */ public class BalanceCheck { public static class TreeNode{ int val; TreeNode left; TreeNode right; TreeNode(int val){this.val = val;} } public static TreeNode listToTree(String src){ src = src.substring(1,src.length()-1); String[] strList = src.split(\u0026#34;,\u0026#34;); TreeNode root ; TreeNode result = null; Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); for (int i =0 ; i\u0026lt; strList.length ; i++){ if (i == 0){ root = new TreeNode(Integer.parseInt(strList[i])); result = root; queue.add(root); } if (!queue.isEmpty()){ root = queue.poll(); }else { break; } if ( i+1 \u0026lt; strList.length \u0026amp;\u0026amp; !strList[i+1].equals( \u0026#34;null\u0026#34;)){ root.left = new TreeNode(Integer.parseInt(strList[i +1])); queue.add(root.left); } if ( i + 2 \u0026lt; strList.length \u0026amp;\u0026amp; !strList[i+2].equals( \u0026#34;null\u0026#34;)){ root.right = new TreeNode(Integer.parseInt(strList[i +2])); queue.add(root.right); } i = i +1; } return result; } public static int getHeight(TreeNode root){ if(root==null){ return 0; } return Math.max(getHeight(root.left),getHeight(root.right))+1; } public static boolean isBalanced(TreeNode root){ if(root==null){ return true; } int leftHeight = getHeight(root.left); int rightHeight = getHeight(root.right); return Math.abs(leftHeight-rightHeight)\u0026lt;=1 \u0026amp;\u0026amp; isBalanced(root.left) \u0026amp;\u0026amp; isBalanced(root.right); } public static void main(String[] args) { String tree = \u0026#34;[3,9,20,null,null,15,7]\u0026#34;; TreeNode root = listToTree(tree); System.out.println(isBalanced(root)); } } 一些知识点   java内部类：\n 成员内部类中不能存在任何static的变量和方法 成员内部类是依附于外围类的，所以只有先创建了外围类才能够创建内部类    java的static关键字\n static变量也称作静态变量，静态变量被所有的对象所共享，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化。 为什么说static块可以用来优化程序性能，是因为它的特性:只会在类加载的时候执行一次。 在C/C++中static是可以作用域局部变量的，但是在Java中切记：static是不允许用来修饰局部变量。    反思  自己还是缺乏代码经验，代码写不出来，面试官明显十分失望，失去耐心之后多次叹气捂脸，说实话压力还是挺大的，毕竟确实挺丢脸的。以后也要多写一点代码。 打算把自己的学习总结下来，于是也有了这篇博客，希望自己可以坚持写博客，说实话，面试完压力挺大，感觉自己就是一个无敌铁废物，写博客写出来感觉好一些了。  ","date":"2021-11-29T16:52:26+08:00","image":"https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/bytedance.jpg","permalink":"https://tweakzx.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AD%97%E8%8A%82%E9%9D%A2%E8%AF%95%E5%87%89%E7%BB%8F/","title":"【实习面试】字节面试凉经"}]