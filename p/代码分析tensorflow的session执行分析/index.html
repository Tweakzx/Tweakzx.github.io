<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Tensorflow kernal launch 的过程 分析session执行的过程， 并分析Antman对执行过程的修改 函数调用链 Run()&amp;ndash;&amp;gt;RunInternel()&amp;ndash;&amp;gt;RunAsync()&amp;ndash;&amp;gt;ScheduleReady()&amp;ndash;&amp;gt;Process() 修改了direct_session.cc , 在sess'><title>【代码分析】Tensorflow的session执行分析</title>

<link rel='canonical' href='https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90tensorflow%E7%9A%84session%E6%89%A7%E8%A1%8C%E5%88%86%E6%9E%90/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='【代码分析】Tensorflow的session执行分析'>
<meta property='og:description' content='Tensorflow kernal launch 的过程 分析session执行的过程， 并分析Antman对执行过程的修改 函数调用链 Run()&amp;ndash;&amp;gt;RunInternel()&amp;ndash;&amp;gt;RunAsync()&amp;ndash;&amp;gt;ScheduleReady()&amp;ndash;&amp;gt;Process() 修改了direct_session.cc , 在sess'>
<meta property='og:url' content='https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90tensorflow%E7%9A%84session%E6%89%A7%E8%A1%8C%E5%88%86%E6%9E%90/'>
<meta property='og:site_name' content='Tweakzx'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Tensorflow' /><meta property='article:tag' content='GPU' /><meta property='article:tag' content='Antman' /><meta property='article:published_time' content='2022-12-23T18:57:14&#43;08:00'/><meta property='article:modified_time' content='2022-12-23T18:57:14&#43;08:00'/>
<meta name="twitter:title" content="【代码分析】Tensorflow的session执行分析">
<meta name="twitter:description" content="Tensorflow kernal launch 的过程 分析session执行的过程， 并分析Antman对执行过程的修改 函数调用链 Run()&amp;ndash;&amp;gt;RunInternel()&amp;ndash;&amp;gt;RunAsync()&amp;ndash;&amp;gt;ScheduleReady()&amp;ndash;&amp;gt;Process() 修改了direct_session.cc , 在sess">
    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/" >
                代码分析
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90tensorflow%E7%9A%84session%E6%89%A7%E8%A1%8C%E5%88%86%E6%9E%90/">【代码分析】Tensorflow的session执行分析</a>
    </h2>

    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Dec 23, 2022</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    6 minute read
                </time>
            </div>
        
    </footer>
    
</div>

    

</header>

    <section class="article-content">
    <h1 id="tensorflow-kernal-launch-的过程">Tensorflow kernal launch 的过程</h1>
<p>分析session执行的过程， 并分析Antman对执行过程的修改</p>
<p>函数调用链 Run()&ndash;&gt;RunInternel()&ndash;&gt;RunAsync()&ndash;&gt;ScheduleReady()&ndash;&gt;Process()</p>
<p>修改了direct_session.cc ,  在session执行前后运行中间件框架</p>
<p>修改了executor.cc ， 新增一个异步调用队列， 并将需要插入时间槽的异步Op加入队列，在OpManager线程中等待执行。</p>
<p><figure 
	>
	<a href="https://shengyg.github.io/repository/assets/pic/tf-architecture.png" >
		<img src="https://shengyg.github.io/repository/assets/pic/tf-architecture.png"
			
			
			
			loading="lazy"
			alt="img">
	</a>
	
	<figcaption>img</figcaption>
	
</figure></p>
<h2 id="session的执行">Session的执行</h2>
<p>Session的代码逻辑在TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/direct_session.cc的Run()函数中，</p>
<h3 id="directsessionrun">DirectSession::Run</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="n">Status</span> <span class="n">DirectSession</span><span class="o">::</span><span class="n">Run</span><span class="p">(</span><span class="k">const</span> <span class="n">RunOptions</span><span class="o">&amp;</span> <span class="n">run_options</span><span class="p">,</span>
                          <span class="k">const</span> <span class="n">NamedTensorList</span><span class="o">&amp;</span> <span class="n">inputs</span><span class="p">,</span>
                          <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;&amp;</span> <span class="n">output_names</span><span class="p">,</span>
                          <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;&amp;</span> <span class="n">target_nodes</span><span class="p">,</span>
                          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;*</span> <span class="n">outputs</span><span class="p">,</span>
                          <span class="n">RunMetadata</span><span class="o">*</span> <span class="n">run_metadata</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">//判断计算图是否构建
</span><span class="c1"></span>  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">CheckNotClosed</span><span class="p">());</span>
  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">CheckGraphCreated</span><span class="p">(</span><span class="s">&#34;Run()&#34;</span><span class="p">));</span>
  <span class="c1">//计数器
</span><span class="c1"></span>  <span class="n">direct_session_runs</span><span class="o">-&gt;</span><span class="n">GetCell</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">IncrementBy</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

  <span class="c1">// Extract the inputs names for this run of the session.
</span><span class="c1"></span>  <span class="c1">//提取输入的张量名字和张量大小
</span><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">input_tensor_names</span><span class="p">;</span>
  <span class="n">input_tensor_names</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="n">size_t</span> <span class="n">input_size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">input_tensor_names</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">);</span>
    <span class="n">input_size</span> <span class="o">+=</span> <span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">AllocatedBytes</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="n">metrics</span><span class="o">::</span><span class="n">RecordGraphInputTensors</span><span class="p">(</span><span class="n">input_size</span><span class="p">);</span>

  <span class="c1">// Check if we already have an executor for these arguments.
</span><span class="c1"></span>  <span class="c1">// 检查是否已经创建执行器， 没有的话， 创建
</span><span class="c1"></span>  <span class="c1">// 一般情况下 每个设备都有一个执行器， 负责这个设备上计算子图的执行
</span><span class="c1"></span>  <span class="n">ExecutorsAndKeys</span><span class="o">*</span> <span class="n">executors_and_keys</span><span class="p">;</span>
  <span class="n">RunStateArgs</span> <span class="nf">run_state_args</span><span class="p">(</span><span class="n">run_options</span><span class="p">.</span><span class="n">debug_options</span><span class="p">());</span>
  <span class="n">run_state_args</span><span class="p">.</span><span class="n">collective_graph_key</span> <span class="o">=</span>
      <span class="n">run_options</span><span class="p">.</span><span class="n">experimental</span><span class="p">().</span><span class="n">collective_graph_key</span><span class="p">();</span>

  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">GetOrCreateExecutors</span><span class="p">(</span><span class="n">input_tensor_names</span><span class="p">,</span> <span class="n">output_names</span><span class="p">,</span>
                                          <span class="n">target_nodes</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">executors_and_keys</span><span class="p">,</span>
                                          <span class="o">&amp;</span><span class="n">run_state_args</span><span class="p">));</span>
  <span class="p">{</span>
    <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">collective_graph_key_lock_</span><span class="p">);</span>
    <span class="n">collective_graph_key_</span> <span class="o">=</span> <span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">collective_graph_key</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Configure a call frame for the step, which we use to feed and
</span><span class="c1"></span>  <span class="c1">// fetch values to and from the executors.
</span><span class="c1"></span>  <span class="c1">//设置函数调用帧的参数， Tensorflow使用feed和fetch字典来和执行器进行数据交互
</span><span class="c1"></span>  <span class="c1">//feed是输入， fetch是输出
</span><span class="c1"></span>  <span class="c1">//构建FunctionCallFrame call_frame,  Session与执行器之间相互交互
</span><span class="c1"></span>  <span class="c1">//处理执行器的输入与输出
</span><span class="c1"></span>  <span class="n">FunctionCallFrame</span> <span class="nf">call_frame</span><span class="p">(</span><span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">input_types</span><span class="p">,</span>
                               <span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">output_types</span><span class="p">);</span>
  <span class="n">gtl</span><span class="o">::</span><span class="n">InlinedVector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="mi">4</span><span class="o">&gt;</span> <span class="n">feed_args</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">dtype</span><span class="p">()</span> <span class="o">==</span> <span class="n">DT_RESOURCE</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">Tensor</span> <span class="n">tensor_from_handle</span><span class="p">;</span>
      <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span>
          <span class="n">ResourceHandleToInputTensor</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">tensor_from_handle</span><span class="p">));</span>
      <span class="n">feed_args</span><span class="p">[</span><span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">input_name_to_index</span><span class="p">[</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">]]</span> <span class="o">=</span>
          <span class="n">tensor_from_handle</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">feed_args</span><span class="p">[</span><span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">input_name_to_index</span><span class="p">[</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">]]</span> <span class="o">=</span> <span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="c1">// 设置输入参数
</span><span class="c1"></span>  <span class="k">const</span> <span class="n">Status</span> <span class="n">s</span> <span class="o">=</span> <span class="n">call_frame</span><span class="p">.</span><span class="n">SetArgs</span><span class="p">(</span><span class="n">feed_args</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">errors</span><span class="o">::</span><span class="n">IsInternal</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">errors</span><span class="o">::</span><span class="n">InvalidArgument</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">error_message</span><span class="p">());</span>
  <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="o">!</span><span class="n">s</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">s</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">const</span> <span class="n">int64</span> <span class="n">step_id</span> <span class="o">=</span> <span class="n">step_id_counter_</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">LogMemory</span><span class="o">::</span><span class="n">IsEnabled</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">LogMemory</span><span class="o">::</span><span class="n">RecordStep</span><span class="p">(</span><span class="n">step_id</span><span class="p">,</span> <span class="n">run_state_args</span><span class="p">.</span><span class="n">handle</span><span class="p">);</span>
  <span class="p">}</span>
<span class="c1">//准备好执行环境之后， 开始调用RunInternal执行计算
</span><span class="c1"></span>  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">RunInternal</span><span class="p">(</span><span class="n">step_id</span><span class="p">,</span> <span class="n">run_options</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">call_frame</span><span class="p">,</span>
                                 <span class="n">executors_and_keys</span><span class="p">,</span> <span class="n">run_metadata</span><span class="p">,</span>
                                 <span class="kr">thread</span><span class="o">::</span><span class="n">ThreadPoolOptions</span><span class="p">()));</span>

  <span class="p">...</span> <span class="p">...</span>
<span class="c1">// 获取并处理计算图的执行结果
</span><span class="c1"></span><span class="p">}</span>
</code></pre></div><h3 id="directsessionruninternal">DirectSession::RunInternal()</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="c1">// RunInternal会启动多个并行的执行器， 
</span><span class="c1">// 创建执行器的barrier， 确保执行器都执行完， 执行完后返回Run()函数
</span><span class="c1"></span><span class="n">Status</span> <span class="n">DirectSession</span><span class="o">::</span><span class="n">RunInternal</span><span class="p">(</span>
    <span class="n">int64</span> <span class="n">step_id</span><span class="p">,</span> <span class="k">const</span> <span class="n">RunOptions</span><span class="o">&amp;</span> <span class="n">run_options</span><span class="p">,</span>
    <span class="n">CallFrameInterface</span><span class="o">*</span> <span class="n">call_frame</span><span class="p">,</span> <span class="n">ExecutorsAndKeys</span><span class="o">*</span> <span class="n">executors_and_keys</span><span class="p">,</span>
    <span class="n">RunMetadata</span><span class="o">*</span> <span class="n">run_metadata</span><span class="p">,</span>
    <span class="k">const</span> <span class="kr">thread</span><span class="o">::</span><span class="n">ThreadPoolOptions</span><span class="o">&amp;</span> <span class="n">threadpool_options</span><span class="p">)</span> <span class="p">{</span>
  
  <span class="k">const</span> <span class="n">uint64</span> <span class="n">start_time_usecs</span> <span class="o">=</span> <span class="n">options_</span><span class="p">.</span><span class="n">env</span><span class="o">-&gt;</span><span class="n">NowMicros</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">int64</span> <span class="n">executor_step_count</span> <span class="o">=</span> <span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">step_count</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
  <span class="c1">////////////////////////////////////////////////////////
</span><span class="c1"></span>  <span class="c1">// Running all pre session run action in grouping     //
</span><span class="c1"></span>  <span class="c1">// 在session计算执行之前添加SessionRunActionRegistry     //
</span><span class="c1"></span>  <span class="c1">// 以运行在session开始之前的中间件                        //
</span><span class="c1"></span>  <span class="c1">////////////////////////////////////////////////////////
</span><span class="c1"></span>  <span class="n">SessionRunActionOptions</span> <span class="n">action_options</span><span class="p">;</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">device_mgr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">device_mgr_</span><span class="p">;</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">sess_ptr</span> <span class="o">=</span> <span class="k">this</span><span class="p">;</span>
  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">Global</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">RunGrouping</span><span class="p">(</span>
      <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">PRE_SESSION_RUN</span><span class="p">,</span> <span class="n">action_options</span><span class="p">));</span>
 <span class="c1">//
</span><span class="c1"></span> <span class="c1">//
</span><span class="c1"></span> <span class="c1">//标记运行状态
</span><span class="c1"></span>  <span class="n">RunState</span> <span class="nf">run_state</span><span class="p">(</span><span class="n">step_id</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">devices_</span><span class="p">);</span>
    
 <span class="p">...</span> <span class="p">...</span> <span class="c1">// profiler TraceMe 
</span><span class="c1"></span>     
 <span class="c1">//构建 IntraProcessRendezvous 用于本地Tensor管理
</span><span class="c1"></span>  <span class="n">run_state</span><span class="p">.</span><span class="n">rendez</span> <span class="o">=</span> <span class="k">new</span> <span class="n">IntraProcessRendezvous</span><span class="p">(</span><span class="n">device_mgr_</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
    
 <span class="p">...</span> <span class="p">...</span> <span class="c1">// ifndef _ANDROID 
</span><span class="c1"></span>     
  <span class="c1">// Start parallel Executors.
</span><span class="c1"></span>  <span class="c1">//开始并行执行器
</span><span class="c1"></span>  <span class="c1">//构建 ExecutorBarrier 用于协调多个 Executor 并行计算，保持 graph 一致性
</span><span class="c1"></span>  <span class="k">const</span> <span class="n">size_t</span> <span class="n">num_executors</span> <span class="o">=</span> <span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">items</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
  <span class="n">ExecutorBarrier</span><span class="o">*</span> <span class="n">barrier</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ExecutorBarrier</span><span class="p">(</span>
      <span class="n">num_executors</span><span class="p">,</span> <span class="n">run_state</span><span class="p">.</span><span class="n">rendez</span><span class="p">,</span> <span class="p">[</span><span class="o">&amp;</span><span class="n">run_state</span><span class="p">](</span><span class="k">const</span> <span class="n">Status</span><span class="o">&amp;</span> <span class="n">ret</span><span class="p">)</span> <span class="p">{</span>
        <span class="p">{</span>
          <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">run_state</span><span class="p">.</span><span class="n">mu_</span><span class="p">);</span>
          <span class="n">run_state</span><span class="p">.</span><span class="n">status</span><span class="p">.</span><span class="n">Update</span><span class="p">(</span><span class="n">ret</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">run_state</span><span class="p">.</span><span class="n">executors_done</span><span class="p">.</span><span class="n">Notify</span><span class="p">();</span>
      <span class="p">});</span>

  <span class="p">...</span> <span class="p">...</span>  <span class="c1">//构建args
</span><span class="c1"></span>      
  <span class="c1">// Register this step with session&#39;s cancellation manager, so that
</span><span class="c1"></span>  <span class="c1">// `Session::Close()` will cancel the step.
</span><span class="c1"></span> <span class="p">...</span> <span class="p">...</span><span class="c1">//处理`Session::Close()`
</span><span class="c1"></span>   
  <span class="c1">// Use std::unique_ptr to ensure garbage collection
</span><span class="c1"></span>  <span class="c1">//创建线程池实际运行执行器
</span><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="kr">thread</span><span class="o">::</span><span class="n">ThreadPool</span><span class="o">&gt;</span> <span class="n">threadpool_wrapper</span><span class="p">;</span>
  <span class="kr">thread</span><span class="o">::</span><span class="n">ThreadPool</span><span class="o">*</span> <span class="n">pool</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
  <span class="p">...</span><span class="c1">//设置线程池
</span><span class="c1"></span>      
  <span class="c1">//异步启动执行器
</span><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="nl">item</span> <span class="p">:</span> <span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">items</span><span class="p">)</span> <span class="p">{</span>
    <span class="kr">thread</span><span class="o">::</span><span class="n">ThreadPool</span><span class="o">*</span> <span class="n">device_thread_pool</span> <span class="o">=</span>
        <span class="n">item</span><span class="p">.</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">tensorflow_device_thread_pool</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">device_thread_pool</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">args</span><span class="p">.</span><span class="n">runner</span> <span class="o">=</span> <span class="n">default_runner</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">args</span><span class="p">.</span><span class="n">runner</span> <span class="o">=</span> <span class="p">[</span><span class="k">this</span><span class="p">,</span> <span class="n">device_thread_pool</span><span class="p">](</span><span class="n">Executor</span><span class="o">::</span><span class="n">Args</span><span class="o">::</span><span class="n">Closure</span> <span class="n">c</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">device_thread_pool</span><span class="o">-&gt;</span><span class="n">Schedule</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">c</span><span class="p">));</span>
      <span class="p">};</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">handler</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">args</span><span class="p">.</span><span class="n">user_intra_op_threadpool</span> <span class="o">=</span> <span class="n">handler</span><span class="o">-&gt;</span><span class="n">AsIntraThreadPoolInterface</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="c1">/////////////// 执行器的启动///////////////////
</span><span class="c1"></span>    <span class="n">item</span><span class="p">.</span><span class="n">executor</span><span class="o">-&gt;</span><span class="n">RunAsync</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">barrier</span><span class="o">-&gt;</span><span class="n">Get</span><span class="p">());</span>
    <span class="c1">/////////////////////////////////////////////
</span><span class="c1"></span>  <span class="p">}</span>
  <span class="c1">//等待执行结果
</span><span class="c1"></span>  <span class="n">WaitForNotification</span><span class="p">(</span><span class="o">&amp;</span><span class="n">run_state</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">step_cancellation_manager</span><span class="p">,</span>
                      <span class="n">run_options</span><span class="p">.</span><span class="n">timeout_in_ms</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
                          <span class="o">?</span> <span class="n">run_options</span><span class="p">.</span><span class="n">timeout_in_ms</span><span class="p">()</span>
                          <span class="o">:</span> <span class="n">operation_timeout_in_ms_</span><span class="p">);</span>
  <span class="p">...</span> <span class="p">...</span>
  <span class="c1">//保存运行结果
</span><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">run_state</span><span class="p">.</span><span class="n">tensor_store</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">run_state</span><span class="p">.</span><span class="n">tensor_store</span><span class="p">.</span><span class="n">SaveTensors</span><span class="p">(</span>
        <span class="p">{</span><span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">callable_options</span><span class="p">.</span><span class="n">fetch</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span>
         <span class="n">executors_and_keys</span><span class="o">-&gt;</span><span class="n">callable_options</span><span class="p">.</span><span class="n">fetch</span><span class="p">().</span><span class="n">end</span><span class="p">()},</span>
        <span class="o">&amp;</span><span class="n">session_state_</span><span class="p">));</span>
  <span class="p">}</span>
  <span class="p">...</span> <span class="p">...</span>
  <span class="c1">///////////////////////////////////////////////////////////
</span><span class="c1"></span>  <span class="c1">// Running all post session run action in grouping       //
</span><span class="c1"></span>  <span class="c1">// 在session计算执行结束之后添加SessionRunActionRegistry，   //
</span><span class="c1"></span>  <span class="c1">// 以运行在session结束之后的中间件                           //  
</span><span class="c1"></span>  <span class="c1">///////////////////////////////////////////////////////////
</span><span class="c1"></span>  <span class="n">uint64</span> <span class="n">session_end_time</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">::</span><span class="n">Env</span><span class="o">::</span><span class="n">Default</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">NowMicros</span><span class="p">();</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">sess_duration_us</span> <span class="o">=</span> <span class="n">time_duration_usecs</span><span class="p">;</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">graph_id</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">uint64</span><span class="o">&gt;</span><span class="p">(</span><span class="n">executors_and_keys</span><span class="p">);</span>
  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">Global</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">RunGrouping</span><span class="p">(</span>
      <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">POST_SESSION_RUN</span><span class="p">,</span> <span class="n">action_options</span><span class="p">));</span>

  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div><h2 id="执行器逻辑">执行器逻辑</h2>
<h3 id="executorstaterunasyn">ExecutorState::RunAsyn()</h3>
<div class="highlight"><pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="cp"># ExecutorState::RunAsync 的实现
</span><span class="cp"># 概述：初始化ready队列， 开启线程池
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">ExecutorState</span><span class="o">::</span><span class="n">RunAsync</span><span class="p">(</span><span class="n">Executor</span><span class="o">::</span><span class="n">DoneCallback</span> <span class="n">done</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Graph</span><span class="o">*</span> <span class="n">graph</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">graph_</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
  <span class="n">TaggedNodeSeq</span> <span class="n">ready</span><span class="p">;</span>

  <span class="c1">// 获取 context map，即运行时上下文
</span><span class="c1"></span>  <span class="n">Device</span><span class="o">*</span> <span class="n">device</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">params_</span><span class="p">.</span><span class="n">device</span><span class="p">;</span>
  <span class="k">const</span> <span class="n">Status</span> <span class="n">fill_status</span> <span class="o">=</span>
      <span class="n">device</span><span class="o">-&gt;</span><span class="n">FillContextMap</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">device_context_map_</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">fill_status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">done</span><span class="p">(</span><span class="n">fill_status</span><span class="p">);</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// 初始化 ready 队列，即存放入度为0的node
</span><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="n">Node</span><span class="o">*</span> <span class="nl">n</span> <span class="p">:</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">root_nodes_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">DCHECK_EQ</span><span class="p">(</span><span class="n">n</span><span class="o">-&gt;</span><span class="n">in_edges</span><span class="p">().</span><span class="n">size</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">ready</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">TaggedNode</span><span class="p">{</span><span class="n">n</span><span class="p">,</span> <span class="n">root_frame_</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">false</span><span class="p">});</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ready</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">done</span><span class="p">(</span><span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">());</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">num_outstanding_ops_</span> <span class="o">=</span> <span class="n">ready</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
    <span class="n">root_frame_</span><span class="o">-&gt;</span><span class="n">iterations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">outstanding_ops</span> <span class="o">=</span> <span class="n">ready</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
    <span class="n">done_cb_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">done</span><span class="p">);</span>
    <span class="c1">// 线程池入口
</span><span class="c1"></span>    <span class="n">ScheduleReady</span><span class="p">(</span><span class="n">ready</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><h3 id="executorstatescheduleready">ExecutorState::ScheduleReady()</h3>
<div class="highlight"><pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="cp"># ExecutorState::ScheduleReady 的实现
</span><span class="cp"># 概述：将节点分为 expensive &amp; inexpensive 节点，将inexpensive节点放入 inline_ready 中
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">ExecutorState</span><span class="o">::</span><span class="n">ScheduleReady</span><span class="p">(</span><span class="k">const</span> <span class="n">TaggedNodeSeq</span><span class="o">&amp;</span> <span class="n">ready</span><span class="p">,</span>
                                  <span class="n">TaggedNodeReadyQueue</span><span class="o">*</span> <span class="n">inline_ready</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ready</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="k">return</span><span class="p">;</span>

  <span class="n">int64</span> <span class="n">scheduled_usec</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">stats_collector_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">scheduled_usec</span> <span class="o">=</span> <span class="n">nodestats</span><span class="o">::</span><span class="n">NowInUsec</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">inline_ready</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 运行所有 ready ops
</span><span class="c1"></span>	<span class="c1">// 运行ready队列里的节点 ready是当前线程要处理的队列
</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="nl">tagged_node</span> <span class="p">:</span> <span class="n">ready</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">runner_</span><span class="p">([</span><span class="o">=</span><span class="p">]()</span> <span class="p">{</span> <span class="n">Process</span><span class="p">(</span><span class="n">tagged_node</span><span class="p">,</span> <span class="n">scheduled_usec</span><span class="p">);</span> <span class="p">});</span>
    <span class="p">}</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// 将节点分类，运行 expensive node
</span><span class="c1"></span>  <span class="k">const</span> <span class="n">GraphView</span><span class="o">&amp;</span> <span class="n">gview</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">gview_</span><span class="p">;</span>
  <span class="k">const</span> <span class="n">TaggedNode</span><span class="o">*</span> <span class="n">curr_expensive_node</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="nl">tagged_node</span> <span class="p">:</span> <span class="n">ready</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">NodeItem</span><span class="o">&amp;</span> <span class="n">item</span> <span class="o">=</span> <span class="o">*</span><span class="n">gview</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">id</span><span class="p">());</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">is_dead</span> <span class="o">||</span> <span class="o">!</span><span class="n">item</span><span class="p">.</span><span class="n">kernel_is_expensive</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">//
</span><span class="c1"></span>      <span class="n">inline_ready</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="n">tagged_node</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="c1">//对于高开销节点启动新的线程去执行
</span><span class="c1"></span>      <span class="k">if</span> <span class="p">(</span><span class="n">curr_expensive_node</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">runner_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ExecutorState</span><span class="o">::</span><span class="n">Process</span><span class="p">,</span> <span class="k">this</span><span class="p">,</span> <span class="o">*</span><span class="n">curr_expensive_node</span><span class="p">,</span>
                          <span class="n">scheduled_usec</span><span class="p">));</span>
      <span class="p">}</span>
      <span class="n">curr_expensive_node</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">tagged_node</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">curr_expensive_node</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">//高开销节点
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">inline_ready</span><span class="o">-&gt;</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
      <span class="c1">// inline_ready为空， 将首个高开销节点放入inline_ready
</span><span class="c1"></span>      <span class="n">inline_ready</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="o">*</span><span class="n">curr_expensive_node</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="c1">// inline_ready不为空， 将高开销节点放入其他线程中执行
</span><span class="c1"></span>      <span class="n">runner_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ExecutorState</span><span class="o">::</span><span class="n">Process</span><span class="p">,</span> <span class="k">this</span><span class="p">,</span> <span class="o">*</span><span class="n">curr_expensive_node</span><span class="p">,</span>
                        <span class="n">scheduled_usec</span><span class="p">));</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="p">...</span> <span class="p">...</span>
<span class="p">}</span>
</code></pre></div><h3 id="executorstateprocess">ExecutorState::Process()</h3>
<div class="highlight"><pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="cp"># ExecutorState::Process 详解
</span><span class="cp"># 概述：线程池中跑的内容，代码太长不贴了。
</span><span class="cp"># 主要流程：
</span><span class="cp">#       + 将当前节点添加到 inline_ready 队列中。
</span><span class="cp">#       + 循环从 inline_ready 队列获取节点并运行，运行完毕后执行 NodeDone（有可能会添加新节点到inline_ready队列）
</span><span class="cp">#       + 当inline ready队列为空时，跳出循环。
</span><span class="cp"># 其他重要内容：
</span><span class="cp">#       + 运行节点通过 device 的 ComputeAsync 或 Compute 方法
</span><span class="cp">#       + 处理输出结果使用 ProcessOutputs 函数和 PropagateOutputs 函数
</span><span class="cp">#       + 计算结束后通过 NodeDone 来收尾
</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">ExecutorState</span><span class="o">::</span><span class="n">Process</span><span class="p">(</span><span class="n">TaggedNode</span> <span class="n">tagged_node</span><span class="p">,</span> <span class="n">int64</span> <span class="n">scheduled_nsec</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">WithContext</span> <span class="nf">wc</span><span class="p">(</span><span class="n">context_</span><span class="p">);</span>
   <span class="p">...</span> <span class="p">...</span>
  <span class="c1">// Parameters passed to OpKernel::Compute.
</span><span class="c1"></span>  <span class="n">TensorValueVec</span> <span class="n">inputs</span><span class="p">;</span>
  <span class="n">DeviceContextVec</span> <span class="n">input_device_contexts</span><span class="p">;</span>
  <span class="n">AllocatorAttributeVec</span> <span class="n">input_alloc_attrs</span><span class="p">;</span>

  <span class="n">OpKernelContext</span><span class="o">::</span><span class="n">Params</span> <span class="n">params</span><span class="p">;</span>
  <span class="n">params</span><span class="p">.</span><span class="n">step_id</span> <span class="o">=</span> <span class="n">step_id_</span><span class="p">;</span>
  <span class="c1">// Override device&#39;s threadpool if user provides an intra_op_threadpool
</span><span class="c1"></span>  <span class="n">Device</span><span class="o">*</span> <span class="n">device</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">params_</span><span class="p">.</span><span class="n">device</span><span class="p">;</span>
  <span class="p">...</span> <span class="p">...</span>
  <span class="kt">bool</span> <span class="n">completed</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="n">inline_ready</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">tagged_node</span><span class="p">);</span>
  <span class="n">uint64</span> <span class="n">sess_op_num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="c1">//循环处理inline_ready中的每个节点 直到为空
</span><span class="c1"></span>  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">IsAsyncGPUOpQueueEmpty</span><span class="p">()</span> <span class="o">||</span> <span class="o">!</span><span class="n">inline_ready</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>

    <span class="n">tagged_node</span> <span class="o">=</span> <span class="n">inline_ready</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
    <span class="n">inline_ready</span><span class="p">.</span><span class="n">pop_front</span><span class="p">();</span>
    <span class="p">...</span> <span class="p">...</span>
      <span class="c1">//准备输入数据， 确保输入是有效的
</span><span class="c1"></span>      <span class="n">s</span> <span class="o">=</span> <span class="n">PrepareInputs</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">first_input</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">input_device_contexts</span><span class="p">,</span>
                        <span class="o">&amp;</span><span class="n">input_alloc_attrs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">is_input_dead</span><span class="p">);</span>
      <span class="p">...</span> <span class="p">...</span>
      <span class="c1">// 绝大多数的Op是同步计算模式， send/recv是异步计算模式
</span><span class="c1"></span>      <span class="k">if</span> <span class="p">(</span><span class="n">item</span><span class="p">.</span><span class="n">kernel_is_async</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">//异步计算, send/recv是高开销的
</span><span class="c1"></span>        <span class="n">launched_asynchronously</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
        <span class="n">Device</span><span class="o">*</span> <span class="n">kernel_device</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">params_</span><span class="p">.</span><span class="n">device</span><span class="p">;</span>
        <span class="c1">// Only enqueue this op if it is an async GPU op.
</span><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">need_to_insert_idle_time_</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">kernel_device</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()).</span><span class="n">find</span><span class="p">(</span><span class="s">&#34;GPU&#34;</span><span class="p">)</span> <span class="o">!=</span> <span class="n">string</span><span class="o">::</span><span class="n">npos</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">//////////////////////////////////////////////////////////////////////
</span><span class="c1"></span>          <span class="c1">// 把这个GPU Op放入async_gpu_op_queue队列中 如果需要在它启动之前插入时间槽的话
</span><span class="c1"></span>          <span class="c1">// Enqueue this GPU op therefore we can insert a time slot before launching this op.
</span><span class="c1"></span>          <span class="c1">// 将原本执行异步计算代码的Op放入自定义的async_gpu_op_queue队列中， 
</span><span class="c1"></span>          <span class="c1">// 交由OpManager执行
</span><span class="c1"></span>          <span class="c1">//////////////////////////////////////////////////////////////////////
</span><span class="c1"></span>          <span class="n">sess_op_num</span><span class="o">++</span><span class="p">;</span>
			<span class="p">...</span> <span class="p">...</span>
          <span class="c1">// Enqueue this asyn GPU op.
</span><span class="c1"></span>          <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
          <span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">async_gpu_kernel</span><span class="p">);</span>
          <span class="n">num_queued_op</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
          <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
          <span class="c1">/////////////////////////////////////////////////////////////////////
</span><span class="c1"></span>          <span class="c1">// 不需要插入时间槽， 所以不放入async_gpu_op_queue队列
</span><span class="c1"></span>          <span class="c1">// Do not enqueue this op.
</span><span class="c1"></span>          <span class="c1">// 调用原本的计算异步的函数
</span><span class="c1"></span>          <span class="c1">/////////////////////////////////////////////////////////////////////
</span><span class="c1"></span>          <span class="n">device</span><span class="o">-&gt;</span><span class="n">ComputeAsync</span><span class="p">(</span><span class="n">async</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">,</span> <span class="n">done</span><span class="p">);</span>
          
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="c1">// 同步计算
</span><span class="c1"></span>            
        <span class="c1">// Synchronous computes.
</span><span class="c1"></span>        <span class="n">OpKernelContext</span> <span class="nf">ctx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">params</span><span class="p">,</span> <span class="n">item</span><span class="p">.</span><span class="n">num_outputs</span><span class="p">);</span>
        <span class="n">nodestats</span><span class="o">::</span><span class="n">SetOpStart</span><span class="p">(</span><span class="n">stats</span><span class="p">);</span>
        <span class="p">...</span> <span class="p">...</span> <span class="c1">//进行计算 deivce-&gt;Compute(op_kernel, &amp;ctx)
</span><span class="c1"></span>        <span class="n">nodestats</span><span class="o">::</span><span class="n">SetOpEnd</span><span class="p">(</span><span class="n">stats</span><span class="p">);</span>
        <span class="c1">//处理输出
</span><span class="c1"></span>        <span class="n">s</span> <span class="o">=</span> <span class="n">ProcessOutputs</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="n">stats</span><span class="p">);</span>
      <span class="p">...</span> <span class="p">...</span>
	  <span class="c1">//传播输出
</span><span class="c1"></span>      <span class="k">if</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
        <span class="n">PropagateOutputs</span><span class="p">(</span><span class="n">tagged_node</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">item</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ready</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="p">...</span> <span class="p">...</span> <span class="c1">//传播后处理
</span><span class="c1"></span>      <span class="c1">//结束
</span><span class="c1"></span>      <span class="n">completed</span> <span class="o">=</span> <span class="n">NodeDone</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">item</span><span class="p">.</span><span class="n">node</span><span class="p">,</span> <span class="n">ready</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">inline_ready</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>  <span class="c1">// while !inline_ready.empty()
</span><span class="c1"></span>
  <span class="k">if</span> <span class="p">(</span><span class="n">sess_op_num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Record the total number of the queued op running in this session.
</span><span class="c1"></span>    <span class="n">GPUResourceManagement</span><span class="o">*</span> <span class="n">rm</span> <span class="o">=</span> <span class="n">GetGPUResourceManagement</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">rm</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">rm</span><span class="o">-&gt;</span><span class="n">SetExecutorQueuedOpNum</span><span class="p">(</span><span class="n">impl_</span><span class="p">,</span> <span class="n">sess_op_num</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="c1">// This thread of computation is done if completed = true.
</span><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">completed</span><span class="p">)</span> <span class="n">ScheduleFinish</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div><h3 id="executorstateasyncgpuopmanager">ExecutorState::AsyncGPUOpManager()</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">ExecutorState</span><span class="o">::</span><span class="n">AsyncGPUOpManager</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">uint64</span> <span class="n">sleep_time_us</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">need_to_insert_idle_time_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="n">GPUResourceManagement</span><span class="o">*</span> <span class="n">rm</span> <span class="o">=</span> <span class="n">GetGPUResourceManagement</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">rm</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">terminate_op_magager_thread_</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">//设置队列中的Op是否需要插入时间槽
</span><span class="c1"></span>    <span class="n">need_to_insert_idle_time_</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="nb">true</span> <span class="o">:</span> <span class="nb">false</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="kt">void</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="o">&gt;</span> <span class="n">queued_call_func</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
    <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">queued_call_func</span> <span class="o">=</span> <span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">queued_call_func</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">queued_call_func</span><span class="p">();</span>
      <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
        <span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
        <span class="n">num_queued_op</span><span class="p">.</span><span class="n">fetch_sub</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>

      <span class="c1">// Estimate idle time
</span><span class="c1"></span>      <span class="n">uint64</span> <span class="n">idle_time</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">();</span>
      <span class="n">uint64</span> <span class="n">queued_op_num</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetExecutorQueuedOpNum</span><span class="p">(</span><span class="n">impl_</span><span class="p">);</span>
      <span class="n">idle_time</span> <span class="o">=</span> <span class="n">queued_op_num</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="p">(</span><span class="n">idle_time</span> <span class="o">/</span> <span class="n">queued_op_num</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
      <span class="n">usleep</span><span class="p">(</span><span class="n">idle_time</span><span class="p">);</span>
      <span class="n">uint64</span> <span class="n">remain_time</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">();</span>
      <span class="n">remain_time</span> <span class="o">=</span> <span class="n">remain_time</span> <span class="o">&gt;</span> <span class="n">idle_time</span> <span class="o">?</span> <span class="p">(</span><span class="n">remain_time</span> <span class="o">-</span> <span class="n">idle_time</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
      <span class="n">rm</span><span class="o">-&gt;</span><span class="n">SetEstimatedIdleTime</span><span class="p">(</span><span class="n">remain_time</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">usleep</span><span class="p">(</span><span class="n">default_check_interval</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div><h2 id="antman对内存分配器的修改">Antman对内存分配器的修改</h2>
<p>主要新增了自己的vmen内存分配器， 调用host的内存</p>
<p>在<a class="link" href="https://sourcegraph.com/github.com/alibaba/GPU-scheduler-for-deep-learning@d6dd4e639aa0f63a6c7473e639b2105681b2fc37/-/blob/TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/gpu/gpu_process_state.cc"  target="_blank" rel="noopener"
    ><strong>TensorFlow-with-dynamic-scaling/tensorflow/core/common_runtime/gpu/gpu_process_state.cc</strong></a><a class="link" href="https://sourcegraph.com/github.com/alibaba/GPU-scheduler-for-deep-learning/-/compare/2820fd7a59a4860c2f220e8ce70a17c8e23a9cb2...d6dd4e639aa0f63a6c7473e639b2105681b2fc37?utm_source=chrome-extension&amp;utm_campaign=open-diff-on-sourcegraph&amp;visible=6#diff-c9e739a63a7b5d26510b9663d33eb2a2"  target="_blank" rel="noopener"
    >#</a>做了修改</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="n">Allocator</span><span class="o">*</span> <span class="n">GPUProcessState</span><span class="o">::</span><span class="n">GetGPUAllocator</span><span class="p">(</span><span class="k">const</span> <span class="n">GPUOptions</span><span class="o">&amp;</span> <span class="n">options</span><span class="p">,</span>
                                            <span class="n">TfGpuId</span> <span class="n">tf_gpu_id</span><span class="p">,</span>
                                            <span class="n">size_t</span> <span class="n">total_bytes</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">CHECK</span><span class="p">(</span><span class="n">process_state_</span><span class="p">);</span>
<span class="cp">#if (defined(GOOGLE_CUDA) &amp;&amp; GOOGLE_CUDA) || \
</span><span class="cp">    (defined(TENSORFLOW_USE_ROCM) &amp;&amp; TENSORFLOW_USE_ROCM)
</span><span class="cp"></span>  <span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">allocator_type</span> <span class="o">=</span> <span class="n">options</span><span class="p">.</span><span class="n">allocator_type</span><span class="p">();</span>
  <span class="n">mutex_lock</span> <span class="nf">lock</span><span class="p">(</span><span class="n">mu_</span><span class="p">);</span>
  <span class="n">GpuIdUtil</span><span class="o">::</span><span class="n">CheckValidTfGpuId</span><span class="p">(</span><span class="n">tf_gpu_id</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">tf_gpu_id</span><span class="p">.</span><span class="n">value</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">int64</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gpu_allocators_</span><span class="p">.</span><span class="n">size</span><span class="p">()))</span> <span class="p">{</span>
    <span class="n">gpu_allocators_</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">tf_gpu_id</span><span class="p">.</span><span class="n">value</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">AllocatorParts</span><span class="o">&amp;</span> <span class="n">allocator_parts</span> <span class="o">=</span> <span class="n">gpu_allocators_</span><span class="p">[</span><span class="n">tf_gpu_id</span><span class="p">.</span><span class="n">value</span><span class="p">()];</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">allocator_parts</span><span class="p">.</span><span class="n">allocator</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Validate allocator types.
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">allocator_type</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">allocator_type</span> <span class="o">!=</span> <span class="s">&#34;BFC&#34;</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Invalid allocator type: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">allocator_type</span><span class="p">;</span>
      <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">PlatformGpuId</span> <span class="n">platform_gpu_id</span><span class="p">;</span>
    <span class="n">TF_CHECK_OK</span><span class="p">(</span><span class="n">GpuIdManager</span><span class="o">::</span><span class="n">TfToPlatformGpuId</span><span class="p">(</span><span class="n">tf_gpu_id</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">platform_gpu_id</span><span class="p">));</span>
    <span class="kt">int</span> <span class="n">bus_id</span> <span class="o">=</span> <span class="n">BusIdForGPU</span><span class="p">(</span><span class="n">tf_gpu_id</span><span class="p">);</span>
    <span class="n">DCHECK_GE</span><span class="p">(</span><span class="n">bus_id</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">bus_id</span> <span class="o">&gt;=</span> <span class="n">gpu_visitors_</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">gpu_visitors_</span><span class="p">.</span><span class="n">push_back</span><span class="p">({});</span>
    <span class="p">}</span>
    <span class="n">se</span><span class="o">::</span><span class="n">StreamExecutor</span><span class="o">*</span> <span class="n">stream_exec</span> <span class="o">=</span>
        <span class="n">GpuIdUtil</span><span class="o">::</span><span class="n">ExecutorForPlatformGpuId</span><span class="p">(</span><span class="n">platform_gpu_id</span><span class="p">).</span><span class="n">ValueOrDie</span><span class="p">();</span>
    <span class="n">GPUMemAllocator</span><span class="o">*</span> <span class="n">sub_allocator</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GPUMemAllocator</span><span class="p">(</span>
        <span class="n">stream_exec</span><span class="p">,</span>
        <span class="n">platform_gpu_id</span><span class="p">,</span>
        <span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">per_process_gpu_memory_fraction</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">1.0</span> <span class="o">||</span>
         <span class="n">options</span><span class="p">.</span><span class="n">experimental</span><span class="p">().</span><span class="n">use_unified_memory</span><span class="p">()),</span>
        <span class="n">gpu_visitors_</span><span class="p">[</span><span class="n">bus_id</span><span class="p">],</span> <span class="p">{});</span>
    <span class="n">GPUBFCAllocator</span><span class="o">*</span> <span class="n">gpu_bfc_allocator</span> <span class="o">=</span>
        <span class="k">new</span> <span class="n">GPUBFCAllocator</span><span class="p">(</span><span class="n">sub_allocator</span><span class="p">,</span> <span class="n">total_bytes</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span>
                            <span class="n">strings</span><span class="o">::</span><span class="n">StrCat</span><span class="p">(</span><span class="s">&#34;GPU_&#34;</span><span class="p">,</span> <span class="n">tf_gpu_id</span><span class="p">.</span><span class="n">value</span><span class="p">(),</span> <span class="s">&#34;_bfc&#34;</span><span class="p">));</span>
    <span class="n">Allocator</span><span class="o">*</span> <span class="n">gpu_allocator</span> <span class="o">=</span> <span class="n">gpu_bfc_allocator</span><span class="p">;</span>
    <span class="c1">// GPUVMemAllocator will allocate host memory as backup after running out of
</span><span class="c1"></span>    <span class="c1">// gpu device memory to avoid OOM failures
</span><span class="c1"></span>      
    <span class="c1">//////////////////////////////////////////////////////////////////////////////////
</span><span class="c1"></span>    <span class="n">gpu_allocator</span> <span class="o">=</span> <span class="n">maybe_create_gpu_vmem_allocator</span><span class="p">(</span><span class="n">gpu_allocator</span><span class="p">,</span>
                                                        <span class="n">bus_id</span><span class="p">,</span>
                                                        <span class="n">platform_gpu_id</span><span class="p">,</span>
                                                        <span class="n">tf_gpu_id</span><span class="p">.</span><span class="n">value</span><span class="p">(),</span>
                                                        <span class="n">stream_exec</span><span class="p">);</span>
    <span class="c1">//////////////////////////////////////////////////////////////////////////////////
</span><span class="c1"></span>      
    <span class="n">SharedCounter</span><span class="o">*</span> <span class="n">timing_counter</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">experimental</span><span class="p">().</span><span class="n">timestamped_allocator</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">timing_counter</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SharedCounter</span><span class="p">;</span>
      <span class="n">gpu_bfc_allocator</span><span class="o">-&gt;</span><span class="n">SetTimingCounter</span><span class="p">(</span><span class="n">timing_counter</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="c1">// If true, checks for memory overwrites by writing
</span><span class="c1"></span>    <span class="c1">// distinctive patterns on both ends of allocated memory.
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">useCudaMemoryGuardAllocator</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">gpu_allocator</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GPUDebugAllocator</span><span class="p">(</span><span class="n">gpu_allocator</span><span class="p">,</span> <span class="n">platform_gpu_id</span><span class="p">);</span>
      <span class="n">gpu_allocator</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GPUNanResetAllocator</span><span class="p">(</span><span class="n">gpu_allocator</span><span class="p">,</span> <span class="n">platform_gpu_id</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">useCudaMallocAllocator</span><span class="p">())</span> <span class="p">{</span>
      <span class="c1">// If true, passes all allocation requests through to cudaMalloc
</span><span class="c1"></span>      <span class="c1">// useful for doing memory debugging with tools like cuda-memcheck
</span><span class="c1"></span>      <span class="c1">// **WARNING** probably will not work in a multi-gpu scenario
</span><span class="c1"></span>      <span class="n">gpu_allocator</span> <span class="o">=</span>
          <span class="k">new</span> <span class="n">GPUcudaMallocAllocator</span><span class="p">(</span><span class="n">gpu_allocator</span><span class="p">,</span> <span class="n">platform_gpu_id</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">Allocator</span><span class="o">*</span> <span class="n">recording_allocator</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">process_state_</span><span class="o">-&gt;</span><span class="n">ProcessState</span><span class="o">::</span><span class="n">FLAGS_brain_gpu_record_mem_types</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">ProcessState</span><span class="o">::</span><span class="n">MemDesc</span> <span class="n">md</span><span class="p">;</span>
      <span class="n">md</span><span class="p">.</span><span class="n">loc</span> <span class="o">=</span> <span class="n">ProcessState</span><span class="o">::</span><span class="n">MemDesc</span><span class="o">::</span><span class="n">GPU</span><span class="p">;</span>
      <span class="n">md</span><span class="p">.</span><span class="n">dev_index</span> <span class="o">=</span> <span class="n">platform_gpu_id</span><span class="p">.</span><span class="n">value</span><span class="p">();</span>
      <span class="n">md</span><span class="p">.</span><span class="n">gpu_registered</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
      <span class="n">md</span><span class="p">.</span><span class="n">nic_registered</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
      <span class="n">recording_allocator</span> <span class="o">=</span> <span class="k">new</span> <span class="n">internal</span><span class="o">::</span><span class="n">RecordingAllocator</span><span class="p">(</span>
          <span class="o">&amp;</span><span class="n">process_state_</span><span class="o">-&gt;</span><span class="n">mem_desc_map_</span><span class="p">,</span> <span class="n">gpu_allocator</span><span class="p">,</span> <span class="n">md</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mu_</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">allocator_parts</span> <span class="o">=</span> <span class="p">{</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">Allocator</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gpu_allocator</span><span class="p">),</span>
                       <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">SharedCounter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">timing_counter</span><span class="p">),</span>
                       <span class="n">gpu_bfc_allocator</span><span class="p">,</span> <span class="n">sub_allocator</span><span class="p">,</span>
                       <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">Allocator</span><span class="o">&gt;</span><span class="p">(</span><span class="n">recording_allocator</span><span class="p">)};</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">process_state_</span><span class="o">-&gt;</span><span class="n">ProcessState</span><span class="o">::</span><span class="n">FLAGS_brain_gpu_record_mem_types</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">allocator_parts</span><span class="p">.</span><span class="n">recording_allocator</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">allocator_parts</span><span class="p">.</span><span class="n">allocator</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
  <span class="p">}</span>
<span class="cp">#else
</span><span class="cp"></span>  <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;GPUAllocator unavailable. Not compiled with --config=cuda or &#34;</span>
                <span class="s">&#34;--config=rocm.&#34;</span><span class="p">;</span>
  <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
<span class="cp">#endif  </span><span class="c1">// GOOGLE_CUDA || TENSORFLOW_USE_ROCM
</span><span class="c1"></span><span class="p">}</span>
</code></pre></div><p>gpu_process_state 是个单例模式,  只有一个实例存在</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="cm">/*static*/</span> <span class="n">GPUProcessState</span><span class="o">*</span> <span class="n">GPUProcessState</span><span class="o">::</span><span class="n">singleton</span><span class="p">(</span><span class="n">GPUProcessState</span><span class="o">*</span> <span class="n">ps</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">static</span> <span class="n">GPUProcessState</span><span class="o">*</span> <span class="n">instance</span> <span class="o">=</span> <span class="n">ps</span> <span class="o">?</span> <span class="nl">ps</span> <span class="p">:</span> <span class="k">new</span> <span class="n">GPUProcessState</span><span class="p">;</span>
  <span class="n">DCHECK</span><span class="p">((</span><span class="o">!</span><span class="n">ps</span><span class="p">)</span> <span class="o">||</span> <span class="p">(</span><span class="n">ps</span> <span class="o">==</span> <span class="n">instance</span><span class="p">))</span>
      <span class="o">&lt;&lt;</span> <span class="s">&#34;Multiple calls to GPUProcessState with non-null ps&#34;</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">instance</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">GPUProcessState</span><span class="o">::</span><span class="n">GPUProcessState</span><span class="p">()</span> <span class="o">:</span> <span class="n">gpu_device_enabled_</span><span class="p">(</span><span class="nb">false</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">process_state_</span> <span class="o">=</span> <span class="n">ProcessState</span><span class="o">::</span><span class="n">singleton</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div>
</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/tensorflow/">Tensorflow</a>
        
            <a href="/tags/gpu/">GPU</a>
        
            <a href="/tags/antman/">Antman</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer="true"
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="">
    <a href="/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90antman%E5%AF%B9tensorflow%E7%9A%84%E4%BF%AE%E6%94%B9/">
        
        

        <div class="article-details">
            <h2 class="article-title">【代码分析】Antman对Tensorflow的修改</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>

     
    
        
    <script
    src="https://giscus.app/client.js"
    data-repo="Tweakzx/Tweakzx.github.io"
    data-repo-id="MDEwOlJlcG9zaXRvcnkzMjY5MTY1NTE="
    data-category="Announcements"
    data-category-id="DIC_kwDOE3xZx84CTfL-"
    data-mapping="pathname"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-theme="light_tritanopia"
    crossorigin="anonymous"
    async
></script>
<script>
    function setGiscusTheme(theme) {
        let giscus = document.querySelector('iframe.giscus-frame');
        if (giscus) {
            giscus.contentWindow.postMessage(
                { 
                    giscus: {
                        setConfig: { 
                            theme: theme 
                        }
                    }
                },
                "https://giscus.app"
            );
        };
    };

    (function(){
        addEventListener('message', (e) => {
            if (event.origin !== 'https://giscus.app') return;
            handler()
        });
        window.addEventListener('onColorSchemeChange', handler);

        function handler() {
            if (document.documentElement.dataset.scheme === "light") {
                setGiscusTheme('light_tritanopia');
            } else {
                setGiscusTheme('dark_tritanopia');
            };
        };
    }());
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2023 Tweakzx
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.5.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">Table of contents</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ol>
    <li><a href="#session的执行">Session的执行</a>
      <ol>
        <li><a href="#directsessionrun">DirectSession::Run</a></li>
        <li><a href="#directsessionruninternal">DirectSession::RunInternal()</a></li>
      </ol>
    </li>
    <li><a href="#执行器逻辑">执行器逻辑</a>
      <ol>
        <li><a href="#executorstaterunasyn">ExecutorState::RunAsyn()</a></li>
        <li><a href="#executorstatescheduleready">ExecutorState::ScheduleReady()</a></li>
        <li><a href="#executorstateprocess">ExecutorState::Process()</a></li>
        <li><a href="#executorstateasyncgpuopmanager">ExecutorState::AsyncGPUOpManager()</a></li>
      </ol>
    </li>
    <li><a href="#antman对内存分配器的修改">Antman对内存分配器的修改</a></li>
  </ol>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
