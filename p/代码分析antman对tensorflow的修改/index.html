<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Antman对Tensorflow的代码修改 总体的关系图 graph TD Agpu_resource_manage_file] B[SessionRunRegistry] C[SessionRunAction] D[Executor] E[GPUResouceManagement] F[GPU Statistic] G[GpuOpManager] H[GpuUsageAdjustment] I(dump gpu statistic) J[GPU Process State] K[GPUVMemAllocator] L[GPUAdjustableAllocator] A --|FileListener| E B --|Register| E E --|need_to_adjust_memory_| H H --|new| L H --|get| K C --|Derive| E C --|Derive| F B --|Register|'><title>【代码分析】Antman对Tensorflow的修改</title>

<link rel='canonical' href='https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90antman%E5%AF%B9tensorflow%E7%9A%84%E4%BF%AE%E6%94%B9/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='【代码分析】Antman对Tensorflow的修改'>
<meta property='og:description' content='Antman对Tensorflow的代码修改 总体的关系图 graph TD Agpu_resource_manage_file] B[SessionRunRegistry] C[SessionRunAction] D[Executor] E[GPUResouceManagement] F[GPU Statistic] G[GpuOpManager] H[GpuUsageAdjustment] I(dump gpu statistic) J[GPU Process State] K[GPUVMemAllocator] L[GPUAdjustableAllocator] A --|FileListener| E B --|Register| E E --|need_to_adjust_memory_| H H --|new| L H --|get| K C --|Derive| E C --|Derive| F B --|Register|'>
<meta property='og:url' content='https://tweakzx.github.io/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90antman%E5%AF%B9tensorflow%E7%9A%84%E4%BF%AE%E6%94%B9/'>
<meta property='og:site_name' content='Tweakzx'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2022-12-04T23:08:20&#43;08:00'/><meta property='article:modified_time' content='2022-12-04T23:08:20&#43;08:00'/>
<meta name="twitter:title" content="【代码分析】Antman对Tensorflow的修改">
<meta name="twitter:description" content="Antman对Tensorflow的代码修改 总体的关系图 graph TD Agpu_resource_manage_file] B[SessionRunRegistry] C[SessionRunAction] D[Executor] E[GPUResouceManagement] F[GPU Statistic] G[GpuOpManager] H[GpuUsageAdjustment] I(dump gpu statistic) J[GPU Process State] K[GPUVMemAllocator] L[GPUAdjustableAllocator] A --|FileListener| E B --|Register| E E --|need_to_adjust_memory_| H H --|new| L H --|get| K C --|Derive| E C --|Derive| F B --|Register|">
    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <h2 class="article-title">
        <a href="/p/%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90antman%E5%AF%B9tensorflow%E7%9A%84%E4%BF%AE%E6%94%B9/">【代码分析】Antman对Tensorflow的修改</a>
    </h2>

    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Dec 04, 2022</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    8 minute read
                </time>
            </div>
        
    </footer>
    
</div>

    
         
         <script async src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    

</header>

    <section class="article-content">
    <h2 id="antman对tensorflow的代码修改">Antman对Tensorflow的代码修改</h2>
<p>总体的关系图</p>
<div class="mermaid">
  

graph TD
  A>gpu_resource_manage_file]
  B[SessionRunRegistry]
  C[SessionRunAction] 
  D[Executor]
  E[GPUResouceManagement]
  F[GPU Statistic]
  G[GpuOpManager]
  H[GpuUsageAdjustment]
  I(dump gpu statistic)
  J[GPU Process State]
  K[GPUVMemAllocator]
  L[GPUAdjustableAllocator]

  A -->|FileListener| E
  B -->|Register| E
  E -->|need_to_adjust_memory_| H
  H -->|new| L
  H -->|get| K
  C -->|Derive| E
  C -->|Derive| F
  B -->|Register| F
  F -->|need_to_dump_statistics_| I

  B -->|Run| C

  J -->|maybe_create_gpu_vmem_allocator|K
  D -->|run thread| G
  E -->|GetEstimatedIdleTime| G


</div>
<h2 id="gpuvmemallocator">GPUVMemAllocator</h2>
<p>GPUVMemAllocator 可以分配host的mem作为显存的备用，以免出现OOM错误。</p>
<h3 id="创建allocator">创建allocator</h3>
<p>maybe_create_gpu_vmem_allocator(&hellip;)可以根据情况返回合适的allocator</p>
<div class="mermaid">
  

graph TD
  A([start]) -->|gpu_allocator|B[maybe_create_gpu_vmem_allocator]-->C{if !gpu_vmem} 
  C -->|true| D([返回gpu_allocator])
  C -->|false|Z(准备生成VMemAllocator)
  Z --> E[new GpuHostAllocator]
  E -->|sub_allocator| F[new BFCAllocator]
  Z -->|gpu_allocator| G[new GPUVMemAllocator]
  F -->|host_allocator|G
  G -->|gpu_vmem_allocator|H([返回gpu_vmem_allocator])


</div>
<div class="highlight"><pre class="chroma"><code class="language-c++" data-lang="c++"><span class="n">Allocator</span><span class="o">*</span> <span class="nf">maybe_create_gpu_vmem_allocator</span><span class="p">(</span><span class="n">Allocator</span><span class="o">*</span> <span class="n">gpu_allocator</span><span class="p">,</span>
                                           <span class="kt">int</span> <span class="n">bus_id</span><span class="p">,</span>
                                           <span class="n">PlatformGpuId</span> <span class="n">platform_gpu_id</span><span class="p">,</span>
                                           <span class="kt">int</span> <span class="n">tf_gpu_id</span><span class="p">,</span>
                                           <span class="n">se</span><span class="o">::</span><span class="n">StreamExecutor</span><span class="o">*</span> <span class="n">stream_exec</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">bool</span> <span class="n">gpu_vmem</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="n">Status</span> <span class="n">status</span> <span class="o">=</span> <span class="n">ReadBoolFromEnvVar</span><span class="p">(</span><span class="s">&#34;TF_GPU_VMEM&#34;</span><span class="p">,</span>
                                     <span class="nb">true</span><span class="cm">/*enabled by default*/</span><span class="p">,</span>
                                     <span class="o">&amp;</span><span class="n">gpu_vmem</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;GetGPUAllocator: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">status</span><span class="p">.</span><span class="n">error_message</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">gpu_vmem</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">gpu_allocator</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">SubAllocator</span><span class="o">*</span> <span class="n">sub_allocator</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GpuHostAllocator</span><span class="p">(</span>
      <span class="n">GpuIdUtil</span><span class="o">::</span><span class="n">ExecutorForPlatformGpuId</span><span class="p">(</span><span class="n">platform_gpu_id</span><span class="p">).</span><span class="n">ValueOrDie</span><span class="p">(),</span>
      <span class="n">bus_id</span><span class="p">,</span> <span class="p">{},</span> <span class="p">{});</span>
  <span class="n">int64</span> <span class="n">cuda_host_mem_limit_in_mb</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
  <span class="n">status</span> <span class="o">=</span> <span class="n">ReadInt64FromEnvVar</span><span class="p">(</span><span class="s">&#34;TF_CUDA_HOST_MEM_LIMIT_IN_MB&#34;</span><span class="p">,</span>
                               <span class="mi">1LL</span> <span class="o">&lt;&lt;</span> <span class="mi">16</span> <span class="cm">/*64GB max by default*/</span><span class="p">,</span>
                               <span class="o">&amp;</span><span class="n">cuda_host_mem_limit_in_mb</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;GetGpuHostAllocator: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">status</span><span class="p">.</span><span class="n">error_message</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="n">int64</span> <span class="n">cuda_host_mem_limit</span> <span class="o">=</span> <span class="n">cuda_host_mem_limit_in_mb</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1LL</span> <span class="o">&lt;&lt;</span> <span class="mi">20</span><span class="p">);</span>
  <span class="n">Allocator</span><span class="o">*</span> <span class="n">host_allocator</span> <span class="o">=</span>
      <span class="k">new</span> <span class="n">BFCAllocator</span><span class="p">(</span><span class="n">sub_allocator</span><span class="p">,</span> <span class="n">cuda_host_mem_limit</span><span class="p">,</span>
                       <span class="nb">true</span> <span class="cm">/*allow_growth*/</span><span class="p">,</span>
                       <span class="n">strings</span><span class="o">::</span><span class="n">StrCat</span><span class="p">(</span><span class="s">&#34;GPUHost_&#34;</span><span class="p">,</span> <span class="n">tf_gpu_id</span><span class="p">,</span> <span class="s">&#34;_bfc&#34;</span><span class="p">));</span>
  <span class="n">Allocator</span><span class="o">*</span> <span class="n">gpu_vmem_allocator</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GPUVMemAllocator</span><span class="p">(</span><span class="n">gpu_allocator</span><span class="p">,</span>
                                                       <span class="n">host_allocator</span><span class="p">,</span>
                                                       <span class="n">tf_gpu_id</span><span class="p">,</span>
                                                       <span class="n">stream_exec</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">gpu_vmem_allocator</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><h3 id="分配虚拟内存">分配虚拟内存</h3>
<p>先尝试分配GPU内存， 分配成功则返回， 分配失败则分配CPU内存。</p>
<div class="highlight"><pre class="chroma"><code class="language-c++" data-lang="c++"><span class="kt">void</span><span class="o">*</span> <span class="n">GPUVMemAllocator</span><span class="o">::</span><span class="n">AllocateRaw</span><span class="p">(</span><span class="n">size_t</span> <span class="n">alignment</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">num_bytes</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">lock_</span><span class="p">);</span>
    <span class="n">AllocationAttributes</span> <span class="n">new_attr</span><span class="p">;</span>
    <span class="c1">// Tell the device_allocator_ not to retry
</span><span class="c1"></span>    <span class="c1">// since we can alloc host memory as backup
</span><span class="c1"></span>    <span class="n">new_attr</span><span class="p">.</span><span class="n">no_retry_on_failure</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
    <span class="kt">void</span><span class="o">*</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">device_allocator_</span><span class="o">-&gt;</span><span class="n">AllocateRaw</span><span class="p">(</span><span class="n">alignment</span><span class="p">,</span> <span class="n">num_bytes</span><span class="p">,</span> <span class="n">new_attr</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">device_ptrs_</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">ret</span><span class="p">);</span>
      <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">host_allocator_</span><span class="o">-&gt;</span><span class="n">AllocateRaw</span><span class="p">(</span><span class="n">alignment</span><span class="p">,</span> <span class="n">num_bytes</span><span class="p">);</span>
    <span class="n">VLOG</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;host_allocator_ allocates &#34;</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">num_bytes</span><span class="o">/</span><span class="mf">1024.0</span><span class="o">/</span><span class="mi">1024</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; MiB&#34;</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><h2 id="sessionrunactionregistry中间件框架">SessionRunActionRegistry（中间件框架）</h2>
<p>添加了一个SessionRunActionRegistry框架， 方便在session开始之前或者结束之后添加执行动作</p>
<h3 id="修改direct_sessioncc-和-master_sessioncc">修改direct_session.cc 和 master_session.cc</h3>
<p>/修改了原先的session执行流程 在session执行前后分别执行actions</p>
<div class="highlight"><pre class="chroma"><code class="language-c++" data-lang="c++">
<span class="c1">// Running all pre session run action in grouping
</span><span class="c1"></span>  <span class="n">SessionRunActionOptions</span> <span class="n">action_options</span><span class="p">;</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">device_mgr</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">device_mgr_</span><span class="p">;</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">sess_ptr</span> <span class="o">=</span> <span class="k">this</span><span class="p">;</span>
  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">Global</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">RunGrouping</span><span class="p">(</span>
      <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">PRE_SESSION_RUN</span><span class="p">,</span> <span class="n">action_options</span><span class="p">));</span>
<span class="p">...</span>
    
 <span class="k">const</span> <span class="n">uint64</span> <span class="n">time_duration_usecs</span> <span class="o">=</span> <span class="n">options_</span><span class="p">.</span><span class="n">env</span><span class="o">-&gt;</span><span class="n">NowMicros</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time_usecs</span><span class="p">;</span>
  <span class="n">metrics</span><span class="o">::</span><span class="n">UpdateGraphExecTime</span><span class="p">(</span><span class="n">time_duration_usecs</span><span class="p">);</span>

  <span class="c1">// Running all post session run action in grouping
</span><span class="c1"></span>  <span class="n">uint64</span> <span class="n">session_end_time</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">::</span><span class="n">Env</span><span class="o">::</span><span class="n">Default</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">NowMicros</span><span class="p">();</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">sess_duration_us</span> <span class="o">=</span> <span class="n">time_duration_usecs</span><span class="p">;</span>
  <span class="n">action_options</span><span class="p">.</span><span class="n">graph_id</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">uint64</span><span class="o">&gt;</span><span class="p">(</span><span class="n">executors_and_keys</span><span class="p">);</span>
  <span class="n">TF_RETURN_IF_ERROR</span><span class="p">(</span><span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">Global</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">RunGrouping</span><span class="p">(</span>
      <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">POST_SESSION_RUN</span><span class="p">,</span> <span class="n">action_options</span><span class="p">));</span>

</code></pre></div><h3 id="注册action">注册action</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">Register</span><span class="p">(</span>
    <span class="n">Grouping</span> <span class="n">grouping</span><span class="p">,</span> <span class="kt">int</span> <span class="n">phase</span><span class="p">,</span> <span class="n">SessionRunAction</span><span class="o">*</span> <span class="n">action</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">VLOG</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Register session run action &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">action</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">();</span>
  <span class="n">groups_</span><span class="p">[</span><span class="n">grouping</span><span class="p">][</span><span class="n">phase</span><span class="p">].</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">action</span><span class="p">);</span>
<span class="p">}</span>


<span class="c1">// 一些宏函数: 提供注册中间件的接口
</span><span class="c1"></span><span class="cp">#define REGISTER_SESSION_RUN_ACTION(grouping, phase, action) \
</span><span class="cp">  REGISTER_ACTION_UNIQ_HELPER(__COUNTER__, grouping, phase, action)
</span><span class="cp"></span>
<span class="cp">#define REGISTER_ACTION_UNIQ_HELPER(ctr, grouping, phase, action) \
</span><span class="cp">  REGISTER_ACTION_UNIQ(ctr, grouping, phase, action)
</span><span class="cp"></span>
<span class="cp">#define REGISTER_ACTION_UNIQ(ctr, grouping, phase, action)             \
</span><span class="cp">  static ::tensorflow::session_run_action_registration::               \
</span><span class="cp">      SessionRunActionRegistration register_session_run_action_##ctr(  \
</span><span class="cp">          grouping, phase, new action(),                               \
</span><span class="cp">          #action)
</span><span class="cp"></span>
<span class="p">}</span>  <span class="c1">// namespace tensorflow
</span><span class="c1"></span>
<span class="cp">#endif  </span><span class="c1">// TENSORFLOW_CORE_COMMON_RUNTIME_SESSION_RUN_ACTION_REGISTRY_H_
</span><span class="c1"></span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="c1">//定义了一个RunAction()的接口， Action 必须实现这个接口
</span><span class="c1"></span><span class="k">class</span> <span class="nc">SessionRunAction</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">virtual</span> <span class="o">~</span><span class="n">SessionRunAction</span><span class="p">()</span> <span class="p">{}</span>
  <span class="k">virtual</span> <span class="n">Status</span> <span class="nf">RunAction</span><span class="p">(</span><span class="k">const</span> <span class="n">SessionRunActionOptions</span><span class="o">&amp;</span> <span class="n">options</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="kt">void</span> <span class="nf">set_name</span><span class="p">(</span><span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">name</span><span class="p">)</span> <span class="p">{</span> <span class="n">name_</span> <span class="o">=</span> <span class="n">name</span><span class="p">;</span> <span class="p">}</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="n">name_</span><span class="p">;</span> <span class="p">}</span>

 <span class="k">private</span><span class="o">:</span>
  <span class="c1">// The name of the action, which is the same as the inherited
</span><span class="c1"></span>  <span class="c1">// class name.
</span><span class="c1"></span>  <span class="n">string</span> <span class="n">name_</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div><h2 id="gpu-memory-limit-adjustment">GPU memory limit adjustment</h2>
<p>实现动态资源分配， 自动调整现存限制， 当发现 Host 内存被使用的时候，会提高显存的限制阈值，这样所有的 Tensor 都可以申请在显卡上。这样只会影响一个 mini batch 的性能，后面的 mini batch 跑前向后向计算的时候，所有的 Tensor 都会被申请在显存上。</p>
<h3 id="修改了tensorflow-原本的-bfc_allocatorh">修改了tensorflow 原本的 BFC_Allocator.h</h3>
<p>增加了friend class GPUAdjustableAllocator;</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc">  <span class="c1">// Declare the GPUAdjustableAllocator to be friend of the BFCAllocator,
</span><span class="c1"></span>  <span class="c1">// therefore it can adjust the memory limit by modifying the private
</span><span class="c1"></span>  <span class="c1">// member variables of BFCAllocator.
</span><span class="c1"></span>  <span class="k">friend</span> <span class="k">class</span> <span class="nc">GPUAdjustableAllocator</span><span class="p">;</span>
</code></pre></div><h3 id="增加了扩缩显存分配的函数">增加了扩缩显存分配的函数</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="k">class</span> <span class="nc">GPUAdjustableAllocator</span> <span class="k">final</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="c1">// Adjust the memory_limit_ to allow memory grow/shrink at runtime
</span><span class="c1"></span>  <span class="c1">// Returns adjusted memory_limit_. If the return value is less than
</span><span class="c1"></span>  <span class="c1">// the new_memory_limit, the adjustment failed.
</span><span class="c1"></span>  <span class="n">size_t</span> <span class="n">AdjustMemoryLimit</span><span class="p">(</span><span class="n">size_t</span> <span class="n">new_memory_limit</span><span class="p">,</span>
                           <span class="n">BFCAllocator</span><span class="o">*</span> <span class="n">bfc_allocator</span><span class="p">);</span>

  <span class="c1">// Get the memory pool size and in used memory size of the bfc_allocator.
</span><span class="c1"></span>  <span class="kt">void</span> <span class="nf">GetMemPoolStats</span><span class="p">(</span><span class="n">BFCAllocator</span><span class="o">*</span> <span class="n">bfc_allocator</span><span class="p">,</span>
                      <span class="kt">int64_t</span><span class="o">*</span> <span class="n">deviceMemPoolSize</span><span class="p">,</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="n">deviceMemStable</span><span class="p">);</span>

 <span class="k">private</span><span class="o">:</span>
  <span class="c1">// Free the memory regions that are not in use
</span><span class="c1"></span>  <span class="n">size_t</span> <span class="n">FreeEmptyMemory</span><span class="p">(</span><span class="n">size_t</span> <span class="n">target_memory_bytes</span><span class="p">,</span>
                         <span class="n">BFCAllocator</span><span class="o">*</span> <span class="n">bfc_allocator</span><span class="p">)</span>
      <span class="n">EXCLUSIVE_LOCKS_REQUIRED</span><span class="p">(</span><span class="n">lock_</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="n">size_t</span> <span class="n">GPUAdjustableAllocator</span><span class="o">::</span><span class="n">AdjustMemoryLimit</span><span class="p">(</span><span class="n">size_t</span> <span class="n">new_memory_limit</span><span class="p">,</span> <span class="n">BFCAllocator</span><span class="o">*</span> <span class="n">bfc_allocator</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">lock_</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">new_memory_limit</span> <span class="o">&gt;=</span> <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">total_region_allocated_bytes_</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 1) new_memory_limit &gt;= memory_limit_ : grow memory size
</span><span class="c1"></span>    <span class="c1">// 2) memory_limit_ &gt; new_memory_limit &gt;= total_region_allocated_bytes_:
</span><span class="c1"></span>    <span class="c1">//    shrink, but don&#39;t need to free memory
</span><span class="c1"></span>    <span class="c1">// In both cases, no action needed by changing the memory limit
</span><span class="c1"></span>    <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">memory_limit_</span> <span class="o">=</span> <span class="n">new_memory_limit</span><span class="p">;</span>
    <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">stats_</span><span class="p">.</span><span class="n">bytes_limit</span> <span class="o">=</span> <span class="n">new_memory_limit</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// total_region_allocated_bytes_ &gt; new_memory_limit:
</span><span class="c1"></span>    <span class="c1">// shrink, need to free memory
</span><span class="c1"></span>    <span class="n">size_t</span> <span class="n">free_res</span> <span class="o">=</span> <span class="n">FreeEmptyMemory</span><span class="p">(</span>
        <span class="n">new_memory_limit</span><span class="p">,</span> <span class="n">bfc_allocator</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">free_res</span> <span class="o">&lt;=</span> <span class="n">new_memory_limit</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">memory_limit_</span> <span class="o">=</span> <span class="n">new_memory_limit</span><span class="p">;</span>
      <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">stats_</span><span class="p">.</span><span class="n">bytes_limit</span> <span class="o">=</span> <span class="n">new_memory_limit</span><span class="p">;</span>

    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">memory_limit_</span> <span class="o">=</span> <span class="n">free_res</span><span class="p">;</span>
      <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">stats_</span><span class="p">.</span><span class="n">bytes_limit</span> <span class="o">=</span> <span class="n">free_res</span><span class="p">;</span>

    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">bfc_allocator</span><span class="o">-&gt;</span><span class="n">memory_limit_</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><h2 id="file-listener">File Listener</h2>
<p>监控配置文件是否发生改变， 如果发生改变则触发响应的handler,以及一个回调函数</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">FileListener</span><span class="o">::</span><span class="n">RegisterFileListener</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">file_path</span><span class="p">,</span>
                                        <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">handler_name</span><span class="p">,</span>
                                        <span class="n">callback</span> <span class="n">callback_func</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Register a file listener named &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">handler_name</span>
             <span class="o">&lt;&lt;</span> <span class="s">&#34; on file &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">file_path</span><span class="p">;</span>
  <span class="n">FileInfo</span> <span class="nf">new_file</span><span class="p">(</span><span class="n">file_path</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">CallbackFunc</span><span class="o">&gt;</span> <span class="n">new_handlers</span><span class="p">;</span>
  <span class="n">InfoAndHandlers</span> <span class="n">value</span> <span class="o">=</span> <span class="p">{</span><span class="n">new_file</span><span class="p">,</span> <span class="n">new_handlers</span><span class="p">};</span>
  <span class="n">CallbackFunc</span> <span class="nf">new_callback</span><span class="p">(</span><span class="n">handler_name</span><span class="p">,</span> <span class="n">callback_func</span><span class="p">);</span>

  <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">lock_</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">res</span> <span class="o">=</span> <span class="n">listeners_</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>

  <span class="n">res</span><span class="p">.</span><span class="n">first</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">file_handlers_</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">new_callback</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">file_monitor_thread_</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Note we should start only one monitor thread
</span><span class="c1"></span>    <span class="n">StartMonitorThread</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><h2 id="gpu-resource-management中间件">GPU resource management（中间件）</h2>
<p>继承了SessionRunAction, 是一个资源管理中间件，定义如下</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="k">class</span> <span class="nc">GPUResourceManagement</span> <span class="o">:</span> <span class="k">public</span> <span class="n">SessionRunAction</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="c1">// Note that we will enable TF_FORCE_GPU_ALLOW_GROWTH and TF_GPU_VMEM
</span><span class="c1"></span>  <span class="c1">// automatically if the GPUResourceManagement feature is enabled.
</span><span class="c1"></span>  <span class="n">GPUResourceManagement</span><span class="p">();</span>
  <span class="o">~</span><span class="n">GPUResourceManagement</span><span class="p">()</span> <span class="k">override</span><span class="p">;</span>
  <span class="p">...</span>	
  <span class="p">...</span>    
  <span class="c1">//🚨配置文件更新后， 解析新的配置并存放于此
</span><span class="c1"></span>  <span class="c1">// For recording the parsed new gpu resource limit.
</span><span class="c1"></span>  <span class="c1">//🚨 GPU 资源限制
</span><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span> <span class="n">GPUResourceLimitInfo</span><span class="o">&gt;</span>
      <span class="n">gpu_resource_management_info_</span><span class="p">;</span>

  <span class="c1">// For recording the parsed new gpu performance limitation
</span><span class="c1"></span>  <span class="c1">// (if the value is 0, then it means to suspend this job).
</span><span class="c1"></span>  <span class="c1">// 🚨 GPU 性能限制， 如果值为0， 意味着挂起这个job
</span><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">gpu_perf_control_</span><span class="p">;</span>

  <span class="c1">// For recording the total time of all inserted time slot.
</span><span class="c1"></span>  <span class="n">uint64</span> <span class="n">total_time_slot_</span><span class="p">;</span>

  <span class="c1">// For recording the estimated total idle time.
</span><span class="c1"></span>  <span class="n">uint64</span> <span class="n">estimated_total_idle_time_</span><span class="p">;</span>

  <span class="c1">// For recording the total number of queued GPU op running in
</span><span class="c1"></span>  <span class="c1">// the specified executor.
</span><span class="c1"></span>  <span class="c1">//  🚨记录每一个Executor要执行的OP数目
</span><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">void</span><span class="o">*</span><span class="p">,</span> <span class="n">uint64</span><span class="o">&gt;</span> <span class="n">executor_queued_op_num_</span><span class="p">;</span>

  <span class="c1">// Determine if we need to adjust the GPU usage limit.
</span><span class="c1"></span>  <span class="c1">// 🚨表示是否需要更改配置
</span><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="n">need_to_adjust_memory_</span><span class="p">;</span>

  <span class="c1">// For performing the adjustment.
</span><span class="c1"></span>  <span class="c1">// 修改配置的类的实例
</span><span class="c1"></span>  <span class="n">GPUUsageAdjustment</span><span class="o">*</span> <span class="n">gpu_usage_adjustment_</span><span class="p">;</span>

  <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">FILE_LISTENER_NAME</span> <span class="o">=</span> <span class="s">&#34;GPUResourceManage&#34;</span><span class="p">;</span>
<span class="p">};</span>

</code></pre></div><h3 id="gpuresourcemanagement">GPUResourceManagement()</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="n">GPUResourceManagement</span><span class="o">::</span><span class="n">GPUResourceManagement</span><span class="p">()</span>
    <span class="o">:</span> <span class="n">need_to_adjust_memory_</span><span class="p">(</span><span class="nb">false</span><span class="p">),</span>
    <span class="n">gpu_perf_control_</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
    <span class="n">gpu_usage_adjustment_</span><span class="p">(</span><span class="k">new</span> <span class="n">GPUUsageAdjustment</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">//从环境变量中读取gpu配置文件的路径
</span><span class="c1"></span>  <span class="n">ReadStringFromEnvVar</span><span class="p">(</span><span class="s">&#34;GPU_CONFIG_FILE&#34;</span><span class="p">,</span> <span class="s">&#34;&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">gpu_resource_manage_file_path_</span><span class="p">);</span>
        
  <span class="k">if</span> <span class="p">(</span><span class="n">gpu_resource_manage_file_path_</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">enable_gpu_resource_manage_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">enable_gpu_resource_manage_</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
    <span class="c1">// Note that we will enable TF_FORCE_GPU_ALLOW_GROWTH and TF_GPU_VMEM
</span><span class="c1"></span>    <span class="c1">// automatically if the GPUResourceManagement feature is enabled.
</span><span class="c1"></span>    <span class="n">setenv</span><span class="p">(</span><span class="s">&#34;TF_FORCE_GPU_ALLOW_GROWTH&#34;</span><span class="p">,</span> <span class="s">&#34;true&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">setenv</span><span class="p">(</span><span class="s">&#34;TF_GPU_VMEM&#34;</span><span class="p">,</span> <span class="s">&#34;true&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

    <span class="c1">// Register a handler that will be triggered when the file named
</span><span class="c1"></span>    <span class="n">FileListener</span><span class="o">::</span><span class="n">GlobalFileListener</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">RegisterFileListener</span><span class="p">(</span>
        <span class="n">gpu_resource_manage_file_path_</span><span class="p">,</span> <span class="n">FILE_LISTENER_NAME</span><span class="p">,</span> <span class="c1">//FILE_LISTENER_NAME: &#34;GPUResourceManage&#34;
</span><span class="c1"></span>        <span class="p">[](</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">str</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">// The callback func which is invoked when file changed.
</span><span class="c1"></span>          <span class="c1">// 传入一个json文件，包含ManageInfo   
</span><span class="c1"></span>          <span class="c1">// 当文件更改时， 获取相应的在session结束后调用顺序为2的action中名为GPUResourceManagement的action
</span><span class="c1"></span>          <span class="c1">// action解析新的配置信息
</span><span class="c1"></span>          <span class="c1">// 等到session 触发RunAction()， 更新限制
</span><span class="c1"></span>          <span class="n">SessionRunAction</span><span class="o">*</span> <span class="n">act</span> <span class="o">=</span>
              <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">Global</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">GetAction</span><span class="p">(</span>
                  <span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">POST_SESSION_RUN</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="s">&#34;GPUResourceManagement&#34;</span><span class="p">);</span>
          <span class="k">if</span> <span class="p">(</span><span class="n">act</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Cannot get the instance of GPUResourceManagement </span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
          <span class="p">}</span>
          <span class="k">if</span> <span class="p">(</span><span class="n">act</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">GPUResourceManagement</span><span class="o">*</span> <span class="n">rm</span> <span class="o">=</span>
                <span class="k">dynamic_cast</span><span class="o">&lt;</span><span class="n">GPUResourceManagement</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">act</span><span class="p">);</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">rm</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
              <span class="n">rm</span><span class="o">-&gt;</span><span class="n">ParseManageInfoFromJson</span><span class="p">(</span><span class="n">str</span><span class="p">);</span>
            <span class="p">}</span>
          <span class="p">}</span>
        <span class="p">});</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><h3 id="注册中间件">注册中间件</h3>
<p>意味着session 结束前后要执行RunAction（&hellip;）</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="cp">#if GOOGLE_CUDA
</span><span class="cp"></span><span class="c1">// We register the GPUResourceManagement as a POST_SESSION_RUN action
</span><span class="c1">// during the initialization phase of the program.
</span><span class="c1"></span><span class="n">REGISTER_SESSION_RUN_ACTION</span><span class="p">(</span><span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">POST_SESSION_RUN</span><span class="p">,</span>
                            <span class="mi">2</span><span class="p">,</span> <span class="n">GPUResourceManagement</span><span class="p">);</span>

<span class="cp">#endif  </span><span class="c1">// GOOGLE_CUDA 
</span></code></pre></div><h3 id="实现函数runaction">实现函数RunAction（&hellip;）</h3>
<p>如果需要进行显存调整， 则调用GPUUsageAdjustment调整资源</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="n">Status</span> <span class="n">GPUResourceManagement</span><span class="o">::</span><span class="n">RunAction</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">SessionRunActionOptions</span><span class="o">&amp;</span> <span class="n">options</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">need_to_adjust_memory_</span> <span class="o">&amp;&amp;</span> <span class="n">gpu_perf_control_</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// TODO(shiru): do we need to unregister the
</span><span class="c1"></span>    <span class="c1">// GPUResourceManagement if the environment variable
</span><span class="c1"></span>    <span class="c1">// GPU_CONFIG_FILE is set to null?
</span><span class="c1"></span>    <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">need_to_adjust_memory_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">manage_mu_</span><span class="p">);</span>
    <span class="c1">// Start to adjust the resource limit as required.
</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">gpu_resource_management_info_</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">gpu_usage_adjustment_</span><span class="o">-&gt;</span><span class="n">AdjustMemLimit</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">,</span> <span class="c1">//GPU总线id
</span><span class="c1"></span>          <span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">mem_limit_</span><span class="p">,</span> <span class="n">options</span><span class="p">.</span><span class="n">device_mgr</span><span class="p">,</span>	  <span class="c1">//新的显存限制， 设备管理器
</span><span class="c1"></span>          <span class="n">options</span><span class="p">.</span><span class="n">device_set</span><span class="p">);</span>						  <span class="c1">//设备集合
</span><span class="c1"></span>    <span class="p">}</span>
    <span class="n">need_to_adjust_memory_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="p">}</span>
    
 <span class="c1">// 暂停一段时间 或者 挂起这个job
</span><span class="c1"></span>  <span class="n">DoSleepOrSuspend</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">sess_duration_us</span><span class="p">);</span>

  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
<span class="p">}</span>

</code></pre></div><h3 id="gpuusageadjustmentcc">GPUUsageAdjustment.cc</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">bool</span> <span class="n">GPUUsageAdjustment</span><span class="o">::</span><span class="n">AdjustMemLimit</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">gpu_pci_bus_id</span><span class="p">,</span>
    <span class="n">size_t</span> <span class="n">new_mem_limit</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">tensorflow</span><span class="o">::</span><span class="n">DeviceMgr</span><span class="o">&gt;*</span> <span class="n">device_mgr</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">DeviceSet</span><span class="o">&gt;*</span> <span class="n">device_set</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">adj_mu_</span><span class="p">);</span>
	<span class="c1">//一个记录对应gpu使用情况的map
</span><span class="c1"></span>  <span class="k">auto</span> <span class="n">cur_info</span> <span class="o">=</span> <span class="n">cur_usage_info_</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">gpu_pci_bus_id</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">cur_info</span> <span class="o">==</span> <span class="n">cur_usage_info_</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">//如果没有相应的使用信息， 则立刻获取使用信息
</span><span class="c1"></span>    <span class="n">GPUBFCAllocator</span><span class="o">*</span> <span class="n">allo</span> <span class="o">=</span> <span class="n">GetGPUAllocator</span><span class="p">(</span><span class="n">device_mgr</span><span class="p">,</span>
        <span class="n">device_set</span><span class="p">,</span> <span class="n">gpu_pci_bus_id</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">allo</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed to get the allocator of gpu_pci_bus_id: &#34;</span>
                 <span class="o">&lt;&lt;</span> <span class="n">gpu_pci_bus_id</span><span class="p">;</span>
      <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">GPUUsageInfo</span> <span class="n">usage_info</span><span class="p">;</span>
    <span class="n">usage_info</span><span class="p">.</span><span class="n">gpu_allocator_</span> <span class="o">=</span> <span class="n">allo</span><span class="p">;</span>
    <span class="n">usage_info</span><span class="p">.</span><span class="n">cur_limit_</span><span class="p">.</span><span class="n">mem_limit_</span> <span class="o">=</span> <span class="n">ULONG_MAX</span><span class="p">;</span>
    <span class="c1">// Get the VGPU_MEMORY_LIMIT
</span><span class="c1"></span>    <span class="n">absl</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">AllocatorStats</span><span class="o">&gt;</span> <span class="n">device_stats</span> <span class="o">=</span> <span class="n">allo</span><span class="o">-&gt;</span><span class="n">GetStats</span><span class="p">();</span>
    <span class="n">usage_info</span><span class="p">.</span><span class="n">cur_limit_</span><span class="p">.</span><span class="n">initial_mem_limit_</span> <span class="o">=</span> 
      <span class="n">device_stats</span> <span class="o">?</span> <span class="o">*</span><span class="n">device_stats</span><span class="o">-&gt;</span><span class="nl">bytes_limit</span> <span class="p">:</span> <span class="n">ULONG_MAX</span><span class="p">;</span>

    <span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">cur_usage_info_</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">gpu_pci_bus_id</span><span class="p">,</span> <span class="n">usage_info</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ret</span><span class="p">.</span><span class="n">second</span> <span class="o">==</span> <span class="nb">false</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">cur_info</span> <span class="o">=</span> <span class="n">ret</span><span class="p">.</span><span class="n">first</span><span class="p">;</span>
  <span class="p">}</span>
	<span class="c1">//如果超出虚拟GPU的使用限制， 则使用上限
</span><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">new_mem_limit</span> <span class="o">&gt;</span> <span class="n">cur_info</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">cur_limit_</span><span class="p">.</span><span class="n">initial_mem_limit_</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// The new mem size limit exceeds VGPU_MEMORY_LIMIT
</span><span class="c1"></span>    <span class="n">new_mem_limit</span> <span class="o">=</span> <span class="n">cur_info</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">cur_limit_</span><span class="p">.</span><span class="n">initial_mem_limit_</span><span class="p">;</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">WARNING</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;The new mem size limit exceeds VGPU_MEMORY_LIMIT, &#34;</span>
                 <span class="o">&lt;&lt;</span> <span class="s">&#34;therefore, adjust the new mem size limit to : &#34;</span>
                 <span class="o">&lt;&lt;</span> <span class="n">new_mem_limit</span><span class="p">;</span>
  <span class="p">}</span>
	<span class="c1">//如果在限制范围内，并且需要调整， 且调整后的值不为0， 
</span><span class="c1"></span>    <span class="c1">//调用GPUAdjustableAllocator， 更改内存限制， 并且更新使用信息
</span><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">cur_info</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">cur_limit_</span><span class="p">.</span><span class="n">mem_limit_</span> <span class="o">!=</span> <span class="n">new_mem_limit</span>
      <span class="o">&amp;&amp;</span> <span class="n">new_mem_limit</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Adjust the memory limit of this GPU
</span><span class="c1"></span>    <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Start to manage the mem size limit to &#34;</span>
              <span class="o">&lt;&lt;</span> <span class="n">new_mem_limit</span>
              <span class="o">&lt;&lt;</span> <span class="s">&#34; of device gpu_pci_bus_id: &#34;</span>
              <span class="o">&lt;&lt;</span> <span class="n">gpu_pci_bus_id</span><span class="p">;</span>
    <span class="n">GPUAdjustableAllocator</span><span class="o">*</span> <span class="n">adj</span> <span class="o">=</span> <span class="k">new</span> <span class="n">GPUAdjustableAllocator</span><span class="p">();</span>
    <span class="n">size_t</span> <span class="n">cur_mem_limit</span> <span class="o">=</span> <span class="n">adj</span><span class="o">-&gt;</span><span class="n">AdjustMemoryLimit</span><span class="p">(</span><span class="n">new_mem_limit</span><span class="p">,</span>
        <span class="n">cur_info</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">gpu_allocator_</span><span class="p">);</span>
    <span class="n">cur_info</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">cur_limit_</span><span class="p">.</span><span class="n">mem_limit_</span> <span class="o">=</span> <span class="n">cur_mem_limit</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">cur_mem_limit</span> <span class="o">&gt;</span> <span class="n">new_mem_limit</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed to manage the mem size limit to &#34;</span>
                 <span class="o">&lt;&lt;</span> <span class="n">new_mem_limit</span>
                 <span class="o">&lt;&lt;</span> <span class="s">&#34; of device gpu_pci_bus_id: &#34;</span>
                 <span class="o">&lt;&lt;</span> <span class="n">gpu_pci_bus_id</span><span class="p">;</span>
      <span class="c1">// TODO(shiru): need to check is gpu_allocator_ has been changed!
</span><span class="c1"></span>      <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><h2 id="gpu-statistics-中间件">Gpu Statistics （中间件）</h2>
<p>在session运行结束后执行，执行顺序为1， 判断是否需要导出GPU统计数据</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="n">Status</span> <span class="n">GPUStatistics</span><span class="o">::</span><span class="n">RunAction</span><span class="p">(</span><span class="k">const</span> <span class="n">SessionRunActionOptions</span><span class="o">&amp;</span> <span class="n">options</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">need_to_dump_statistics_</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="kt">bool</span> <span class="n">huge_change</span> <span class="o">=</span> <span class="n">RecordSessionRunDuration</span><span class="p">(</span>
      <span class="n">options</span><span class="p">.</span><span class="n">graph_id</span><span class="p">,</span> <span class="n">options</span><span class="p">.</span><span class="n">sess_duration_us</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ShouldCheckGPUStatistics</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">huge_change</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="p">{</span>
    <span class="c1">// Global lock.
</span><span class="c1"></span>    <span class="n">mutex_lock</span> <span class="nf">l</span><span class="p">(</span><span class="n">check_mu_</span><span class="p">);</span>
    <span class="kt">bool</span> <span class="n">dur_flag</span> <span class="o">=</span> <span class="n">CheckSessionRunDuration</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">graph_id</span><span class="p">,</span>
        <span class="n">options</span><span class="p">.</span><span class="n">sess_duration_us</span><span class="p">);</span>
    <span class="kt">bool</span> <span class="n">stat_flag</span> <span class="o">=</span> <span class="n">CheckGPUVMemAllocatorStatistics</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">device_mgr</span><span class="p">,</span>
        <span class="n">options</span><span class="p">.</span><span class="n">device_set</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">dur_flag</span> <span class="o">||</span> <span class="n">stat_flag</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">dumpGPUStatistics</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="n">gpu_statistics_last_write_</span> <span class="o">=</span> <span class="n">time</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="kt">void</span> <span class="n">GPUStatistics</span><span class="o">::</span><span class="n">dumpGPUStatistics</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">Json</span><span class="o">::</span><span class="n">Value</span> <span class="n">dump_json</span><span class="p">;</span>
  <span class="n">Json</span><span class="o">::</span><span class="n">Value</span> <span class="n">gpu_info_json</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="nl">a</span> <span class="p">:</span> <span class="n">allocator_status_lists_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">Json</span><span class="o">::</span><span class="n">Value</span> <span class="n">device_json</span><span class="p">;</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;deviceMemUsedMax&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">deviceMemUsedMax</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;deviceMemUsedMin&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">deviceMemUsedMin</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;deviceMemPoolSize&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">deviceMemPoolSize</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;deviceMemStable&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">deviceMemStable</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;hostMemUsedMax&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">hostMemUsedMax</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;hostMemUsedMin&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">hostMemUsedMin</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;hostMemPoolSize&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">hostMemPoolSize</span><span class="p">);</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;swapReason&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">swapReason</span><span class="p">;</span>
    <span class="n">device_json</span><span class="p">[</span><span class="s">&#34;deviceMemUsedNvidia&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">Int64</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">gpu_info_json</span><span class="p">[</span><span class="n">a</span><span class="p">.</span><span class="n">gpu_pci_bus_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_json</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">dump_json</span><span class="p">[</span><span class="s">&#34;gpuUsageInfo&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_info_json</span><span class="p">;</span>

  <span class="n">Json</span><span class="o">::</span><span class="n">Value</span> <span class="n">sess_json</span><span class="p">;</span>
  <span class="n">uint64</span> <span class="n">max_duration</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="nl">s</span> <span class="p">:</span> <span class="n">sess_run_durations_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">uint64</span> <span class="n">du</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">duration_</span><span class="p">;</span>
    <span class="n">time_t</span> <span class="n">rec</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">recording_time_</span><span class="p">;</span>
    <span class="n">sess_json</span><span class="p">[</span><span class="s">&#34;graph_&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">first</span><span class="p">)]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">UInt64</span><span class="p">(</span><span class="n">du</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">du</span> <span class="o">&gt;</span> <span class="n">max_duration</span> <span class="o">&amp;&amp;</span> <span class="n">time</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">rec</span> <span class="o">&lt;</span> <span class="n">max_record_interval</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">max_duration</span> <span class="o">=</span> <span class="n">du</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">dump_json</span><span class="p">[</span><span class="s">&#34;miniBatchDuration&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Json</span><span class="o">::</span><span class="n">UInt64</span><span class="p">(</span><span class="n">max_duration</span><span class="p">);</span>
  <span class="n">dump_json</span><span class="p">[</span><span class="s">&#34;Durations&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sess_json</span><span class="p">;</span>

  <span class="n">Json</span><span class="o">::</span><span class="n">StreamWriterBuilder</span> <span class="n">stream_writer</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">Json</span><span class="o">::</span><span class="n">StreamWriter</span><span class="o">&gt;</span> <span class="n">writer</span><span class="p">(</span><span class="n">stream_writer</span><span class="p">.</span><span class="n">newStreamWriter</span><span class="p">());</span>
  <span class="n">std</span><span class="o">::</span><span class="n">ofstream</span> <span class="n">statistics_file</span><span class="p">;</span>
  <span class="n">statistics_file</span><span class="p">.</span><span class="n">open</span><span class="p">(</span><span class="n">gpu_statistics_file_</span><span class="p">);</span>
  <span class="n">writer</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="n">dump_json</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">statistics_file</span><span class="p">);</span>
  <span class="n">statistics_file</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>
  <span class="c1">// LOG(INFO) &lt;&lt; &#34;gpu_statistics_file updated.&#34;;
</span><span class="c1"></span><span class="p">}</span>
</code></pre></div><p>注册中间件</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="cp">#if GOOGLE_CUDA
</span><span class="cp"></span><span class="c1">// We register the GPUStatistics as a POST_SESSION_RUN action
</span><span class="c1">// during the initialization phase of the program.
</span><span class="c1"></span><span class="n">REGISTER_SESSION_RUN_ACTION</span><span class="p">(</span><span class="n">SessionRunActionRegistry</span><span class="o">::</span><span class="n">POST_SESSION_RUN</span><span class="p">,</span>
                            <span class="mi">1</span><span class="p">,</span> <span class="n">GPUStatistics</span><span class="p">);</span>
<span class="cp">#endif  </span><span class="c1">// GOOGLE_CUDA
</span></code></pre></div><h2 id="gpuopmanager">GpuOpManager</h2>
<blockquote>
<p>GpuOpManager continuously profiles the GPU operators execution time and
simply distributes idle time slots before launching the GPU operators.</p>
</blockquote>
<p>在GPUResourceManagement.cc 中实现了set()和get() 对应executor要执行OP数的接口，</p>
<p>在Executor中以一个线程的形式运行， 在ExecutorState::ExecutorState中新增了一个thread成员变量</p>
<p>修改Executor.cc</p>
<h3 id="构造函数里初始化gpu_op_manger_thread">构造函数里，初始化gpu_op_manger_thread</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"> <span class="c1">// 获取GPUResourceManagement
</span><span class="c1"></span>  <span class="n">GPUResourceManagement</span><span class="o">*</span> <span class="n">rm</span> <span class="o">=</span> <span class="n">GetGPUResourceManagement</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">rm</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">enable_op_management</span> <span class="o">=</span> <span class="p">(</span><span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">enable_op_management</span> <span class="o">&amp;&amp;</span> <span class="n">gpu_op_manager_thread_</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">gpu_op_manager_thread_</span> <span class="o">=</span>
      <span class="k">new</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ExecutorState</span><span class="o">::</span><span class="n">AsyncGPUOpManager</span><span class="p">,</span> <span class="k">this</span><span class="p">);</span>
  <span class="p">}</span>
</code></pre></div><p>manager负责插入时间槽， 也就是计算好sleep的时间， 释放资源</p>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="c1">// The manager thread which is in charge of inserting the time slot
</span><span class="c1">// before launching each queued async GPU op.
</span><span class="c1"></span><span class="kt">void</span> <span class="n">ExecutorState</span><span class="o">::</span><span class="n">AsyncGPUOpManager</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">uint64</span> <span class="n">sleep_time_us</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">need_to_insert_idle_time_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="n">GPUResourceManagement</span><span class="o">*</span> <span class="n">rm</span> <span class="o">=</span> <span class="n">GetGPUResourceManagement</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">rm</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">terminate_op_magager_thread_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">need_to_insert_idle_time_</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="nb">true</span> <span class="o">:</span> <span class="nb">false</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="kt">void</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="o">&gt;</span> <span class="n">queued_call_func</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
     
    <span class="c1">// 1）从队列中获取第一个要执行的Op  
</span><span class="c1"></span>    <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">queued_call_func</span> <span class="o">=</span> <span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">queued_call_func</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">//2）调用这个Op
</span><span class="c1"></span>      <span class="n">queued_call_func</span><span class="p">();</span>
      <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
      <span class="c1">//3）从队列中删除这个op
</span><span class="c1"></span>      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
        <span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
        <span class="c1">// 数目减一
</span><span class="c1"></span>        <span class="n">num_queued_op</span><span class="p">.</span><span class="n">fetch_sub</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>

      <span class="c1">// Estimate idle time 预测空闲时间
</span><span class="c1"></span>      <span class="n">uint64</span> <span class="n">idle_time</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">();</span>
      <span class="n">uint64</span> <span class="n">queued_op_num</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetExecutorQueuedOpNum</span><span class="p">(</span><span class="n">impl_</span><span class="p">);</span>
      <span class="n">idle_time</span> <span class="o">=</span> <span class="n">queued_op_num</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">?</span> <span class="p">(</span><span class="n">idle_time</span> <span class="o">/</span> <span class="n">queued_op_num</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
      <span class="c1">// 等待一个op的 idle_time
</span><span class="c1"></span>      <span class="n">usleep</span><span class="p">(</span><span class="n">idle_time</span><span class="p">);</span>
      <span class="c1">// 设置剩余时间
</span><span class="c1"></span>      <span class="n">uint64</span> <span class="n">remain_time</span> <span class="o">=</span> <span class="n">rm</span><span class="o">-&gt;</span><span class="n">GetEstimatedIdleTime</span><span class="p">();</span>
      <span class="n">remain_time</span> <span class="o">=</span> <span class="n">remain_time</span> <span class="o">&gt;</span> <span class="n">idle_time</span> <span class="o">?</span> <span class="p">(</span><span class="n">remain_time</span> <span class="o">-</span> <span class="n">idle_time</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
      <span class="n">rm</span><span class="o">-&gt;</span><span class="n">SetEstimatedIdleTime</span><span class="p">(</span><span class="n">remain_time</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">usleep</span><span class="p">(</span><span class="n">default_check_interval</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div><h3 id="process-处理过程">Process 处理过程</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="c1">// Process():
</span><span class="c1"></span>      <span class="p">...</span>      
          
        <span class="n">Device</span><span class="o">*</span> <span class="n">kernel_device</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">params_</span><span class="p">.</span><span class="n">device</span><span class="p">;</span>

        <span class="c1">// Only enqueue this op if it is an async GPU op.
</span><span class="c1"></span>		<span class="c1">// 1 如果是一个异步Op， 则加入到异步OP队列
</span><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">need_to_insert_idle_time_</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">kernel_device</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()).</span><span class="n">find</span><span class="p">(</span><span class="s">&#34;GPU&#34;</span><span class="p">)</span> <span class="o">!=</span> <span class="n">string</span><span class="o">::</span><span class="n">npos</span><span class="p">)</span> <span class="p">{</span>
          <span class="c1">// Enqueue this GPU op therefore we can insert a time slot before launching this op.
</span><span class="c1"></span>          <span class="c1">// 在启动这个op之前我们可以插入一个时间槽
</span><span class="c1"></span>          <span class="n">sess_op_num</span><span class="o">++</span><span class="p">;</span>

          <span class="k">const</span> <span class="n">GraphView</span><span class="o">&amp;</span> <span class="n">gview_t</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">gview_</span><span class="p">;</span>
          <span class="k">const</span> <span class="n">NodeItem</span><span class="o">&amp;</span> <span class="n">item_t</span> <span class="o">=</span> <span class="o">*</span><span class="n">gview_t</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="n">id</span><span class="p">);</span>
          <span class="n">AsyncState</span><span class="o">*</span> <span class="n">state</span> <span class="o">=</span>
              <span class="k">new</span> <span class="n">AsyncState</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">tagged_node</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">item_t</span><span class="p">,</span> <span class="n">first_input</span><span class="p">,</span> <span class="n">stats</span><span class="p">);</span>

          <span class="k">auto</span> <span class="n">async_gpu_kernel</span> <span class="o">=</span> <span class="p">[</span><span class="k">this</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">op_kernel</span><span class="p">,</span> <span class="n">device</span><span class="p">]</span> <span class="p">{</span>
            <span class="n">AsyncOpKernel</span><span class="o">*</span> <span class="n">async</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">kernel</span><span class="o">-&gt;</span><span class="n">AsAsync</span><span class="p">();</span>

            <span class="n">DCHECK</span><span class="p">(</span><span class="n">async</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">);</span>
            <span class="k">auto</span> <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="k">this</span><span class="p">,</span> <span class="n">state</span><span class="p">]()</span> <span class="p">{</span>
              <span class="n">Device</span><span class="o">*</span> <span class="n">device</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">params_</span><span class="p">.</span><span class="n">device</span><span class="p">;</span>
              <span class="n">NodeExecStatsInterface</span><span class="o">*</span> <span class="n">stats</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">stats</span><span class="p">;</span>  <span class="c1">// Shorthand
</span><span class="c1"></span>              <span class="n">Entry</span><span class="o">*</span> <span class="n">first_input</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">first_input</span><span class="p">;</span>     <span class="c1">// Shorthand
</span><span class="c1"></span>
              <span class="n">nodestats</span><span class="o">::</span><span class="n">SetOpEnd</span><span class="p">(</span><span class="n">stats</span><span class="p">);</span>
              <span class="n">EntryVector</span> <span class="n">outputs</span><span class="p">;</span>
              <span class="n">Status</span> <span class="n">s</span> <span class="o">=</span> <span class="n">ProcessOutputs</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="n">stats</span><span class="p">);</span>
              <span class="n">nodestats</span><span class="o">::</span><span class="n">SetMemory</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">);</span>
              <span class="k">if</span> <span class="p">(</span><span class="n">vlog_</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">VLOG</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Async kernel done: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">id</span><span class="p">()</span>
                        <span class="o">&lt;&lt;</span> <span class="s">&#34; step &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">step_id_</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; &#34;</span>
                        <span class="o">&lt;&lt;</span> <span class="n">SummarizeNode</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">)</span>
                        <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">is_dead</span> <span class="o">?</span> <span class="s">&#34; is dead&#34;</span> <span class="o">:</span> <span class="s">&#34;&#34;</span><span class="p">)</span>
                        <span class="o">&lt;&lt;</span> <span class="s">&#34; device: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">();</span>
              <span class="p">}</span>

              <span class="c1">// Clears inputs.
</span><span class="c1"></span>              <span class="k">const</span> <span class="kt">int</span> <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">num_inputs</span><span class="p">;</span>
              <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_inputs</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
                <span class="p">(</span><span class="n">first_input</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">ClearVal</span><span class="p">();</span>
              <span class="p">}</span>
              <span class="n">FrameState</span><span class="o">*</span> <span class="n">input_frame</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">input_frame</span><span class="p">;</span>
              <span class="k">const</span> <span class="n">int64</span> <span class="n">input_iter</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">input_iter</span><span class="p">;</span>
              <span class="k">const</span> <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">id</span><span class="p">();</span>
              <span class="n">MaybeMarkCompleted</span><span class="p">(</span><span class="n">input_frame</span><span class="p">,</span> <span class="n">input_iter</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>
              <span class="n">TaggedNodeSeq</span> <span class="n">ready</span><span class="p">;</span>
              <span class="k">if</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
                <span class="n">PropagateOutputs</span><span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">,</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ready</span><span class="p">);</span>
              <span class="p">}</span>
              <span class="n">outputs</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
              <span class="k">if</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">ok</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">device_record_tensor_accesses_</span><span class="p">)</span> <span class="p">{</span>
                <span class="c1">// Get the list of all tensors accessed during the execution
</span><span class="c1"></span>                <span class="n">TensorReferenceVector</span> <span class="n">accessed</span><span class="p">;</span>
                <span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">.</span><span class="n">retrieve_accessed_tensors</span><span class="p">(</span><span class="o">&amp;</span><span class="n">accessed</span><span class="p">);</span>
                <span class="n">nodestats</span><span class="o">::</span><span class="n">SetReferencedTensors</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="n">accessed</span><span class="p">);</span>
                <span class="c1">// callee takes ownership of the vector
</span><span class="c1"></span>                <span class="n">device</span><span class="o">-&gt;</span><span class="n">ConsumeListOfAccessedTensors</span><span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">.</span><span class="n">op_device_context</span><span class="p">(),</span>
                                                    <span class="n">accessed</span><span class="p">);</span>
              <span class="p">}</span>
              <span class="k">const</span> <span class="kt">bool</span> <span class="n">completed</span> <span class="o">=</span>
                  <span class="n">NodeDone</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">,</span> <span class="n">ready</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">);</span>
              <span class="k">delete</span> <span class="n">state</span><span class="p">;</span>
              <span class="k">if</span> <span class="p">(</span><span class="n">completed</span><span class="p">)</span> <span class="n">ScheduleFinish</span><span class="p">();</span>
            <span class="p">};</span>
              
            <span class="n">nodestats</span><span class="o">::</span><span class="n">SetOpStart</span><span class="p">(</span><span class="n">stats</span><span class="p">);</span>
            <span class="p">{</span>
              <span class="n">profiler</span><span class="o">::</span><span class="n">TraceMe</span> <span class="n">activity</span><span class="p">(</span>
                  <span class="p">[</span><span class="o">&amp;</span><span class="p">]</span> <span class="p">{</span>
                    <span class="k">return</span> <span class="n">strings</span><span class="o">::</span><span class="n">StrCat</span><span class="p">(</span>
                        <span class="n">op_kernel</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">(),</span> <span class="s">&#34;:&#34;</span><span class="p">,</span> <span class="n">op_kernel</span><span class="o">-&gt;</span><span class="n">type_string</span><span class="p">(),</span>
                        <span class="s">&#34;#id=&#34;</span><span class="p">,</span> <span class="n">step_container_</span> <span class="o">?</span> <span class="n">step_container_</span><span class="o">-&gt;</span><span class="n">step_id</span><span class="p">()</span> <span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
                        <span class="s">&#34;,device=&#34;</span><span class="p">,</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">(),</span> <span class="s">&#34;,async=true#&#34;</span><span class="p">);</span>
                  <span class="p">},</span>
                  <span class="n">profiler</span><span class="o">::</span><span class="n">GetTFTraceMeLevel</span><span class="p">(</span><span class="n">op_kernel</span><span class="o">-&gt;</span><span class="n">IsExpensive</span><span class="p">()));</span>
              <span class="n">device</span><span class="o">-&gt;</span><span class="n">ComputeAsync</span><span class="p">(</span><span class="n">async</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">,</span> <span class="n">done</span><span class="p">);</span>
            <span class="p">}</span>
          <span class="p">};</span>

          <span class="c1">// Enqueue this asyn GPU op.
</span><span class="c1"></span>          <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
          <span class="n">async_gpu_op_queue</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">async_gpu_kernel</span><span class="p">);</span> <span class="c1">//添加kernel到op队列
</span><span class="c1"></span>          <span class="n">num_queued_op</span><span class="p">.</span><span class="n">fetch_add</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span> <span class="c1">//加一
</span><span class="c1"></span>          <span class="n">async_gpu_op_queue_lock_</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            
          <span class="c1">//2 如果不是异步的OP 则不加入
</span><span class="c1"></span>          <span class="c1">// Do not enqueue this op.
</span><span class="c1"></span>          <span class="n">AsyncOpKernel</span><span class="o">*</span> <span class="n">async</span> <span class="o">=</span> <span class="n">item</span><span class="p">.</span><span class="n">kernel</span><span class="o">-&gt;</span><span class="n">AsAsync</span><span class="p">();</span>
          <span class="n">DCHECK</span><span class="p">(</span><span class="n">async</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">);</span>
          <span class="n">AsyncState</span><span class="o">*</span> <span class="n">state</span> <span class="o">=</span>
              <span class="k">new</span> <span class="n">AsyncState</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">tagged_node</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">item</span><span class="p">,</span> <span class="n">first_input</span><span class="p">,</span> <span class="n">stats</span><span class="p">);</span>

          <span class="k">auto</span> <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="k">this</span><span class="p">,</span> <span class="n">state</span><span class="p">]()</span> <span class="p">{</span>
            <span class="n">Device</span><span class="o">*</span> <span class="n">device</span> <span class="o">=</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">params_</span><span class="p">.</span><span class="n">device</span><span class="p">;</span>
            <span class="n">NodeExecStatsInterface</span><span class="o">*</span> <span class="n">stats</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">stats</span><span class="p">;</span>  <span class="c1">// Shorthand
</span><span class="c1"></span>            <span class="n">Entry</span><span class="o">*</span> <span class="n">first_input</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">first_input</span><span class="p">;</span>       <span class="c1">// Shorthand
</span><span class="c1"></span>
            <span class="n">nodestats</span><span class="o">::</span><span class="n">SetOpEnd</span><span class="p">(</span><span class="n">stats</span><span class="p">);</span>
            <span class="n">EntryVector</span> <span class="n">outputs</span><span class="p">;</span>
            <span class="n">Status</span> <span class="n">s</span> <span class="o">=</span> <span class="n">ProcessOutputs</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="n">stats</span><span class="p">);</span>
            <span class="n">nodestats</span><span class="o">::</span><span class="n">SetMemory</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">);</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">vlog_</span><span class="p">)</span> <span class="p">{</span>
              <span class="n">VLOG</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Async kernel done: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">id</span><span class="p">()</span>
                      <span class="o">&lt;&lt;</span> <span class="s">&#34; step &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">step_id_</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; &#34;</span>
                      <span class="o">&lt;&lt;</span> <span class="n">SummarizeNode</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">)</span>
                      <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">is_dead</span> <span class="o">?</span> <span class="s">&#34; is dead&#34;</span> <span class="o">:</span> <span class="s">&#34;&#34;</span><span class="p">)</span>
                      <span class="o">&lt;&lt;</span> <span class="s">&#34; device: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">();</span>
            <span class="p">}</span>

            <span class="c1">// Clears inputs.
</span><span class="c1"></span>            <span class="k">const</span> <span class="kt">int</span> <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">num_inputs</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_inputs</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
              <span class="p">(</span><span class="n">first_input</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">ClearVal</span><span class="p">();</span>
            <span class="p">}</span>
            <span class="n">FrameState</span><span class="o">*</span> <span class="n">input_frame</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">input_frame</span><span class="p">;</span>
            <span class="k">const</span> <span class="n">int64</span> <span class="n">input_iter</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">input_iter</span><span class="p">;</span>
            <span class="k">const</span> <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">.</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">id</span><span class="p">();</span>
            <span class="n">MaybeMarkCompleted</span><span class="p">(</span><span class="n">input_frame</span><span class="p">,</span> <span class="n">input_iter</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>
            <span class="n">TaggedNodeSeq</span> <span class="n">ready</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
              <span class="n">PropagateOutputs</span><span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">tagged_node</span><span class="p">,</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ready</span><span class="p">);</span>
            <span class="p">}</span>
            <span class="n">outputs</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">s</span><span class="p">.</span><span class="n">ok</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">impl_</span><span class="o">-&gt;</span><span class="n">device_record_tensor_accesses_</span><span class="p">)</span> <span class="p">{</span>
              <span class="c1">// Get the list of all tensors accessed during the execution
</span><span class="c1"></span>              <span class="n">TensorReferenceVector</span> <span class="n">accessed</span><span class="p">;</span>
              <span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">.</span><span class="n">retrieve_accessed_tensors</span><span class="p">(</span><span class="o">&amp;</span><span class="n">accessed</span><span class="p">);</span>
              <span class="n">nodestats</span><span class="o">::</span><span class="n">SetReferencedTensors</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="n">accessed</span><span class="p">);</span>
              <span class="c1">// callee takes ownership of the vector
</span><span class="c1"></span>              <span class="n">device</span><span class="o">-&gt;</span><span class="n">ConsumeListOfAccessedTensors</span><span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">.</span><span class="n">op_device_context</span><span class="p">(),</span>
                                                  <span class="n">accessed</span><span class="p">);</span>
            <span class="p">}</span>
            <span class="k">const</span> <span class="kt">bool</span> <span class="n">completed</span> <span class="o">=</span>
                <span class="n">NodeDone</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">state</span><span class="o">-&gt;</span><span class="n">item</span><span class="o">-&gt;</span><span class="n">node</span><span class="p">,</span> <span class="n">ready</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">);</span>
            <span class="k">delete</span> <span class="n">state</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">completed</span><span class="p">)</span> <span class="n">ScheduleFinish</span><span class="p">();</span>
          <span class="p">};</span>
          <span class="n">nodestats</span><span class="o">::</span><span class="n">SetOpStart</span><span class="p">(</span><span class="n">stats</span><span class="p">);</span>
          <span class="p">{</span>
            <span class="n">profiler</span><span class="o">::</span><span class="n">TraceMe</span> <span class="n">activity</span><span class="p">(</span>
                <span class="p">[</span><span class="o">&amp;</span><span class="p">]</span> <span class="p">{</span>
                  <span class="k">return</span> <span class="n">strings</span><span class="o">::</span><span class="n">StrCat</span><span class="p">(</span>
                      <span class="n">op_kernel</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">(),</span> <span class="s">&#34;:&#34;</span><span class="p">,</span> <span class="n">op_kernel</span><span class="o">-&gt;</span><span class="n">type_string</span><span class="p">(),</span>
                      <span class="s">&#34;#id=&#34;</span><span class="p">,</span> <span class="n">step_container_</span> <span class="o">?</span> <span class="n">step_container_</span><span class="o">-&gt;</span><span class="n">step_id</span><span class="p">()</span> <span class="o">:</span> <span class="mi">0</span><span class="p">,</span>
                      <span class="s">&#34;,device=&#34;</span><span class="p">,</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">(),</span> <span class="s">&#34;,async=true#&#34;</span><span class="p">);</span>
                <span class="p">},</span>
                <span class="n">profiler</span><span class="o">::</span><span class="n">GetTFTraceMeLevel</span><span class="p">(</span><span class="n">op_kernel</span><span class="o">-&gt;</span><span class="n">IsExpensive</span><span class="p">()));</span>
            <span class="n">device</span><span class="o">-&gt;</span><span class="n">ComputeAsync</span><span class="p">(</span><span class="n">async</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">ctx</span><span class="p">,</span> <span class="n">done</span><span class="p">);</span>
          <span class="p">}</span>
        <span class="p">}</span>
</code></pre></div><h3 id="结束函数">结束函数</h3>
<div class="highlight"><pre class="chroma"><code class="language-cc" data-lang="cc"><span class="c1">// Finish():
</span><span class="c1"></span>

<span class="k">if</span> <span class="p">(</span><span class="n">gpu_op_manager_thread_</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">terminate_op_magager_thread_</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">gpu_op_manager_thread_</span><span class="o">-&gt;</span><span class="n">joinable</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">gpu_op_manager_thread_</span><span class="o">-&gt;</span><span class="n">join</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="k">delete</span> <span class="n">gpu_op_manager_thread_</span><span class="p">;</span>
    <span class="n">terminate_op_magager_thread_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></div><h2 id="总结">总结</h2>
<div class="mermaid">
  

graph TD
  A>gpu_resource_manage_file]
  B[SessionRunRegistry]
  C[SessionRunAction] 
  D[Executor]
  E[GPUResouceManagement]
  F[GPU Statistic]
  G[GpuOpManager]
  H[GpuUsageAdjustment]
  I(dump gpu statistic)
  J[GPU Process State]
  K[GPUVMemAllocator]
  L[GPUAdjustableAllocator]

  A -->|FileListener| E
  B -->|Register| E
  E -->|need_to_adjust_memory_| H
  H -->|new| L
  H -->|get| K
  C -->|Derive| E
  C -->|Derive| F
  B -->|Register| F
  F -->|need_to_dump_statistics_| I

  B -->|Run| C

  J -->|maybe_create_gpu_vmem_allocator|K
  D -->|run thread| G
  E -->|GetEstimatedIdleTime| G


</div>

</section>


    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer="true"
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    <aside class="related-contents--wrapper">
    
    
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (DISQUS) {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2022 Tweakzx
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.5.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">Table of contents</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ol>
    <li><a href="#antman对tensorflow的代码修改">Antman对Tensorflow的代码修改</a></li>
    <li><a href="#gpuvmemallocator">GPUVMemAllocator</a>
      <ol>
        <li><a href="#创建allocator">创建allocator</a></li>
        <li><a href="#分配虚拟内存">分配虚拟内存</a></li>
      </ol>
    </li>
    <li><a href="#sessionrunactionregistry中间件框架">SessionRunActionRegistry（中间件框架）</a>
      <ol>
        <li><a href="#修改direct_sessioncc-和-master_sessioncc">修改direct_session.cc 和 master_session.cc</a></li>
        <li><a href="#注册action">注册action</a></li>
      </ol>
    </li>
    <li><a href="#gpu-memory-limit-adjustment">GPU memory limit adjustment</a>
      <ol>
        <li><a href="#修改了tensorflow-原本的-bfc_allocatorh">修改了tensorflow 原本的 BFC_Allocator.h</a></li>
        <li><a href="#增加了扩缩显存分配的函数">增加了扩缩显存分配的函数</a></li>
      </ol>
    </li>
    <li><a href="#file-listener">File Listener</a></li>
    <li><a href="#gpu-resource-management中间件">GPU resource management（中间件）</a>
      <ol>
        <li><a href="#gpuresourcemanagement">GPUResourceManagement()</a></li>
        <li><a href="#注册中间件">注册中间件</a></li>
        <li><a href="#实现函数runaction">实现函数RunAction（&hellip;）</a></li>
        <li><a href="#gpuusageadjustmentcc">GPUUsageAdjustment.cc</a></li>
      </ol>
    </li>
    <li><a href="#gpu-statistics-中间件">Gpu Statistics （中间件）</a></li>
    <li><a href="#gpuopmanager">GpuOpManager</a>
      <ol>
        <li><a href="#构造函数里初始化gpu_op_manger_thread">构造函数里，初始化gpu_op_manger_thread</a></li>
        <li><a href="#process-处理过程">Process 处理过程</a></li>
        <li><a href="#结束函数">结束函数</a></li>
      </ol>
    </li>
    <li><a href="#总结">总结</a></li>
  </ol>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
