<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>论文 on Tweakzx</title>
    <link>https://tweakzx.github.io/categories/%E8%AE%BA%E6%96%87/</link>
    <description>Recent content in 论文 on Tweakzx</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Aug 2022 15:41:42 +0800</lastBuildDate><atom:link href="https://tweakzx.github.io/categories/%E8%AE%BA%E6%96%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AntMan论文阅读笔记</title>
      <link>https://tweakzx.github.io/p/antman%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Mon, 29 Aug 2022 15:41:42 +0800</pubDate>
      
      <guid>https://tweakzx.github.io/p/antman%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>《AntMan: Dynamic Scaling on GPU Clusters for Deep Learning》论文阅读笔记 Abstract 如何在大规模GPU集群上有效调度深度学习工作， 对于工作性能，系统吞吐量和硬件利用率至关重要。
随着深度学习的工作量变得更加复杂，它变得越来越具有挑战性。
本文将介绍Antman， 这是一种深入学习的基础设施，该基础架构共同设计了集群调度程序，并已在阿里巴巴部署在生产中，以管理数以万计的每日深度学习工作。
 Antman适应深度学习训练的波动资源需求。因此，它利用备用GPU资源在共享GPU上共同执行多个作业。 Antman利用深度学习训练的独特特征，在深度学习框架内为显存和计算资源引入动态缩放机制。这允许job之间的细粒度协调并防止工作干扰。 评估表明，Antman在我们的多租户集群中不损害公平性的情况下，整体将显存利用率提高了42％，计算资源利用率提高了34％，为有效利用大规模的GPU提出了新方法。  Introduction  在共享多租户的DL集群， 许多工作排队等待资源的时候会导致GPU利用率低下，有两个原因  大多数的训练任务在执行过程中不能完全利用GPU资源  训练一个DL模型通常需要多种计算的混合 当使用分布式的训练的时候，90%的时间会被浪费到网络通信上   基于资源预留的集群调度方案导致显著的GPU空闲，DL工作中总有部分资源没有投入使用  例如，随机梯度下降是同步的，需要获取所有的资源以进行gang-scheduling， 在得到所有资源之前，已得到的部分资源就会陷入空闲     在共享GPU上进行packing job  可以提高GPU的利用率，可以使得同样的集群整体上胜任更多的job。 但是这个策略在生产集群上很少使用， 原因  尽管提升GPU的利用率是有利的，但也要保证resource-guarantee jobs（RGJ，资源保证性job）的性能。同一个GPU上同时执行多个job会导致干扰&amp;ndash;&amp;gt;RGJ性能出现显著下降 job packing策略可以给并发job引入内存竞争。job需要的资源陡然增加的话，有可能导致训练任务failure。   因此，job资源的独占分配在显存的GPU集群生产环境中比较典型   我们提出AntMan  简述  一个DL系统提高GPU集群的利用率 同时保证公平性与RGJ的性能 通过合作性的资源扩缩来减少job干扰   DL系统中引入了新的分配机制在job训练过程中来动态地精确分配所需资源（显存和计算单元） 使用超卖机制使得任何空闲的GPU资源（显存和计算单元）都能被利用 重新设计了集群调度器和DL框架来适应生产job的波动的资源特点  通过  框架信息感知调度 透明显存扩展 快速可持续的任务间协调   使用这个结构，Antman为DL任务的同时执行的policy design 开辟了空间   AntMan采用了简单且有效的策略来最大化集群吞吐  为RGJ提供资源保证的同时 分配一些偶然性的任务来尽力而为的利用GPU资源（低优先级且不保证资源）     本文主要贡献如下  对生产环境的DL集群进行调研，发现低利用率来自于三个方面：硬件，集群调度，job行为 在DL框架中为显存和计算单元管理引入了两种动态放缩机制，来解决GPU共享问题。新机制利用DL的工作特征来动态调整DL job的资源使用情况，在作业执行期间。 通过共同设计集群调度器和DL框架以利用动态缩放机制，我们为GPU共享引入了一种新的工业方法。这在多租户集群中维护工作服务级协议（SLA），同时通过机会调度来改善集群利用率。 在超过5000个GPU上进行了实验    Motivation deep learning Training  深度学习训练通常包括数百万次迭代，每个迭代过程都称为mini-batch。  通常，训练mini-batch可以分为三个阶段。  首先，计算样品和模型权重以产生一组得分，称为forward pass 其次，使用目标函数在产生的分数和所需的分数之间计算loss error。然后，损失通过模型向后扩散，以计算梯度，称为backward pass。 最后，通过由优化器定义的学习率来缩放梯度，以更新模型参数。   正向通行的计算输出通常包括许多数据输出，每个数据输出称为张量。这些张量应暂时保存在内存中，并被向后通过以计算梯度。通常，为了监视培训中的模型质量，定期触发评估.</description>
    </item>
    
    <item>
      <title>GaiaGPU 论文阅读笔记</title>
      <link>https://tweakzx.github.io/p/gaiagpu-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 26 Aug 2022 17:20:32 +0800</pubDate>
      
      <guid>https://tweakzx.github.io/p/gaiagpu-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>《GaiaGPU：Sharing GPUs in Container Clouds》论文笔记 Abstract  对于云服务的提供商， 如何在容器间共享GPU， 是一个有吸引力的问题  容器的轻量与伸缩性 GPU强大的并行计算能力 在云环境，容器需要使用一个或多个GPU来满足资源需要的同时， 容器独占式的GPU往往使用率很低   我们提出GaiaGPU，能够在容器间共享显存和算力  将物理GPU划分为多个虚拟GPU 采用弹性资源分配和动态资源分配来提高资源利用率 实验结果显示， 有效的实现了容器间资源的分配和隔离的同时，平均只增加了1.015%的开销。    Introduction  容器化是一种虚拟化技术  涉及到量身定制一个标准操作系统，方便它在一个物理机上运行由多个用户处理的不同应用程序 与VM模拟底层硬件不同  容器模拟的是操作系统 轻量，可伸缩，易部署 微服务打包与发布应用的事实标准   云服务提供商整合容器编排框架（如k8s）到基础架构中来提供容器云   GPU 图像处理单元  有很强的并行处理能力  因为一个芯片上集成了数以千计的计算核 GPU被广泛用于计算密集型任务，以加快计算 随着技术的发展趋势，现代GPU内将集成入越来越多的计算资源   CUDA是多功能GPU最流行的平台，提供了API方便GPU的使用 卓越的性能吸引了很多云提供商将GPU引入云环境  在云环境中，部署在容器中的一个应用程序可能需要一个或多个GPU才能执行， 而另一方面，应用程序的专用GPU资源导致资源不足。 因此，如何在不同的容器中共享GPU对大多数云提供商都非常感兴趣     GPU虚拟化技术是在隔离的虚拟环境（例如VM， 容器）之间共享GPU的技术  多数的GPU虚拟化技术应用于VM， 容器间的虚拟化技术还在起始阶段 现阶段的基于容器的GPU虚拟化技术有以下局限性  需要特定的硬件设备（NVIDIA GRID） 将一整个GPU分配给单个容器， 不能共享 （NVIDIA Docker） 容器间只能共享GPU显存 （ConvGPU） 只支持单个GPU （ConvGPU）     我们提出GaiaGPU，能够在容器间透明地共享显存和算力  用户不用修改容器镜像来共享底层GPU  我们使用k8s的device plugin 框架将物理GPU划分为多个虚拟GPU   每个镜像可以按需分配一个或者多个vGPU 提供了两者方式在运行时更改镜像资源  弹性资源分配：暂时改变资源 动态资源分配：永久改变资源     vGPU包括GPU显存和计算资源  共享显存  容器包含GPU显存的一小部分 vGPU分配的是GPU的物理内存   共享计算资源  共享计算资源意味着每个容器都拥有GPU线程的一部分以并行执行计算。 VGPU的计算资源由GPU的利用率衡量（采样时段内， 容器使用GPU的时间比例）     总结：本文做了如下贡献  提出了GaiaGPU：一种在容器间透明共享显存与算力的方法 采用弹性分配和动态分配的方式提高了资源的利用率 进行了四个实验来验证GaiaGPU的性能。结果：实现了容器间资源的分配和隔离的同时，平均只增加了1.</description>
    </item>
    
    <item>
      <title>PolarDB Serverless论文阅读报告</title>
      <link>https://tweakzx.github.io/p/polardb-serverless%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Wed, 22 Dec 2021 14:17:12 +0800</pubDate>
      
      <guid>https://tweakzx.github.io/p/polardb-serverless%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</guid>
      <description>PolarDB Serverless论文阅读报告 摘要 数据库管理系统的上云是近期很火的研究趋势，因为这样可以获得更高的弹性，可用性以及更低的成本，传统的独块的数据库架构很难满足这样的要求。高速网络与新的内存技术（例如RDMA）的发展，给分散式数据库带来了可能：它将原先的独块的服务器资源分离解耦到资源池中，再通过高速网络连接起来。下一代的云原生数据库就是为了分散化的数据中心而设计。
PolarDB Serverless 遵循分散式的设计范式而设计，解耦了计算节点的CPU，内存，存储资源。
 每种资源可以随需求而独立的增长或缩小，保障可靠性的同时，可以进行多维度的按需供应。 同时采用了优化策略如乐观锁和预取策略来改善性能。 还可以实现更快的故障恢复。  介绍 使用云数据库的三个好处：  按需付费可以使得用户减小开支。 更高的资源弹性可以应对短暂的资源使用高峰期。 更快的升级与更快的错误修复。快速的升级迭代可以保证产品竞争力，错误修复可以在不影响产品可用性的前提下进行。  经典的云数据库的架构  monolithic machine 独块的机器  特点：所有的资源都是紧耦合的 问题：  在进行数据库实例到机器的分发过程中要解决一个装箱问题 总是有碎片化的资源，难以达到高的使用率 运行时不能根据负载调整资源 资源间的命运共享，一个资源的故障会导致其他资源的故障，不能独立透明地修复故障，导致修复时间很长     存算分离的架构：  两种：  virtual machine with remote disk 搭载远程硬盘的虚拟机 shared storage 共享存储   优点  DBaaS可以提高存储池的使用率 共享存储可以进一步减少成本：原数据与只读备份可以共享存储   问题  CPU和内存的装箱问题依旧存在 缺乏灵活可放缩的内存资源 每个只读备份在内存中都要有冗余的内存开销      本文提出了一种新架构，分散架构（disaggragation architecture)  运行在分散的数据中心（DDC），CPU、内存和存储解耦，资源间通过高速网络连接 效果  每一种资源都可以提高其利用率，可以独立地放缩其资源量 解决了资源间的命运共享问题，资源故障可以被独立修复 数据页可以在远程内存池中被共享，所以解决了备份的内存冗余问题    云原生数据库 多数的云数据库是基于共享存储的架构，内存和CPU绑成最小的资源单元，只能按照最小资源单元的粒度来增长和释放资源，这会带来很多的资源浪费。</description>
    </item>
    
  </channel>
</rss>
