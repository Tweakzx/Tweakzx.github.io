---
title: "【论文笔记】HUVM论文阅读笔记"
author: "Tweakzx"
description: 
date: 2023-06-25T16:38:08+08:00
image: https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202306261236383.png
math: 
license: 
hidden: false
comments: true
draft: false
categories: 
tags: 
---

# Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory

**摘要**

- 随着对 GPU 需求的不断增长，大多数组织允许用户共享多 GPU 服务器。
  - 然而，我们观察到，当合并显示高度不同的资源需求的各种工作负载时，跨 GPU 的内存空间没有得到充分有效的利用。
  - 这是因为当前的内存管理技术仅仅是为单个 GPU 设计的，而不是为共享多 GPU 环境设计的。

- 本研究提出一种新颖的方法，通过结合相邻 GPU 的暂时空闲内存，为 GPU 提供一种虚拟内存空间的错觉，称为分层统一虚拟内存(HUVM)。
  - 由于现代的 GPU 通过快速互连相互连接，与通过 PCIe 的主机内存相比，它提供了较低的访问相邻 GPU 内存的延迟。
  - 在 HUVM 之上，我们设计了一个新的内存管理器，称为 memHarvester，以有效和高效地获取临时可用的邻居 GPU 的内存。
  - 对于不同的整合情景与 DNN 训练和图分析工作负载，我们的实验结果显示，高达2.71倍的性能改善相比，以前的方法在多 GPU 环境中。

## 引言

- 随着对 GPU 需求的爆炸式增长，在学术界和工业界，在一台服务器上安装多个 GPU 并使其可共享已经成为一种常见的做法。
  - 业内许多企业已经建立了由一组多 GPU 服务器组成的大型 GPU 集群，以满足从深度学习[1,13,18,26,36]到图形应用[6,10,19,31]的各种工作负载的需求，同时通过共享节省基础设施成本。
  - 在这样的多 GPU 服务器中实现高 GPU 资源效率仍然是一个挑战。
  - 图1表明，当前的内存管理技术对于共享多 GPU 来说不够有效多个作业在 GPU 之间独立运行的环境。虽然在一个或几个 GPU 中，从几百 MB 到几 GB 的少量内存仍然处于空闲状态，但是在内存压力较大的情况下，其他 GPU 依赖主机内存作为交换设备，这比同一服务器中的远程 GPU 要慢得多。
  - ![image-20230626093958673](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202306260942806.png)

- GPU 供应商也面临着扩展单个 GPU 内存容量的挑战。
  - 为了克服 GPU 的有限容量，以前的一系列研究旨在利用主机内存给任务提供了一种无限内存的错觉[11,14,17,25,28]。
  - 然而，在商品化的多 GPU 系统中，这些工作都没有利用相邻 GPU 的空闲内存。
  - 由于现代 GPU 服务器通常配备8 ~ 16个 GPU，通过 NVLink 等高速互连连接，访问相邻 GPU 的空闲内存要比访问主机的空闲内存快得多。
    - 例如，NVIDIA DGX-2有16个 GPU，通过 NVLink 3.0进行点对点连接，在600GB/s 的双向带宽下产生512GB 的 GPU 内存池[23]。
    - 另一方面，通过最新的 PCIe 4.0将 GPU 内存交换到主机 DRAM 可以只利用32GB/s 的带宽。

- 在本研究中，我们介绍一种新的方法，为 GPU 提供虚拟内存空间的假象，称为分层统一虚拟内存(HUVM) 
  - 它由本地 GPU、相邻 GPU 的空闲内存和主机内存组成
  - HUVM 通过以最小的性能开销增加有效的内存空间，为内存虚拟化开辟了一个新的机会。
    - 当本地 GPU 内存没有空闲空间时，HUVM 利用相邻 GPU 中的空余内存作为 GPU 和主机之间的受害者缓存，而不是直接将数据交换到主机内存。

- 然而，由于空闲内存的数量是高度可变的，并且是先验未知的，因此要有效和高效地获取相邻 GPU 内存中的零星可用的一小部分是具有挑战性的。
  - 从单一 GPU 的角度出发，重新设计了现代多 GPU 服务器的内存管理方案。
  - HUVM 系统必须找到最好的利用内存的一小部分，同时尽量减少对相邻 GPU 中运行的工作负载的性能影响。

- 为此，我们提出了一个 HUVM 的内存管理器，称为 memHarvester，在 GPU 驱动层实现。MemHarvester 在 HUVM 中作为数据路径的集中协调器。
  - 作为 memHarvester 的一个重要组成部分，我们提出了一种新的多路并行预取器，它利用了 HUVM 的路径分集，包括 PCIe 和 NVLink。
    - 与以前的许多方法[11,14,17,25,28]不同，memHarvester 首先通过 NVLink 从空闲内存预取数据到本地 GPU。
    - 同时，如果连接到邻居 GPU 的 PCIe 通道不竞争，memHarvester 允许通过 PCIe 通道并行地从主机内存预取数据到邻居 GPU 的空闲内存。
    - 因此，我们可以有效地将从主机存储器获取数据的延迟转换为空闲存储器的延迟。
  - MemHarvester 管理收获内存的空间。
    - 由于空闲内存空间有限，memHarvester 无法将所有被驱逐的数据保留在收获内存中，最终导致主机内存交换。
    - 为了减少从 GPU 到主机的数据清除的性能开销，memHarvester 支持在主机内存中清除2MB 大的页面，而不是4KB 的基本页面

- 当 HUVM 和 memHarvester 承载多个工作负载时, 它可以提高内存利用率和整体服务器效率。
  - 然而，不利的一面是，memHarvester 可能会对运行在 GPU 上的应用程序造成性能干扰，从而产生空闲内存。
    - 因此，memHarvester 立即回收空闲内存，以最小的延迟将其返回到原始物理内存空间，只要在产生 GPU 上运行的应用程序需要额外内存，从而最小化性能干扰。

- 我们在 NVIDIA 的统一虚拟内存(UVM)驱动程序版本460.67的基础上实现了我们的原型系统，该版本是可公开访问的[29]。
  - MemHarvester 不需要对应用程序或机器学习平台进行任何修改
    - 就可以透明地检测备用内存的可用性，并动态地构建一个新的内存阶层
  - 我们使用 HUVM 量化 memHarvester 在 AWS p3.8 xlarge 实例上的不同整合场景的有效性。
    - 该服务器有四个通过 NVLink 连接的 NVIDIA V100(16GB)图形处理器。
  - 我们的实验结果表明，memHarvester 可以实现显著的吞吐量改善的大型图形分析工作负载。
    - 对于 DNN 训练和图形分析工作负载的不同整合场景，我们的实验结果显示，与多 GPU 环境下的先前方法相比，在同一服务器上运行的其他工作负载的干扰最小的情况下，性能提高了2.71倍。

## 动机和背景



## HUVM 结构化的内存分配

