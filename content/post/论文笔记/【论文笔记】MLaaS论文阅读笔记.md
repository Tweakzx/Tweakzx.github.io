---
title: "【论文笔记】MLaaS论文阅读笔记"
author: "Tweakzx"
description: 
date: 2022-12-18T21:20:09+08:00
image: https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212182120361.png
math: 
license: 
hidden: false
comments: true
draft: true
categories: 论文 
tags: 
    - 论文
    - AI系统
---

# MLaaS in the Wild 论文阅读笔记

## 摘要

随着机器学习 (ML) 技术的持续进步和最近大量数据集的可用性，科技公司正在部署大型 ML 即服务 (MLaaS) 云，通常使用异构 GPU 来提供大量 ML 应用程序。然而，在异构 GPU 集群中运行不同的 ML 工作负载会带来许多挑战。

- 在本文中，
  - 我们对从阿里巴巴拥有 6,000 多个 GPU 的生产 MLaaS 集群中收集的为期两个月的工作负载跟踪进行了表征研究。
  - 我们解释了集群调度所面临的挑战
    - GPU 利用率低
    - 排队延迟长
    - 需要高端 GPU 
    - 调度要求苛刻的任务难以调度
    - 异构机器之间的负载不平衡
    - 潜在的CPU瓶颈
  - 我们描述了我们当前的解决方案，并呼吁进一步调查仍未解决的挑战。
  - 我们已经发布了公共访问的跟踪，就工作负载和集群规模而言，这是最全面的。

## 引言

- 在本文中，我们分享了在大型 GPU 集群中运行ML 工作负载的经验。
  - 我们展示了从阿里巴巴 PAI（人工智能平台）中具有 6,742 个 GPU 的生产集群收集的为期两个月的工作负载跟踪的广泛特征。
  - 工作负载混合了由 1,300 多名用户提交的训练和推理作业，涵盖了多种 ML 算法
    - 包括卷积和递归神经网络（RNN 和 CNN）
    - 基于转换器的语言模型 
    - GNN-基于（图神经网络）的推荐模型
    - 强化学习
  - 这些作业运行在多个 ML 框架中，具有不同的调度要求，如 GPU 局部性和gang调度，并且需要大范围跨越数量级的可变资源。 
  - GPU 机器在硬件（例如 V100、P100、T4）和资源配置（例如 GPU、CPU 和内存大小）方面也是异构的

ML的工作负载和GPU机器的异构所带来的挑战:

- **由部分 GPU 使用导致的低利用率**。
  - 在我们的集群中，任务实例通常只能使用部分 GPU。现有的粗粒度 GPU 分配方案将整个 GPU 专用于一个任务实例，这将导致我们集群的利用率极低。
  - 我们通过 GPU 共享解决这个问题，调度程序将大量低 GPU 工作负载整合到少量机器上，这种整合不会造成严重的干扰

- **短期运行任务实例的长时间排队延迟。**
  - 短时间运行的任务实例容易因队头阻塞而导致长时间排队延迟。事实上，大约 9% 的短期实例花费了一半以上的完成时间等待安排。
  - 一个有效的解决方案是：**预测任务运行时间并将短任务优先于长任务**。现有方法需要专门的框架支持来跟踪和估计训练进度，这在生产中并不总是可行，因为用户可以在没有此类功能的情况下运行标准或定制的 ML 框架。
  - 然而，也有一线希望。在我们的集群中，大部分工作负载都是重复的。通过仔细的特征工程，我们可以预测大多数重复性任务的持续时间，使用具有预测任务持续时间的最短作业优先调度可将平均完成时间减少 63% 以上。

- **难以安排高 GPU 任务。**
  - 一小部分计算密集型 ML 任务，这些任务需要完整的 GPU，并且可以通过利用 NVLink 等高级硬件功能在高端设备上实现显着的加速——这些挑剔的要求使它们难以安排。
  - **预留**和**打包策略**来区分那些难以调度的高 GPU 任务与其他任务。它为少数具有挑剔调度要求的高 GPU 任务保留高端 GPU 机器（例如，带有 NVLinks 的 V100），同时使用 GPU 共享将其他工作负载打包在不太先进的机器上。预留和打包策略将高 GPU 任务的平均排队延迟减少了 68%，将所有任务的平均排队延迟减少了 45%。

- **负载不平衡。**
  - 我们观察到在异构机器中运行的不平衡负载。一般来说，低端 GPU 的机器比高端 GPU 的机器更拥挤，工作负载和机器之间也存在配置不匹配。

- **CPU 上的瓶颈。**
  - 虽然 ML 工作负载在 GPU 上执行训练和推理，但管道中涉及的许多数据处理（例如，数据获取、特征提取、采样）和模拟任务（例如，强化学习）在 CPU 上运行，这也可能成为瓶颈。我们发现在 CPU 使用率较高的机器上运行的工作负载更有可能变慢。即使是 GPU 要求高的工作负载也会受到 CPU 争用的损害。

潜在系统优化机会

- 改进 ML 框架
- 采用资源分解
- 将数据预处理与 GPU 训练解耦

## 背景

- **快速增长的数据和 GPU 需求。**
  - 对可扩展机器学习的支持在生产数据处理管道中变得越来越重要。海量数据迫使 ML 作业扩展到大量 GPU 机器。
  - 在我们的集群中，最大的单个 ML 作业请求在 1,000 多个 GPU 上运行，这对集群构成了重大的集群调度挑战。
- **阿里巴巴PAI**。
  - 为了适应 ML 工作负载快速增长的计算需求，阿里云提供了人工智能机器学习平台 (PAI)，这是一个一体化的 MLaaS 平台，使开发人员能够以高效、灵活和简化的方式使用 ML 技术。 
  - PAI 提供涵盖整个 ML 管道的各种服务，包括特征工程、模型训练、评估、推理和 autoML。

![image-20221219092057076](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212190920199.png)

> - 图 1 展示了 PAI 的架构概览，
>   - 其中用户提交在各种框架中开发的 ML 作业，例如 TensorFlow [14]、PyTorch [48]、Graph-Learn [75]、RLlib [38]。
>   - 提交作业后，用户提供应用程序代码并指定所需的计算资源，例如作为 GPU、CPU 和内存。
>   - 每个作业被翻译成不同角色的多个任务，例如用于训练作业的参数服务器（PS）和工作者，以及用于推理作业的评估器。
>   - 每个任务可能包含一个或多个实例，并且可以在多台机器上运行。 PAI 使用 Docker 容器来实例化任务，以简化异构硬件上的调度和执行。

- 跟踪分析。
  - 在云规模的共享 GPU 集群中运行各种 ML 工作负载会带来艰巨的挑战。跟踪分析对于了解这些挑战和提供有关系统优化的新见解至关重要。
  - 然而，现有的分析是在具有**有限大小**、工作负载多样性和机器异构性的 GPU 集群上进行的，因此不能完全代表最先进的技术水平
  - 并且不清楚使用什么**类型**的 GPU 来运行这些工作负载，这可能会对调度产生重大影响
  - Philly仅包括**训练**工作负载，而在共享 MLaaS 平台中同时运行**训练和推理**作业是很常见的

## Workload Characterizaion

### Trace Overview

- **跟踪信息**。跟踪记录了各种级别（例如，作业、任务和实例）的工作负载在 GPU、CPU、GPU 内存和主内存中的到达时间、完成时间、资源请求和使用情况。手动检查了一些工作负载，并将它们的应用程序名称（例如，点击率预测和强化学习）包含在跟踪中，以尽可能提供一些线索。跟踪中还提供了机器级信息，包括硬件规格和由定期查询 Linux 内核和 GPU 驱动程序。

- **作业、任务和实例**。在 PAI 中，用户提交作业。每个作业都有一个或多个任务承担不同的计算角色。每个任务在 Docker 容器中运行一个或多个实例。一个任务的所有实例都具有相同的资源需求，并且可能是成组调度的。我们在本小节中的描述主要集中在任务实例上。

- **严重倾斜的实例分布**。 图 2a 描绘了用户运行的任务实例的分布，这是严重倾斜的。更具体地说，大约 77% 的任务实例是由前 5% 的用户提交的，每个用户运行超过 17.5k 个实例，而后 50% 的用户每个运行的实例少于 180 个。
  - ![image-20221219103602761](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191036857.png)

- **gang调度的普遍性**。我们的分布式 ML 作业需要群组调度。如图2b所示，在所有任务实例中，大约85%有这样的需求，其中20%必须在100多个GPU上进行集群调度，有的甚至要求超过1000个。具有组合调度实例的任务加起来占 GPU 总需求的 79%。这些任务的普遍存在使得难以实现高利用率。

- **GPU 局部性**。除了集群调度之外，一个任务可能会请求在一台机器上的多个 GPU 上运行其所有实例，这一要求称为 GPU 局部性。尽管这种要求通常会导致长时间的调度延迟，但它可以在单个节点内使用高速 GPU 到 GPU 互连，从而显着加速分布式训练

- **显卡共享**。 PAI 支持 GPU 共享，允许多个任务实例以低成本分时共享一个 GPU。使用此功能，用户可以在 (0, 1) 中指定 GPU 请求并运行它的任务实例使用部分 GPU。

- **多种 GPU 类型可供选择**。 PAI 提供异构 GPU，并允许用户指定所需的 GPU 类型来运行他们的任务。可用的选择包括 NVIDIA Tesla T4、P100、V100、V100M32（具有 32 GB 内存的 V100 SXM2）和其他老一代的 GPU（表 1 中的其他），例如 Tesla K40m、K80 和 M60。

### Temporal Pattern

- 每日任务提交和资源请求。
  - 24h-144h也就是工作日提交的任务略高于周末。
  - 除了白天，午夜也是任务提交的高峰期，大多数在午夜提交的任务计算密集度较低，只有少数实例并请求少量资源
  - ![image-20221219111018334](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191110436.png)
- 实例运行时间范围很广。图 4a 显示了实例运行时间在很宽的范围内变化
- 非均匀排队延迟。
  - 从任务提交到任务实例开始的排队延迟在不同实例之间差异很大。
  - 与长时间运行的实例相比，短期运行的实例通常在排队上花费更多的时间。
  - 为了看到这一点，我们使用中值运行时间作为阈值并将实例分为长时间运行和短期运行的实例，其中长时间运行（短期运行）的任务实例的运行时间比中值更长（更短）。在图 4b 中，我们比较了这些任务实例的排队延迟与其完成时间（排队延迟加上运行时间）的关系。大约 9% 的短期运行实例花费超过一半的完成时间等待安排；当涉及到长时间运行的实例时，这个数字下降到 3%。
- 任务实例的排队延迟还取决于其 GPU 请求。图 4c 显示愿意共享 GPU 的实例（即 (0, 1) 中的 GPU 请求）可以被快速调度，第 90 个百分位 (P90) 排队延迟为 497 秒。相比之下，不接受 GPU 共享的实例需要等待更长的时间，对于请求一个 GPU（> 1 GPU）的实例，P90 延迟为 1,150（8,286）秒。
- 在请求高端 GPU 的实例中也会出现长时间的排队延迟。如图 4d 所示，对于在高级 V100 GPU（包括 V100M32）上运行的实例，中值和 P90 延迟分别为 113 秒和 13,709 秒。相比之下，对于在低端杂项 GPU 上运行的实例，中值和 P90 延迟分别仅为 11 秒和 360 秒。

![image-20221219111038344](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191110910.png)

### Spatial Pattern 

最后，我们通过分析 PAI 任务实例的资源请求和使用情况来展示它们的空间模式。 PAI 每 15 秒收集一次运行任务的系统指标，并为用户提供可视化工具 [2, 25] 以分析工作负载模式并找出他们的资源请求。

![image-20221219111139866](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191111932.png)

- 资源请求的重尾分布。
  - 图 5a、5b 和 5c（蓝色实线）分别描绘了所有实例请求的总 CPU、GPU 和内存的分布。这三个分布都是重尾分布，大约 20% 的实例请求大量资源，而其他 80% 请求中小型资源。更具体地说，P95 请求需要 12 个 vCPU 内核2、1 个 GPU 和 59 GiB 内存，是中值请求（6 个 vCPU 内核、0.5 个 GPU 和 29 GiB 内存）的两倍多。

- 资源使用不均匀：GPU 低但 CPU 高。
  - 大多数用户往往会请求比实际需要更多的资源，从而导致资源使用率较低（图 5a、5b 和 5c 中的虚线）。在我们的集群中，实例资源使用的中位数为 1.4 个 vCPU 内核、0.042 个 GPU 和 3.5 GiB 内存，远小于请求的中位数。
  - GPU 使用率低不是由低计算需求引起的，而是由于对 CPU 等其他资源的争用，使得 GPU 在大部分时间处于空闲状态（第 6.2 节）。图 5b 还显示，大约 18% 的实例几乎不使用 GPU：它们执行运行参数服务器、获取和预处理数据等计算，这些计算主要在 CPU 上进行，GPU 参与很少或没有参与。

在 PAI 中，任务的实例可以使用主机中的备用资源，从而可能过度使用比请求更多的资源。与 GPU 和内存相比，过度使用 CPU 更为普遍。为了解这一点，我们针对每个实例测量其资源使用情况与 CPU、GPU 和内存请求之间的差异——正（负）是过度使用（未充分使用）。我们分别根据机器的 CPU、GPU 和内存容量对结果进行归一化，并在图 5d 中描绘了分布。有 19% 的任务实例过度使用 CPU（X > 0 的蓝色实线）。相比之下，只有 3% (9%) 的实例使用的 GPU（内存）比他们请求的多。



## GPU 机器利用率

在研究了工作负载特征之后，我们转向 GPU 机器中的资源利用。

### 计算资源的利用率

我们开始分析计算资源的利用率，包括 CPU、GPU、主内存和 GPU 内存。我们的集群有 1,295 台 2-GPU 机器和 519 台 8-GPU 机器（表 1）。具有 8 个 GPU 的机器的 CPU 与 GPU 比率低于具有 2 个 GPU 的机器。鉴于它们的配置不同，我们分别对两类机器进行测量。每台机器都有监控系统每 15 秒测量一次的资源利用率时间序列数据。

在每个时间戳，我们收集所有 8-GPU 机器的利用率并计算尾部 (P90) 和中位数 (P50)。我们一起获得了在不同时间戳处获取的一系列 P90 和 P50 利用率。我们在图 6 中描绘了它们的分布（左侧的两个子图）。我们在 2-GPU 机器上执行相同的测量，并在右边的两个子图中描述结果。与内存（主内存和 GPU）相比，GPU 和 CPU 的利用率更高。在 8-GPU 机器中（图 6 左上角），GPU（红色点划线）和 CPU（蓝色实线）的平均 P90 利用率，即所有时间戳的 P90 值的算术平均值，达到 82%和 77%。在 2-GPU 机器中（图 6 右上角），P90 GPU 利用率仍然很高（平均 77%），而 P90 CPU 利用率由于较大的 CPU-GPU 比率（32或每个 GPU 48 个 CPU）。在这两种类型的机器中，主内存和 GPU 内存的 P90 利用率几乎始终保持在 60% 以下，这表明我们的任务占用的内存较少。

![image-20221219111209541](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191112623.png)

与其他资源相比，我们测量了 GPU 上更大的利用率变化。如图 6 所示，P90 GPU 利用率的分布范围很广，从不到 40% 到 100% 由机器 GPU 的流式多处理器提供的计算能力； GPU 上的尾部利用率和中位数利用率之间的差异也大于其他资源（将顶部子图与底部子图进行比较）。较大的变化部分是由于在我们的 ML 工作负载 [65、66] 中发现的突发 GPU 使用模式。这也是由于我们的调度程序设计优先于负载均衡打包（第 6.3 节）。

### 网络与IO的低利用率

除了计算资源，网络和 I/O 也经常用于分布式 ML。要了解它们的影响，我们测量具有不同带宽保证（P100 和 Misc ≥ 10 Gbps，T4 ≥ 15 Gbps，V100 ≥ 32 Gbps）的机器中的网络输入速率 3，并在图 7a 中描绘了它们的分布。 P95网络输入速率分别仅达到P100（或Misc）、T4、V100机器提供的保证带宽的54%、48%和34%。

![image-20221219111237775](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191112875.png)

在 I/O 方面，我们收集机器级别的 CPU 使用数据，包括 I/O 等待时间（iowait）和在 usr 和内核模式下的执行时间。图 7b 显示了它们的分布。花在 iowait 上的 CPU 时间比在 usr 和内核模式下少三个数量级，这意味着 CPU 主要忙于处理数据而不是等待 I/O 完成。

## 集群管理的机遇

在 PAI 中，我们的集群管理目标有两个：（1）在 GPU 机器中实现高利用率，以及（2）尽可能快地完成尽可能多的任务。在本节中，我们描述了实现这两个目标的机遇和努力。

### GPU 共享

与 CPU 不同，GPU 本身不支持共享，并且在许多生产集群中被分配为不可分割的资源，其中单个任务实例专门在 GPU 上运行。虽然这种分配提供了强大的性能隔离，但它会导致 GPU 利用率不足，这在我们的集群中尤为突出，因为大多数实例只能利用分配的 GPU 的一小部分。

为了避免这个问题，PAI 集群调度器支持 GPU 共享，允许多个任务实例以空间和时间复用的方式在同一个 GPU 上运行。使用此功能，任务实例可以请求一小部分 GPU（< 1 GPU），并保证在调度（空间多路复用）时分配指定部分的 GPU 内存。需要时，实例还可以在执行期间使用未分配的 GPU 内存。然而，实例无法保证计算单元（即 SM）的分配，这些计算单元在位于同一位置的实例之间动态共享（时分复用）。 

GPU共享的好处。 GPU 共享可以显着节省资源配置。为了解这一点，我们模拟了没有 GPU 共享的场景，在该场景中我们重播跟踪并计算每小时分配的 GPU 数量。图 8 将模拟结果与实际系统中测得的数字进行了比较，以一天中的小时为单位。平均而言，共享只需要 50% 的 GPU。在上午 10 点左右的高峰时段，可节省高达 73%。

![image-20221219111316889](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191113987.png)

GPU 共享会导致争用吗？随着利用率的提高，在共享 GPU 上运行的实例开始争用流处理器 (SM)，从而造成干扰。为了量化争用发生的频率，我们收集了两个月内所有 GPU 的利用率数据，并在图 9a 中描绘了它们的分布。很少测量重利用率 (≥ 95%)，在跟踪中仅占 7% 的情况。我们进一步检查那些高利用率的 GPU，在这些 GPU 中运行的实例很有可能相互竞争。图 9b 显示了 5 天内高利用率 GPU 的数量，其中只有少数（平均 4.5%）运行多个实例（顶部堆叠条）。由于大多数高利用率 GPU 运行单个实例，因此不会发生争用。因此，我们相信 GPU 共享不会在我们的集群中引起严重争用

![image-20221219111332440](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191113507.png)

### 递归工作的可预测持续时间

了解 ML 任务实例的持续时间（也称为运行时）是做出更好的调度决策的关键。现有的 ML 工作负载调度程序根据训练进度（例如，迭代次数、损失曲线和目标准确性）和任务速度预测任务实例持续时间 [29、41、46、49]。获取这些信息需要特定的框架支持（例如，TensorFlow和PyTorch），这在我们的集群中并不总是可能的，因为用户运行各种标准或定制版本的框架，并且他们提交的任务可能不会执行迭代训练（例如，推理） .事实上，我们的集群调度器 [26, 71] 是为容器工作负载而设计的，并且与任务语义无关。

重复性任务的普遍性。尽管调度程序对任务进度不可知，但我们发现大多数任务都是重复发生的，并且可以从过去的执行中很好地预测它们的实例运行时间。然而，在我们的系统中，任务重复性不能简单地从任务 ID 或名称中识别，任务 ID 或名称是为每次提交唯一生成的。相反，我们转向跨多个提交的任务一致指定的元信息，例如入口脚本、命令行参数、数据源和接收器。散列元信息生成一个唯一的组标签，我们用它来识别任务的重复出现。按照这种方法，我们在图 10 中描绘了任务重复的分布：大约 65% 的任务在跟踪中重复运行至少 5 次。

![image-20221219111358941](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191113001.png)

除了周期性训练之外，许多重复性任务执行批量推理。这些任务聚合来自传入请求的数据，然后一次性对一组数据执行批量推理。用户可以配置任务启动间隔，从几分钟到几天不等。作为说明性示例，图 11 显示了跟踪中识别的三个重复任务，这些任务使用预训练的 BERT [23] 模型执行批量推理。所有三个任务都定期运行，具有可以准确预测的稳定平均实例运行时间。

重复任务的实例持续时间预测。一个循环任务可以由不同的用户以不同的资源请求提交，它的实例可能有不同的运行时间。因此，我们根据三个特征预测过去运行的持续时间，即任务的用户名（User）、资源请求（Resource，包括 GPU 和其他资源）和组标签（Group）。将这些特征作为输入，我们预测任务的平均实例持续时间使用带有树回归器的 CART（分类和回归树 [17]）算法。回归器对每棵树最多进行 10 次拆分，并使用平均绝对误差 (MAE) 作为拆分标准。我们选择 MAE 而不是标准均方误差 (MSE)，因为前者比后者对重尾分布中的极端异常值更稳健。

为了评估我们预测的准确性，我们考虑了在跟踪中至少重复出现 5 次的任务。我们使用这些任务中的 80% 来训练预测器，剩下的 20% 用于测试。图 12 比较了使用不同特征输入训练的预测器的准确性，包括组、用户、资源和 GPU（请求的 GPU 类型和数量）。我们使用百分比预测误差 [35] 作为准确度指标，定义为 (true − pred)/true × 100%。我们的评估表明，Group 是最重要的特征，它极大地提高了预测精度。进一步用用户和资源（或 GPU）补充它会导致 78% 实例的预测误差小于 25%。根据先前的研究[16]，具有这种精度的持续时间预测足以做出高质量的调度决策。

![image-20221219111517266](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191115324.png)

对调度的好处。我们提出了一个简单的模拟研究来评估任务实例持续时间的预测如何帮助改进调度。我们开发了一个离散时间模拟器，并用它来重放轨迹。我们从跟踪中对任务进行采样，并将它们的资源请求、到达时间、实际和预测的运行时间提供给模拟器。我们在模拟中假设同质 GPU，并在将任务实例调度到 GPU 时尊重实际持续时间。模拟器和实验脚本都与跟踪一起发布

我们在模拟中配置了两种调度策略，先进先出（FIFO）和最短作业优先（SJF）。图 13 显示了使用 FIFO 和四个 SJF 调度程序的不同大小的 GPU 集群中的平均任务完成时间，其中 SJF-Oracle 根据实际测量的任务实例持续时间（基本事实）做出调度决策，其他使用使用不同输入特征训练的预测器。与 FIFO 相比，四个 SJF 调度器将平均任务完成时间减少了 63-77%，具体取决于它们使用的预测器。特别是，使用 Group 特征训练的预测器会产生更好的性能；包含的特征越多，预测越准确，调度性能越接近最优（SJF-Oracle）。

![image-20221219111550864](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191115955.png)



## 调度的挑战

与之前的模拟相比，在生产集群中调度大异构的 ML 任务要复杂得多。为了理解这种异质性带来的挑战，在本节中，我们将介绍具有高 GPU 请求和低 GPU 请求的两种代表性 ML 任务类型的案例研究。我们描述了我们在生产中部署的调度策略，根据不同的请求和使用模式区分两种类型的任务。然而，许多挑战仍然存在，我们将对此进行详细讨论。

### 高GPU任务的案例研究

在我们的集群中，一小部分任务运行具有高 GPU 请求的计算密集型实例（第 3.3 节）。这些任务训练最先进的模型或使用训练有素的模型为面向用户的关键业务应用程序执行推理。他们需要具有高内存或高级硬件功能（例如 NVLink）的强大 GPU 设备。

具有高级语言模型的 NLP。在我们的集群中运行的大约 6.4% 的任务使用高级模型执行自然语言处理 (NLP)，例如 BERT [23]、ALBERT [37] 和 XLNet [67]。其中，73% 的输入量较大，必须在 16 GiB 或更高内存的 GPU 上运行（即 T4、P100、V100/V100M32）。图 14a 显示了 GPU 请求的分布和 NLP 实例的使用情况，其中 40% 请求超过 1 个 GPU 并使用超过 0.4 个 GPU 的计算能力。比较图 5b 和图 14a，我们观察到 NLP 任务的 GPU 请求和使用率比一般工作负载高得多。

![image-20221219111630429](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191116518.png)

具有大量输出的图像分类。在我们的集群中，一些分布式训练任务要求在具有高速 GPU 到 GPU 互连（例如 NVLink）的一台机器上运行它们的工作实例，以大幅提高性能，这一要求称为 GPU 局部性。一个典型的例子是训练一个分类模型，将商品的图像分类成大量的标准产品单元（SPU）。该模型可以是修改后的 ResNet [33]，最后一个输出层替换为具有 100,000 个 SPU 输出的 softmax 层。如此大的全连接层的存在要求在工作实例之间交换大量梯度更新，从而使通信成为瓶颈。对于这些任务，满足 GPU 局部性至关重要。图 14b 比较了具有和不具有 NVLink（即通过 PCIe）的 8-GPU 机器中具有大量输出的三个分类模型的训练时期的持续时间。所有三个模型都通过 NVLink 实现了显着加速：最大模型 ResNet-100k 加速了 10.5×

### 低GPU任务的案例研究

我们集群中运行的大多数任务的 GPU 请求和使用率都很低（第 3.3 节）。为了理解这个有点出乎意料的结果，我们研究了三个流行的任务。通过分析他们的执行情况，我们发现他们在 CPU 上花费了大量时间进行数据处理（例如，数据获取、特征提取、采样）和模拟（例如，强化学习），而没有充分利用 GPU。

CTR 预测模型训练和推理。在跟踪的所有任务中，超过 6.7% 用于广告点击率 (CTR) 预测。这些任务使用各种 CTR 模型 [30、60、73、74]，其中约 25% 的实例执行训练，其他 75% 的实例执行推理。图 15 显示了这些实例的 CPU 和 GPU 使用情况的分布。与训练相比，推理实例具有更高的 CPU 利用率，因为它们处理大量不断到达的数据。这两个实例的 GPU 利用率都很低：超过 75% 的实例使用不到 0.1 个 GPU。

![image-20221219111650859](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191116949.png)

接下来，我们分别使用 DeepFM、DCN 和 DNN 模型分析三个推理实例的执行情况。图 16a 显示了 I/O、GPU 和 CPU 操作的运行时分解。这三个实例在 CPU 上花费了大约 80% 的运行时间来获取和处理下一个输入批次（TensorFlow [20、40] 中的 IteratorGetNext）； GPU 和 I/O 操作（例如，MatMul、Sum、Cast、MEMCPYHtoD）分别仅占执行时间的 10%。

这些实例的高 CPU 使用率使它们容易受到共处工作负载的干扰，尤其是在 CPU 使用率高的机器中。为了解这一点，我们在具有 8 个 vCPU 内核的容器中运行 DeepFM 模型的训练实例。与一个实例一起，我们使用主机的备用内核运行一些人工负载来产生 CPU 压力。我们配置不同的负载来控制压力水平。图 16b 显示了 48 核机器在不同压力下的实例训练速度，其中 0 到 40 个核心处于空闲状态（最高到没有压力）。尽管并置负载运行在实例未占用的不同 vCPU 内核上，但由于缓存、电源和内存带宽等其他共享资源的争用，仍导致训练速度降低高达 28%

![image-20221219111708609](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191117659.png)

GNN 训练。图神经网络 (GNN) 训练作为另一种流行的计算出现，占我们集群中 2% 的实例，包括 GraphSage [31]、Bipartite GraphSage [75]、GAT [57] 等。图 17a 显示了 CPU 和GNN 训练实例的 GPU 使用情况，其中 CPU 的使用率高于 GPU。在生产 GNN 模型中，图必须经历一个序列预处理，例如 EdgeIteration、NeighborSampling 和 NegativeSampling [75]，然后再变成深度神经网络的嵌入（一种计算可消化的格式，通常是向量）。这种图形预处理目前在 CPU 上执行时具有成本效益。如图 17b 所示，它占不同模型中每次训练迭代的 30-90% 的持续时间。

![image-20221219111803031](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191118103.png)

强化学习。我们的集群还运行许多强化学习 (RL) 任务。 RL 算法通过在 CPU 上并行模拟迭代生成一批数据，并在 GPU 上使用生成的数据进行训练以改进学习策略。图 18a 显示 72% 的 RL 任务至少有 10 个集群调度实例，其中最大的一个运行超过 1,000 个实例。大多数 RL 实例用于运行模拟，占用大量 CPU 和网络带宽，但只占用一小部分 GPU，如图 18b 所示。事实上，在最大的 RL 任务中，每个实例只需要 0.05 个 GPU。

![image-20221219111819344](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202212191118414.png)

### 部署调度policy

与低 GPU 任务相比，高 GPU 任务具有挑剔的调度要求，通常由关键业务应用程序运行。因此，他们与其他任务区分开来，并被安排为一等公民。

保留和包装。在我们的集群中，调度程序采用预留和打包策略。也就是说，它有意为高 GPU 任务保留高端 GPU（例如，带有 NVLinks 的 V100/V100M32），同时将其他工作负载打包到 GPU 不太先进的机器（例如，T4 和 Misc）。具体来说，对于每个任务，调度程序使用一个性能模型来描述其计算效率，该模型考虑了许多任务特征，例如并行度、使用的 ML 模型、嵌入的大小 [59,64,70]，以及历史其他类似任务的配置文件。高计算效率大于某个阈值的任务被识别为高 GPU。

对于每个任务，调度程序生成一个有序的分配计划序列；每个计划指定预期的 GPU 设备并与尝试超时值相关联。调度程序尝试按照有序计划进行分配：它等待中指定的预期 GPU 的可用性当前计划直到超时，然后继续下一个计划进行另一次尝试。对于高 GPU 任务，在有序计划中先尝试高端 GPU 的分配；对于其他任务，顺序相反。我们的 GPU 调度器是在 Fuxi [26, 71] 之上实现的，这是一个基于局部树的调度系统。

负载均衡。考虑到并置任务实例之间潜在的资源争用和干扰（第 6.2 节），在具有相似规格的机器之间保持负载平衡也很重要。因此，在 reserving-packing 下，调度程序还优先将实例调度到分配率低的机器，以分配的 CPU、内存和 GPU 的加权和衡量，这些分配率由机器的容量标准化。

好处。我们的调度程序优先于负载平衡进行预留和打包。为了证明这种设计的合理性，我们使用第 5.2 节中描述的模拟器评估了两种调度策略：1 © 使用渐进式填充（始终将任务的实例调度到最少使用的节点）简单地负载平衡机器，以及 2 © 仅执行保留和打包而不执行考虑负载平衡 (R&P)。我们从跟踪中抽取了 100,000 个具有超过 500,000 个帮派调度实例的任务，并将它们提供给模拟器。图 19a 显示了两种策略下所有实例和任务的排队延迟的 CDF。请注意，任务的排队延迟也是其集群调度实例的排队延迟。超过 90% 的实例和任务在这两个策略下立即启动。与负载均衡相比，reserving-andpacking 将平均任务排队减少了 45%，这主要归因于将尾延迟显着减少了 10,000 多秒。图 19b 进一步比较了在两种策略下请求 V100 GPU 的关键业务任务和实例的排队延迟：reserving-and-packing 将平均任务排队延迟减少了 68%。模拟结果证明了我们的设计优先于负载平衡的预留和打包。

### 开放挑战

- **机器规格与实例请求不匹配**。
  - 我们观察到机器规格和实例请求之间存在不匹配。表 2 比较了具有 8 个和 2 个 GPU 的机器及其运行实例中每个 GPU 的配置和请求 vCPU 内核的平均数量。在 8 GPU 机器中，为每个 GPU 预配了 12 个 vCPU 内核。然而，在这些机器上运行的实例平均每个 GPU 需要 22.8 个 vCPU 内核。另一方面，双 GPU 机器中的 CPU 被过度配置，其中 CPU 与 GPU 的比率是实例请求的两倍以上
  - 为了了解不匹配如何影响机器利用率，我们随机抽取了一些具有不同规格的节点，并在图 20 所示的热图中描述了 CPU 和 GPU 的请求和使用情况，其中每一行对应一台机器，所有值都是归一化到机器的容量。与 8-GPU 节点相比，尽管 GPU 被大量占用，但 2-GPU 机器的 CPU 基本上未得到充分利用。平均而言，P100 (T4) 机器分配了 31% (20%) 的 CPU，CPU 利用率仅为 19% (10%)（图 20c 和 20d）。
  - 我们强调机器规格和实例请求之间的不匹配不是根本原因，因为集群范围内的 CPU 到 GPU 规格仍然接近整体实例请求（23.2 与 21.4，如表 2 所示）。因此，我们认为可以通过改进调度（例如，将 8-GPU 机器中的一些高 CPU 实例重新调度到 2-GPU 节点）来避免或至少减轻不匹配。

- **过度拥挤的弱 GPU 机器**。
  - 与其他机器相比，那些配备不太先进的 GPU 的机器过于拥挤。如图 20a 所示，该问题在 8-GPU 节点（其他 GPU）中变得更加突出。这些机器平均分配了 77% 的 CPU 和 74% 的 GPU。 CPU 的利用率高于 GPU：CPU 的平均利用率为 43%，而 GPU 的平均利用率为 18%。这个结果部分是由于我们的调度算法将弱 GPU 机器优先用于低 GPU 任务（第 6.3 节），这些任务占我们集群中的大量实例。

- **高端机器负载不平衡**。
  - 与其他节点相比，配备高级 V100 GPU 的高端机器不那么拥挤（图 20b），CPU 和 GPU 的平均分配比例分别为 35% 和 49%。这些机器通常被保留用于少数重要的高 GPU 任务，因此利用率低。我们还观察到 V100 机器之间的负载不平衡。靠近底部的机器比其他机器更拥挤。这表明当前的负载均衡算法仍有很大的改进空间

- **CPU 可能是瓶颈**。
  - 如第 6.2 节所示，大量 ML 任务比 GPU 更广泛地使用 CPU。这些任务更有可能在具有高 CPU 争用的机器上变慢。为了解这一点，我们研究了跟踪中机器利用率与实例减速之间的相关性，并在图 21 中描述了结果。我们的分析侧重于重复性任务（第 5.2 节）。在每个任务循环中，我们将实例分为三组：1）持续时间最短 15% 的加速执行实例，2）持续时间中间 70% 的正常执行实例，以及 3）持续时间最长的延迟执行实例 15 %。图 21a 比较了运行加速、正常和延迟实例的机器的 CPU 利用率。通常，运行延迟实例的机器测得的 CPU 利用率高于运行加速和正常实例的机器。但是，在 GPU 上找不到这种相关性。如图 21b 所示，GPU 利用率的分布在运行加速、正常和延迟实例的机器之间没有显着差异。

接下来，我们将放大具有高 CPU 使用率的流行 CTR 预测任务（第 6.2 节）。图 22 分别显示了运行加速和延迟实例的机器中 CPU/GPU 利用率的 CDF。在 CPU 使用率超过 24% 的机器中运行 50% 的延迟实例但只有 10%加速实例（图 22a），证明 CPU 争用与实例减速之间存在很强的相关性。另一方面，GPU 争用对实例减速没有明显贡献

总而言之，GPU 集群中的任务实例调度还应考虑到 CPU 争用引起的潜在干扰。这本质上需要一个多资源调度器，在做出调度决策时共同考虑 CPU、GPU、内存、I/O 和网络。

## 讨论

- 支持弹性调度。
  - 异构集群中 GPU 调度程序面临的一项基本挑战是分布式训练的集群调度要求。因此开发了一些框架 [6、13] 来支持弹性调度，允许训练作业动态调整运行中的工人数量。
  - 与 gang scheduling 作业相比，elastic-scheduling 作业更容易处理：它们可以从少量资源开始，然后在集群变得不那么拥挤时扩展到更多 GPU。
  - 然而，弹性调度为最终模型精度引入了非确定性

- 机器配置和资源分解。 
  - GPU 调度程序还应考虑机器配置：在我们之前的分析中，虽然 8 GPU 机器提供了丰富的 GPU 处理能力，但 2 GPU 机器更适合 CPU 处理繁重的任务。
  - 为了简化问题，许多系统工作建议将单片机分解为许多分布式、分解的硬件组件，以提高硬件弹性 [53]，尽管通信开销不可忽略。 TensorFlow 最近在这个方向上做了框架级别的尝试。
  - 它发布了一项实验性数据服务 [5]，将数据预处理与 GPU 训练分离，从而解决 CPU 瓶颈。
  - 但是，它需要付出不小的努力来更改用户的源代码。