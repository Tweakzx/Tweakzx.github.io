---
title: "【论文笔记】Gandiva Fair论文阅读笔记"
author: "Tweakzx"
description: 
date: 2023-01-09T10:27:34+08:00
image: https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202301091029115.png
math: 
license: 
hidden: false
comments: true
draft: true
categories: 论文 
tags: 
    - GPU
    - kubernetes
    - 调度
---

# Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning

## Abstract

- Gandiva-fair
  - 效率与公平的平衡
    - 效率：Gandiva-fair 提供用户之间的性能隔离，使多个用户可以共享一个集群，从而最大限度地提高集群效率
    - 公平：在活跃用户之间公平分配集群范围 GPU 时间
  - 集群异质性
    - 背景：新一代用户面临更高的需求，老一代 GPU 的利用率很低，从而降低了集群效率
    - 解决方案： Gandiva-fair 描述了来自较新 GPU 的各种作业的可变边际效用，并通过一种新颖的资源交易机制透明地激励用户使用较旧的 GPU
    - 效果：该机制不影响任何用户的公平性保证的情况下最大限度地提高集群效率

## Introdution

- 为什么跨用户分享GPU

  - 跨多个用户对 GPU 进行静态分区可提供可预测性和性能隔离，但会导致集群利用率低下。
  - 跨所有用户的单个共享集群在整体效率方面很有吸引力，但为了实用，这样的集群必须保证每个用户至少获得与静态分区集群相同的性能。

- 使共享复杂化的

  - 如果用户无法使用其配额，则必须在其他活动用户之间共享未使用的容量，从而最大限度地提高集群效率。
  - 硬件异构性，这是 GPU 集群中一个特别突出的问题。GPU迭代导致集群中存在新旧不同的GPU， 用户倾向于使用新的GPU， 这导致旧的GPU利用不充分。对于单个共享集群，调度器需要智能地跨用户分配不同代的 GPU，以在确保公平的同时最大限度地提高效率。

- 其他工作

  - [Tiresias， Optimus，Gandiva]提出了几种 DLT 作业调度程序，但即使在同构集群中，它们也不支持用户公平性。事实上，他们都没有用户的概念，也没有在作业级别上运行，要么专注于提高集群效率 [Gandiva] 或作业完成时间 [Tiresias、Optimus]。
  - 即使是当今公司中使用的调度程序 [Project Philly] 也只是简单地将集群静态划分为虚拟集群以将一组与另一组隔离，从而导致效率低下。

- Gandiva-fair

  - 是一种调度程序，可确保异构 GPU 共享集群中用户在集群范围内公平共享 GPU 分钟数，同时通过在其他活动用户之间公平分配未使用的配额来确保集群效率。

    - Gandiva-fair 使用ticket的概念为特定用户提供集群中资源的公平共享。
    - Gandiva-fair 假设 GPU 是 DLT 作业的唯一主要资源 
    - 如果网络/CPU 也可能成为瓶颈，则可以扩展 Gandiva-fair 以纳入公平性指标

  - 大数据调度器也支持用户公平，但不适合深度学习设置

    - DLT工作需要以All or Nothing的方式调度， 即gang scheduling
    - 大数据调度程序在超额订阅期间依赖于作业抢占。DLT 作业的抢占可能会导致作业状态损失数小时/数天。Gandiva-fair 依赖于工作迁移作为在不丧失工作状态的情况下强制执行公平的关键原语。

  - Gandiva-fair 使用三个关键技术实现公平和效率。

    - 拆分式设计(splite, gang-aware, stride)：在短时间尺度内实施公平性

      - 中央调度器负责大型作业的帮派感知调度，这些作业需要大量 GPU 并跨越多个服务器，而本地每服务器帮派感知调度器调度小作业，也就是 GPU 要求适合该服务器的作业。
      - 这种拆分设计允许中央调度程序协调多个服务器，这对于以组感知方式调度大型作业是必要的，而分布式本地调度程序管理小型作业允许可扩展性。

    - 负载均衡器：保证作业到达和离开时的公平性

      - 负载均衡器使用作业迁移作为关键机制来分配作业，根据它们的ticket权重，在整个集群中均匀分布。

    - 跨用户自动资源交易的新技术：解决了硬件异质性问题

      - 从公平的角度来看，Gandiva-fair 将用户票映射到集群中每一代 GPU 的比例份额。

      - 在保留这种保证的同时，Gandiva-fair 使用**自动交易**来最大限度地提高集群效率。

        -  Insight：是利用更快 GPU 的不同边际效用来处理不同的 DLT 工作。

        - > 例如，如果与 K80 GPU 相比，用户 A 的作业在 V100 上实现了 1.25 倍的加速，而用户 B 的作业在 V100 上实现了 5 倍的加速，则用户 B 的边际效用高出 4 倍，因此会更好用他的 4 个 K80 GPU 换取 1 个 V100；

        - 双方用户都从这种交易中受益，集群效率也得到提高。正如我们在第 2 节中展示的那样，加速因子在作业之间的广泛分布中有所不同，从而实现了这种最大化集群效率的双赢交易。

        - 为了防止用户玩弄这些激励措施，它利用了**第二价格拍卖**的incentive-compatible benefits 

- 我们在论文中做出以下贡献：
  - 我们提出了第一个用于深度学习训练作业的集群调度程序，它保证用户级公平共享集群范围的 GPU 时间，同时最大限度地提高集群效率。
  - 我们证明迁移可以用作一流的原语来实现集群范围内的公平性，而无需诉诸抢占。
  - 我们提出了一种自动交易策略来处理 GPU 异质性，通过利用更快 GPU 的可变边际效用来处理各种工作，从而在确保公平的同时提高效率。
  - 在200 GPU 异构集群上使用原型实现，我们表明Gandiva-fair 能够在大规模多用户工作负载下实现其公平和效率的目标。

## Motivation

### Fairness

为了说明公平调度程序可用的各种选项并激发 Gandiva-fair 的设计选择，请考虑图 1 中所示的简单示例。在此示例中，集群由两台服务器组成，每台服务器有四个 GPU。所有 GPU 都属于同一型号（同类）。让两个用户 A 和 B 的两个 2-GPU 作业在集群的八个 GPU 上运行。

![image-20230109120401373](C:/Users/LZX/AppData/Roaming/Typora/typora-user-images/image-20230109120401373.png)

- 用户之间公平性
  - Gandiva-fair 的目标之一是用户间的公平。为简单起见，让我们假设所有用户都拥有相同数量的令牌。然后，如果每个活跃用户收到的资源分配至少是总集群 GPU 资源除以活跃用户数，则调度程序是用户间公平的。如果活跃用户没有足够数量的作业来利用他们的公平份额，那么调度程序应该分配足够的资源来满足该用户，然后递归地将公平性定义应用于剩余资源和活跃用户。在此示例中，集群有 8 个 GPU，用户 A 和用户 B 各分配了四个 GPU。因此，分配是用户间公平的。
  - 现在让我们假设用户 C 有一个新的 2-GPU 作业到达。用户 C 在这个例子中的公平份额是 8 个 GPU/3 个活跃用户 = 2.66 个 GPU（即 2 个 GPU + 一个 GPU 上的 2/3 时间）但是因为用户只有一个 2-GPU 作业，用户 C 的公平份额是 2 个 GPU。在分配了用户 C 的 2-GPU 公平份额后，为了用户间的公平，剩余的 6 个 GPU 需要在用户 A 和用户 B 之间平分，导致每个用户总共有 3 个 GPU。
  - 现在考虑当前调度程序在这种情况下提供的各种公平选项。像 Optimus [33] 或 Tiresias [19] 这样的调度器没有将用户间公平作为目标，而是基于最小化作业完成时间来优化他们的调度决策。因此，他们要么允许用户 C 的作业留在队列中，要么将现有作业之一移回队列并安排用户 C 的作业取而代之。除了 Optimus 或 Tiresias 之外，任何不分时 GPU 的调度程序（例如 [9, 25]）也将只剩下这两个选项。这些选项分别如图 1(a) 和 (b) 所示。在任何一种情况下，请注意调度程序不是用户间公平的。在前一种情况下，用户 C 没有获得 2 个 GPU 的公平份额，而在后一种情况下，一个用户（如图所示的用户 A）分配了四个 GPU，而另一个用户分配了两个 GPU。因此，这些选项指出了 GPU 资源分时支持用户间公平的必要性。
  - Gandiva [41] 是一个调度器，它使用分时调度比 GPU 调度更多的作业。然而，Gandiva 针对效率而非公平进行了优化。在此示例中，Gandiva 将简单地将用户 C 的作业添加到其中一个节点，并在该节点上对所有作业进行时间分割，如图 1(c) 所示。然而，这种方法也不提供用户间的公平性，因为当用户 C 的公平份额是 2 个 GPU 时，它只会收到 4/3 个 GPU。
  - 正如我们将在第 3 节中展示的那样，Gandiva-fair 分配 GPU，如图 1(d) 所示。在一台服务器上，用户 C 为其工作分配了 2 个 GPU，而用户 A 和用户 B 在 2 个 GPU 上平均分时。这种分配是用户间公平的，因为用户 C 分配了 2 个 GPU，而用户 A 和用户 B 总共分配了三个 GPU。然而，这种分配在用户内部是不公平的，即给定用户的所有作业都不会获得平等的资源。例如，用户 A 的一个工作通过分时获得相当于一个 GPU，而用户 A 的另一个工作获得两个 GPU。
  - 如果有必要实现用户内公平，那么图 1(e) 显示了一种可能的实现方式。为了支持用户内公平，用户 A 和用户 B 的作业在两个服务器之间定期迁移，以便用户 A 和 B 的每个作业平均获得 1.5 个 GPU 资源。虽然这样的解决方案在用户内部是公平的，但它会导致大型集群中大量的作业迁移。此外，尚不清楚严格的用户内公平性是否是深度学习的必要要求，因为属于多作业的作业是在不同时间发布的。因此，用户内公平不是 Gandiva-fair 的目标。相反，Gandiva-fair 通过选择仅保证用户间的公平性来平衡效率和公平性，如图 1(d) 所示。
  - 虽然这个简单的例子激发了各种设计选择，但当我们考虑以下三个现实需求时，提供用户间的公平性变得具有挑战性。
- 服务器之间的gang scheduling
  - 上述场景假设所有用户作业都需要相同数量的 GPU。实际上，用户作业可能需要 1 个 GPU 到 128 个 GPU 或更多。由于多种原因，通过混合工作规模和帮派调度来满足用户间的公平性具有挑战性。
  - ![image-20230109133723242](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202301091337339.png)
  - 考虑图 2 中显示的具有混合作业大小的两台服务器。我们如何确保图中显示的所有作业在不牺牲效率的情况下获得公平份额。事实上，Gandiva [41] 通过基于作业大小的分区避免了这种情况，这样每个服务器只有给定大小的作业。由于不同大小的作业差异很大[25]，这样的分区会导致负载不平衡，造成不公平和低效率。因此，为了用户间的公平，在单个服务器上分时混合多种作业类型是必要的。
  - 在上图中，首先考虑调度仅跨越服务器 1 的小作业。在给定的时间片内，假设我们需要从用户间公平的角度调度用户 A 和用户 C 的作业。但是，由于他们的需求总和为 5 个 GPU，因此此分配不可行。满足此公平性约束的一种选择是在一个时间量程中安排用户 A 的 1-GPU 作业，在下一个时间量程中安排用户 C 的 4-GPU 作业，但这是低效的，因为在第一个时间量程中，四个 GPU 中的三个将保持闲置。我们可以将这三个 GPU 分配给用户 A 和 B 的 1-GPU 和 2-GPU 作业，但是，正如我们将在第 3 节中看到的那样，没有仔细考虑公平性的这种回填 [13] 可能会导致不公平。那么我们如何在确保用户间公平的同时保持所有 GPU 的利用率？我们在第 3 节中展示了 Gandiva-fair 如何平衡效率和公平性以使用团伙感知调度算法来处理此类场景
  - 第二，我们如何确保跨多个服务器的用户 D 的 8-GPU 作业得到调度？出于可伸缩性的原因，我们希望每台服务器都做出独立的调度决策，但我们还需要确保它们做出协调的选择，以便用户 D 的 8-GPU 作业在服务器 1 和 2 之间同时调度。在第 3 节中，我们展示了 Gandiva-fair 的分布式拆分调度程序来应对这一挑战。
  - 第三，到目前为止，我们假设 GPU 是同构的。然而，随着时间的推移，一个集群会产生各种 GPU 模型，因为每年都会发布新的 GPU 模型，而旧硬件的折旧通常需要三年或更长时间。考虑到异构性，用户更喜欢更新的 GPU，因为它们能够更快地运行模型。那么调度程序如何激励用户使用旧的 GPU？我们接下来讨论这个问题。

### GPU 异构

表 1 列出了旧 K80 GPU 模型的不同深度学习训练作业的每个小批量的时间（以毫秒为单位）以及 P40、P100 和 V100 等较新 GPU 的加速比。这些工作涵盖各种深度学习训练任务，包括变分自动编码器 (VAE)、图像超分辨率、深度卷积生成对抗网络 (DCGAN)、门控循环单元 (GRU)、长短期记忆 (LSTM) 和众所周知的卷积ResNet 和 ResNext 等网络模型。显示的加速比是 K80 中的小批量时间除以较新 GPU 中的小批量时间，以百分比表示。这些测量值是使用 PyTorch v0.4（第 5 节中的更多详细信息）获得的，并且选择小批量大小作为模型作者指定的默认值。 V100 中 VAE 和 SuperResolution 任务的加速比其他任务低得多，因为这些是较小的模型，最终显着未充分利用 V100 等强大的 GPU。

![image-20230109120527744](https://cdn.jsdelivr.net/gh/Tweakzx/ImageHost@main/img/202301091205812.png)

对于这些测量，模型参数使用典型的 32 位浮点 (FP32) 格式进行训练。然而，深度学习社区越来越认识到，许多具有 16 位参数训练的模型以及具有 32 位参数的模型和更新的 GPU 都对 FP16 提供了高级支持。因此，最新一代 V100 GPU 具有专为支持 FP16 模型而设计的专用张量核心。与 FP32 相比，在具有 FP16 的 V100 上运行 ResNext-50 会导致进一步加速 10-15%，而在 K80 上运行相同结果会导致减速 20%，从而加剧了 K80 和 FP16 之间已经存在的 6.33 倍的巨大差距V100 超过 8 倍！

- 异构 GPU 性能观察。
  - 从这些测量中可以得出四个有趣的观察结果。首先，很明显，与旧 GPU 相比，所有模型都受益于新一代 GPU。因此，从减少模型训练时间的个人角度来看，用户更喜欢更新的 GPU 模型是很自然的。其次，我们可以看到使用较新 GPU 的边际效用在不同型号之间存在显着差异。例如，虽然在 V100 上运行 VAE 相对于 K80 仅提供 25% 的时间增益，但 ResNext-50 模型的时间增益为 533%。第三个观察结果是很难预测模型在不同 GPU 上的性能。例如，在 DCGAN 的情况下，P40 和 P100 的性能相似，而 V100 的性能明显优于 P40 和 P100。另一方面，对于 SuperResolution，P40、P100 和 V100 各自提供相对于彼此的增量增益。为了应对这种不可预测性，Gandiva-fair 使用跨代 GPU 的作业迁移以及分析来衡量具有不同 GPU 代的每个模型的性能，以做出交易决策。最后，我们注意到使用 FP16 格式作为其参数的模型比 K80 获得了更多的加速，证明了新 GPU 对这种格式的专门支持以及新 GPU 对于专门使用 FP16 参数的模型的边际效用更高。我们在第 3.4 节中讨论了 Gandiva-fair 如何利用这些观察结果来设计自动化 GPU 交易策略，从而在确保公平的同时最大限度地提高效率。

## Design

### Split Stride Gang-scheduling

- 小工作
- 大工作

### Load balancing

### Scheduler Efficiency

### Handling GPU heterogeneity transparently

### Gandiva-fair

## Implementation

Gandiva-fair 使用 Kubernetes [10] 作为集群管理器，使用自定义调度程序将作业分配给节点。作业作为 Docker [31] 容器提交。我们在大约 4k 行 Scala [7] 代码中实现了调度程序，使用 Akka Actors 库 [1] 实现并发，使用 gPRC [5] 执行远程过程调用。有四个主要模块：管理器、调度器、执行器和客户端。

Manager 公开了一个 REST API 和一个 gRPC 端点，供客户端连接到调度程序。调度器做出放置、迁移、票分配、奖励代币管理、交易等决策。有一个全局执行器执行多服务器作业的集群调度，集群中每个服务器都有一个本地执行器，他们共同负责用于根据调度程序分配的票证在服务器上运行作业。最后，与作业一起在容器内运行的客户端也暴露了一个 gRPC 端点，并负责从执行器接收命令以执行诸如暂停/恢复、检查点/迁移、报告作业元数据和报告状态的操作。正在运行的作业。

Gandiva-fair 使用的一个关键机制是能够在节点之间迁移作业。为了迁移作业，我们需要能够按需检查点作业，然后在不同的节点上恢复这些作业。一些 DLT 作业是用检查点功能编写的，以便它们可以从以前的检查点（如果存在）恢复。在撰写本文时对 GitHub 存储库的简单搜索表明，只有大约 20% 的 PyTorch 作业实现了这样的检查点功能。此外，即使在那些使用检查点的 DLT 作业中，它们通常也只在每个时期检查点。一个纪元可以持续几个小时或更长时间。虽然此类检查点有助于防止偶尔的服务器故障，但 Gandiva-fair 需要更细粒度的检查点以实现公平和效率。因此，我们在 Gandiva-fair 中实施了一个自动的、按需的检查点机制。

为了支持作业迁移，我们修改了 PyTorch 和 Tensorflow 框架。我们的实现可以处理未修改的用户代码，并且只需要对两个框架进行手术式更改。尽管存在诸如 CRIU [3] 之类的通用进程迁移工具，但它们无法处理具有 GPU 状态的进程。在我们的实现中，我们从主进程派生了一个代理进程。我们拦截进程发出的所有 CUDA 调用，并通过我们的代理将其定向。这样主进程的地址空间就只保留 CPU，并且可以很容易地通过 CRIU 进行检查点设置。代理进程负责 1) 翻译所有 CUDA 句柄，例如流、上下文等。2) 记录所有状态更改 CUDA 调用的日志，以便它们可以在恢复时重播，以及 3) GPU 内存的内存管理.内存管理器在迁移过程中以一致的方式将虚拟地址空间映射到物理 GPU 地址空间，因此指向 GPU 内存的指针对父进程保持完全透明。在检查点上，代理的内存管理器将 GPU 状态复制到父进程的 CPU 内存并终止。然后可以简单地对父进程进行 CRIU。恢复后，代理进程重放状态更改 CUDA 调用的日志，并将 GPU 内存复制回来。代理和父进程之间的所有通信都是通过共享内存处理的，开销可以忽略不计。 PyTorch 和 Tensorflow 之间的代理实现保持不变，只需要对实际框架进行最少的修改。

我们的暂停-恢复开销类似于 [41]，即大约 100-250 毫秒，具体取决于模型的大小。然而，与 [41] 相比，我们通过实施称为挂起-预加载-恢复的三阶段上下文切换来优化迁移性能开销。当框架被通知暂停时，它通过复制最少的在GPU中的数据在大约 100 毫秒内完成在 CPU 内存（父进程）的小批量训练结束时，因此，允许调度程序在 GPU 上运行另一个作业。如果作业需要跨服务器迁移，那么调度器会在作业容器上执行 CRIU 检查点并将其恢复到目标服务器上。然后框架等待预加载通知。当它接收到预加载时，它会通过重放所有有状态操作的日志来在新 GPU 上设置状态，但不会恢复。因此，预加载隐藏了 GPU 上下文初始化的 5 秒延迟。最后，当框架被通知恢复时，它会将数据复制回 GPU 内存，大约需要 100 毫秒，并快速恢复 GPU 计算。

因此，迁移主要发生在后台，而其他作业则使用 GPU。

